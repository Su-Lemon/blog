<!DOCTYPE html>

<html lang="zh-CN">

<head>
  
  <title>VPG与AC的思想和推导 - SuWei&#39;s Blog</title>
  <meta charset="UTF-8">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  
  

  <link rel="shortcut icon" href="/favicon.ico" type="image/png" />
  <meta name="description" content="策略梯度与Actor-Critic的思想和推导">
<meta property="og:type" content="article">
<meta property="og:title" content="VPG与AC的思想和推导">
<meta property="og:url" content="https://github.com/Su-Lemon/blog.git/2021/02/17/VPG%E4%B8%8EAC%E7%9A%84%E6%80%9D%E6%83%B3%E5%92%8C%E6%8E%A8%E5%AF%BC/index.html">
<meta property="og:site_name" content="SuWei&#39;s Blog">
<meta property="og:description" content="策略梯度与Actor-Critic的思想和推导">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfrynopsj30tp0ga77s.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrbg6to5vj318z0ch45l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjq3p6w8muj30k70cxwgf.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfnq05f7j30uv0gzgpc.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqc1z6kmgj321i0n2e2s.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfpvb4qaj30vg0g377r.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqdcs4up3j30s60dvgnb.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqemesrcyj31bs0rqdpk.jpg">
<meta property="article:published_time" content="2021-02-17T05:29:39.000Z">
<meta property="article:modified_time" content="2021-02-17T06:13:13.407Z">
<meta property="article:author" content="苏苇">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="algorithm">
<meta property="article:tag" content="PG">
<meta property="article:tag" content="AC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/combine/npm/highlight.js@9.15.8/styles/atom-one-dark.css,gh/theme-nexmoe/hexo-theme-nexmoe@latest/source/lib/mdui_043tiny/css/mdui.css,gh/theme-nexmoe/hexo-theme-nexmoe@latest/source/lib/iconfont/iconfont.css,gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css?v=233" crossorigin>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css">
  
  <link rel="stylesheet" href="/blog/css/style.css?v=1613549762565">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body class="mdui-drawer-body-left">
  
  <div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210217133822.png)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/blog/" title="苏苇" class="mdui-btn mdui-btn-icon"><img src="https://cdn.jsdelivr.net/gh/Su-Lemon/sources/imgs/avatar/IMG_0427.jpeg" alt="苏苇"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/blog/" title="苏苇">
            <img src="https://cdn.jsdelivr.net/gh/Su-Lemon/sources/imgs/avatar/IMG_0427.jpeg" alt="苏苇" alt="苏苇">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>2</div>
        <div><span>标签</span>4</div>
        <div><span>分类</span>2</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/about.html" title="关于博客">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博客
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/archives.html" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-container"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
        <form id="search_form" action_e="https://cn.bing.com/search?q=site:nexmoe.com" onsubmit="return search();">
            <label><input id="search_value" name="q" type="search" placeholder="搜索"></label>
        </form>
    </div>
</div>
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://jq.qq.com/?_wv=1027&k=MivHmcOi" target="_blank" mdui-tooltip="{content: 'QQ群'}" style="color: rgb(249, 174, 8);background-color: rgba(249, 174, 8, .1);">
            <i class="nexmoefont icon-QQ"></i>
        </a><a class="mdui-ripple" href="https://space.bilibili.com/76586290" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/Su-Lemon/" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章分类</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/blog/categories/强化学习/">强化学习</a>
          <span class="category-list-count">2</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/blog/categories/强化学习/强化学习落地方法论/">强化学习落地方法论</a>
          <span class="category-list-count">1</span>
        </li>

        
      </ul>

    </div>
  </div>


  
  
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/blog/tags/AC/" style="font-size: 10px;">AC</a> <a href="/blog/tags/PG/" style="font-size: 10px;">PG</a> <a href="/blog/tags/RL/" style="font-size: 20px;">RL</a> <a href="/blog/tags/algorithm/" style="font-size: 10px;">algorithm</a>
    </div>
    
  </div>

  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2021 苏苇
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/Su-Lemon" target="_blank">Su-Lemon</a>
        
    </div>
</div><!-- .nexmoe-drawer -->
  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <div class="nexmoe-post">
  
      <div class="nexmoe-post-cover" style="padding-bottom: 66.66666666666666%;"> 
          <img data-src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210216201538.jpg" data-sizes="auto" alt="VPG与AC的思想和推导" class="lazyload">
          <h1>VPG与AC的思想和推导</h1>
      </div>
  
  
  <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2021年02月17日</a>
    <a><i class="nexmoefont icon-areachart"></i>3k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 14 分钟</a>
</div>

  <div class="nexmoe-post-right">
    
      <div class="nexmoe-fixed">
        <div class="nexmoe-valign">
            <div class="nexmoe-toc">
                
                
                  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#rl%E5%AD%A6%E4%B9%A0%E4%BB%80%E4%B9%88"><span class="toc-number">1.</span> <span class="toc-text">RL学习什么</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#vanilla-policy-gradientvpg"><span class="toc-number">2.</span> <span class="toc-text">Vanilla Policy Gradient（VPG）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">2.1.</span> <span class="toc-text">策略网络的构造</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">2.2.</span> <span class="toc-text">推导最基本的策略梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#vpg%E7%AE%97%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text">VPG算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#actor-critic"><span class="toc-number">3.</span> <span class="toc-text">Actor-Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ac%E7%9A%84%E5%87%BA%E5%8F%91%E7%82%B9"><span class="toc-number">3.1.</span> <span class="toc-text">AC的出发点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">对策略梯度的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E8%A6%81%E5%8F%97%E8%BF%87%E5%8E%BB%E7%9A%84%E5%BD%B1%E5%93%8Ddont-let-the-past-distract-you"><span class="toc-number">3.2.1.</span> <span class="toc-text">不要受过去的影响（Don’t Let the Past Distract You）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hatqs_ta_t-%E7%9A%84baseline"><span class="toc-number">3.2.2.</span> <span class="toc-text">\(\hat{Q}(s_t,a_t)\) 的Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#value-net%E6%80%8E%E4%B9%88%E6%9B%B4%E6%96%B0"><span class="toc-number">3.2.3.</span> <span class="toc-text">Value net怎么更新</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#actor-critic%E7%AE%97%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">Actor-Critic算法</span></a></li></ol></li></ol>
                
            </div>
        </div>
      </div>
    
  </div>

  <article>
    <p>策略梯度与Actor-Critic的思想和推导</p>
<a id="more"></a>
<h1 id="rl学习什么">RL学习什么</h1>
<ul>
<li><p>动作值函数（Q函数）。</p>
<p>以Q-Learning、DQN为代表，这个系列的算法学习最优动作值函数 <span class="math inline">\(Q^*(s,a)\)</span> 的近似函数 <span class="math inline">\(Q_\theta(s,a)\)</span> 。</p>
<p>Q-learning 智能体的动作由下面的式子给出：</p></li>
</ul>
<p><span class="math display">\[
\begin{equation}
a(s)=\arg\,\max_a\, Q_\theta(s,a)
\label{222}
\end{equation}
\]</span></p>
<ul>
<li><p>策略（随机或确定的）。</p>
<p>这个系列的方法将策略显示表示为 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，它们直接对性能目标 <span class="math inline">\(J(\pi_{w})\)</span> 进行梯度下降来优化参数 <span class="math inline">\(w\)</span> ，使得我们输入当前的 <span class="math inline">\(s\)</span> 就能输出应该执行的最佳动作 <span class="math inline">\(a\)</span> 。</p></li>
<li><p>值函数。</p></li>
<li><p>以及/或者环境模型。</p></li>
</ul>
<h1 id="vanilla-policy-gradientvpg">Vanilla Policy Gradient（VPG）</h1>
<p>以下考虑的情况是状态 <span class="math inline">\(s\)</span> 为连续高维变量、动作 <span class="math inline">\(a\)</span> 为分类变量（有限个）的MDP。并且，设环境 <span class="math inline">\(P_{s, s^{\prime}}^{a}\)</span> 与 <span class="math inline">\(r_{5}^{a}\)</span> 为时齐的，不随时间的变化而变化。（状态与动作都是连续变量的MDP有更高效的DDPG等方法解决，不在VPG里讨论）</p>
<h2 id="策略网络的构造">策略网络的构造</h2>
<p>在随机且时齐的MDP中，<strong>策略是状态到动作的映射</strong>。由于 <span class="math inline">\(a\)</span> 是分类变量，所以没有办法直接输出 <span class="math inline">\(a\)</span> ，只能输出一个条件分布 <span class="math inline">\(\pi(a \mid s)\)</span> 。为了拟合这个策略，我们定义一个神经网络policy net。网络的输入是 <span class="math inline">\(s\)</span> ，输出是一个 <span class="math inline">\(n\)</span> 维向量，对它进行softmax之后，得到 <span class="math inline">\(n\)</span> 个不同的概率（其和为1），分别对应于最佳动作是各个 <span class="math inline">\(a\)</span> 的概率。设网络的参数为 <span class="math inline">\(w\)</span> ，则可以将网络输出简记为 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，它表示在 <span class="math inline">\(s\)</span> 状态下最佳动作是 <span class="math inline">\(a\)</span> 的条件概率。</p>
<p>与我们熟悉的分类网络做比较：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg" /></p>
<p>我们可以认为分类网络是在用“权重相同的训练集”去训练，而策略网络则是在用“带有不同权重的训练集”去训练。只要我们能够找出衡量 <span class="math inline">\((s,a)\)</span> 好坏的标准.<span class="math inline">\(v\)</span> ，得到形式 <span class="math inline">\((s,a,v)\)</span> 的数据，就可以把训练策略网络的过程看成“带权重的监督学习”。但是，我们如何找出这个 <span class="math inline">\(v\)</span> 呢？找出之后具体 <span class="math inline">\(v\)</span> 应该按照什么公式训练呢？下面，要详细地根据定义推导出policy gradient的表达式。</p>
<h2 id="推导最基本的策略梯度">推导最基本的策略梯度</h2>
<p>接下来推导策略梯度的公式及其计算方法。</p>
<p>假设策略网络的参数为 <span class="math inline">\(w\)</span> ，则可以将策略记为 <span class="math inline">\(\pi_w\)</span> 。</p>
<ul>
<li><strong>轨迹的概率</strong>。在不同网络参数 <span class="math inline">\(w\)</span> 下，策略 <span class="math inline">\(\pi_w\)</span> 给出的轨迹 <span class="math inline">\(\tau = \left(s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \ldots, s_{t}, a_{t}, r_{t}\right)\)</span> 有不同的分布 <span class="math inline">\(P_{\pi_{w}}(\tau)\)</span> ，简记为<span class="math inline">\(P_{w}(\tau)\)</span> 。</li>
</ul>
<p><span class="math display">\[
P_{w}(\tau)=\Pi_{t=0}^{n} \pi_{w}\left(a_{t} \mid s_{t}\right) \Pi_{t=0}^{T-1} P_{s_{t}, s_{t+1}}^{a_{t}} \Pi_{t=0}^{T} P\left(r_{t} \mid s_{t}, a_{t}\right)
\label{2}
\]</span></p>
<ul>
<li><strong>目标函数</strong>。我们的目标是最大化期望回报 <span class="math inline">\(J(w)=E_{w}(r(\tau))\)</span> ，这里假设回报无衰减（ <span class="math inline">\(\gamma=1\)</span> ，对于 <span class="math inline">\(\gamma&lt;1\)</span> 的情况推导过程类似）。</li>
</ul>
<p><span class="math display">\[
J(w) = E_{w}(r(\tau)) = \int_{\tau} P_{w}(\tau) r(\tau) d \tau
\]</span></p>
<ul>
<li><strong>策略梯度</strong>。我们想求的“策略梯度”就是 <span class="math inline">\(\nabla_{w} J(w)\)</span> 。得到策略梯度就可以通过梯度下降来优化策略 <span class="math inline">\(w_{k+1} = w_{k} + \alpha\,\nabla_{w} J(w)\)</span> 。</li>
</ul>
<p><span class="math display">\[
\nabla_{w} J(w)=\int_{\tau}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau
\label{4}
\]</span></p>
<p>​ 上面的<span class="math inline">\(\nabla_{w} J(w)\)</span>仍然是一个积分式，我们很自然地希望将其表示为<span class="math inline">\(\int_{\tau} P_{w}(\tau) \nabla_{w} f(\tau) d \tau\)</span>的形式，其中的<span class="math inline">\(f\)</span>是某个函数。根据期望的定义，这个积分的结果就是<span class="math inline">\(E_{w}\left[\nabla_{w} f(\tau)\right]\)</span>。这样的形式更加简便，并且也更容易计算——只要我们用当前的策略与环境交互采样很多<span class="math inline">\(\tau\)</span>，并计算出梯度<span class="math inline">\(\nabla_{w} f(\tau)\)</span>的均值，就能将其作为<span class="math inline">\(\nabla_{w} J(w)\)</span>的一个估计。</p>
<ul>
<li><strong>对数导数技巧</strong>。</li>
</ul>
<p><span class="math display">\[
P_{w}(\tau) \nabla_{w} \log P_{w}(\tau)=P_{w}(\tau) \frac{\nabla_{w} P_{w}(\tau)}{P_{w}(\tau)}=\nabla_{w} P_{w}(\tau)
\label{5}
\]</span></p>
<p>​ 将 <span class="math inline">\((\ref{5})\)</span> 带入 <span class="math inline">\((\ref{4})\)</span> 得： <span class="math display">\[
\begin{eqnarray*}
\nabla_{w} J(w) &amp;=&amp; \int_{\gamma}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau \\&amp;=&amp; \int_{\gamma} P_{w}(\tau) \nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau) d \tau \\&amp;=&amp; E_{w}\left[\nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau)\right]
\end{eqnarray*}
\]</span></p>
<ul>
<li><strong>环境函数的梯度</strong>。环境不依赖于参数 <span class="math inline">\(w\)</span> ，所以式 (<span class="math inline">\(\ref{2}\)</span>) 中 <span class="math inline">\(P_{s_{i}, s_{i+1}}^{a_{i}}\)</span> ，<span class="math inline">\(P\left(r_{i} \mid s_{i}, a_{i}\right)\)</span> 的梯度为零。</li>
<li><strong>轨迹对数概率的梯度</strong>。所以轨迹对数概率的梯度为：</li>
</ul>
<p><span class="math display">\[
\nabla_{w} \log \left(P_{w}(\tau)\right)=\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}(a_{t} \mid s_{t})
\]</span></p>
<ul>
<li><strong>简化后的基本策略梯度</strong>。</li>
</ul>
<p><span class="math display">\[
\nabla_{w} J(w)=E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau) \right]
\label{7}
\]</span></p>
<p>​ 这是一个期望，这意味着我们可以使用样本均值对其进行估计。 如果我们收集一组轨迹 <span class="math inline">\(D = {\left\{ \tau_i \right\}}_{i=1,\cdots,N}\)</span> ， 其中每一个轨迹通过让智能体在环境中使用策略 <span class="math inline">\(\pi_w\)</span> 执行操作得到，则策略梯度可以使用以下式子进行估计： <span class="math display">\[
\hat{g} = \frac{1}{N}\sum_{\tau \in D} \sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau)
\label{ref9}
\]</span></p>
<h2 id="vpg算法">VPG算法</h2>
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="header">
<th>Policy gradient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>构建策略网络 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，并随机初始化参数</td>
</tr>
<tr class="even">
<td>重复下面步骤：</td>
</tr>
<tr class="odd">
<td>- 用策略 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> 与环境交互，产生大量 <span class="math inline">\(\tau\)</span></td>
</tr>
<tr class="even">
<td>- 计算 <span class="math inline">\(\nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau)\)</span> 的均值，作为策略梯度 <span class="math inline">\(\nabla_wJ(w)\)</span> 的估计</td>
</tr>
<tr class="odd">
<td>- 让 <span class="math inline">\(w\)</span> 沿着策略梯度的方向前进：<span class="math inline">\(w = w + \alpha\,\nabla_{w} J(w)\)</span></td>
</tr>
<tr class="even">
<td>直到收敛</td>
</tr>
</tbody>
</table>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfrynopsj30tp0ga77s.jpg" style="zoom:50%;" /></p>
<h1 id="actor-critic">Actor-Critic</h1>
<h2 id="ac的出发点">AC的出发点</h2>
<p>上述策略梯度式 <span class="math inline">\((\ref{7})\)</span> 可以理解为：“用带有权重的训练集去训练策略网络”，对于每一步决策，我们用一个能衡量这步决策好坏的“学习权重” <span class="math inline">\(r(\tau)\)</span> 去”促进“或”抑制“当前轨迹 <span class="math inline">\(\tau\)</span> 上的所有决策 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> 。<span class="math inline">\(r(\tau)&gt;0\)</span> ，则”促进“这个轨迹上的所有策略；<span class="math inline">\(r(\tau)&lt;0\)</span> ，则”抑制“这个轨迹上的所有策略。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrbg6to5vj318z0ch45l.jpg" /></p>
<p>但是，即使 <span class="math inline">\(r(\tau)&gt;0\)</span> ，轨迹 <span class="math inline">\(\tau\)</span> 上也有可能出现少量差的决策，如果采用上面的方法，会同时”促进“这些差的决策。<strong>在采样样本比较有限的情况下，这可能会导致巨大的均方误差</strong>。</p>
<p>一个最自然的想法是，<strong>我们不应该将一个 <span class="math inline">\(\tau\)</span> 上所有 <span class="math inline">\((s, a)\)</span> 编成一个batch，用一个统一的“权重” <span class="math inline">\(r(\tau)\)</span> 来衡量它们的好坏。而应该找出一个“权重”能够单独衡量每一个 <span class="math inline">\((s, a)\)</span> 的好坏</strong>。</p>
<h2 id="对策略梯度的优化">对策略梯度的优化</h2>
<h3 id="不要受过去的影响dont-let-the-past-distract-you">不要受过去的影响（Don’t Let the Past Distract You）</h3>
<p>回顾我们的策略梯度表达式 <span class="math inline">\((\ref{7})\)</span> ，他将“轨迹”上<strong>每个动作</strong>的对数概率都乘了一个“权重” <span class="math inline">\(r(\tau)\)</span> （曾经与将来所有奖励的总和）。</p>
<p>但这没有多大意义。智能体实际上仅应根据其<strong>采取动作后的 <em>结果</em> 强化动作</strong>。采取动作之前获得的奖励与该动作的效果无关。这种直觉体现在数学上，可以证明策略梯度也可以表示为： <span class="math display">\[
\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \sum_{t‘=t}^{T}R(s_{t&#39;},a_{t&#39;},s_{t&#39;+1}) \right] \\= E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \hat{Q}(s_t,a_t) \right]
\label{9}
\]</span> <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjq3p6w8muj30k70cxwgf.jpg" style="zoom:60%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfnq05f7j30uv0gzgpc.jpg" style="zoom:50%;" /></p>
<p><strong>为什么这样做会更好</strong>？策略梯度的关键问题是需要多少个样本轨迹才能获得它们的低方差样本估计。 我们从公式开始就包括了与过去的奖励成比例的强化动作的项， 它们均值为零，但方差不为零：导致它们只会给策略梯度的样本估计值增加噪音。 通过删除它们，<strong>减少了所需的样本轨迹数量</strong>。</p>
<h3 id="hatqs_ta_t-的baseline"><span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 的Baseline</h3>
<p>式 <span class="math inline">\((\ref{9})\)</span> 用当前动作的”状态-动作“价值函数的估计 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 作为衡量本次决策的”权重“，看起来是很合适的，但是必须考虑这样一种情形：在回报都是大于零的环境中（例如贪吃蛇游戏，把游戏结束时蛇身的长度作为回报），<strong><span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 会是恒正的值</strong>，按照上面的思路，<strong>即使是一个很差的决策（例如游戏结束时蛇身长为3），策略梯度也会在”权重“ <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 的作用下比较缓慢的”促进“这个决策</strong>。</p>
<p>但是，如果我们采样了 <span class="math inline">\(N\)</span> 条轨迹，就可以<strong>用 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 减去自己的均值，使得比均值小的 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 变为负，比均值大的 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 变为正，”赏罚分明“的进行训练策略</strong>。</p>
<p>那么，给式 <span class="math inline">\((\ref{9})\)</span> 策略梯度的 <span class="math inline">\(\hat{Q}(s_t,a_t)+b(s_t)\)</span> 后该式还成立吗？可以证明它仍然是成立的。</p>
<p>由ELPG引理可得： <span class="math display">\[
E_{w}\left[\nabla_{w} \log \left(P_{w}(x)\right) b\right]=\int_{x} \nabla_{w} P_{w}(x)b\  dx = b \nabla_{w} \int_{x} P_{w}(x)\  dx=0
\]</span> 所以： <span class="math display">\[
\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-b(s_t)\right) \right]
\]</span> 其中 <span class="math inline">\(b(s_t)=\frac{1}{N} \sum_i^N Q_{i,t}\)</span> ，它可以看作是对状态值函数 <span class="math inline">\(V(s_t)=E_w(Q(s_t,a_t))\)</span> 的估计，因此<strong>策略梯度可以表示为</strong>： <span class="math display">\[
\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-V(s_t)\right) \right]
\label{12}
\]</span></p>
<p>而由Bellman Equation知道：<span class="math inline">\(A(s_t,a_t)=Q(s_t,a_t)-V(s_t)\)</span> ，因此策略梯度又可以表示为： <span class="math display">\[
\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) A(s_t,a_t) \right]
\label{13}
\]</span> 由于： <span class="math display">\[
Q(s_t,a_t) = r(s_t,a_t)+V(s_{t+1}) \\
A(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1})-V(s_{t})
\]</span> 所以，计算策略梯度式 <span class="math inline">\((\ref{12})\)</span> 的关键是计算 <span class="math inline">\(V(s_t)\)</span> 。实际上，无法精确计算 <strong><span class="math inline">\(V(s_t)\)</span> ，通常这是通过神经网络 <span class="math inline">\(V_{\phi}(s_t)\)</span> 来近似的（Value net）</strong>。该神经网络会与策略同时进行更新（以便价值网络始终近似于最新策略的值函数）。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqc1z6kmgj321i0n2e2s.jpg" style="zoom:20%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfpvb4qaj30vg0g377r.jpg" style="zoom:50%;" /></p>
<h3 id="value-net怎么更新">Value net怎么更新</h3>
<p>Value net的目的是估计 <span class="math inline">\(V(s_t)\)</span> ，那么最小化它们之间的均方误差就可以作为一个监督信号，用来在神经网络中反向传播学习 <span class="math inline">\(V_{\phi}\)</span> 。 <span class="math display">\[
\begin{eqnarray*}
L(\phi) &amp;=&amp; \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-V(s_{i,t}) \right\|}^2 \\&amp;=&amp; \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-\left(r(s_{i,t},a_{i,t})+V_{\phi}(s_{i,t+1})\right) \right\|}^2
\end{eqnarray*}
\]</span> 至此，Value net可以通过训练，很好的估计 <span class="math inline">\(V(s_t)\)</span> 。将它替代VPG中的 <span class="math inline">\(r(\tau)\)</span> 作为”权重“，指引策略网络学习最优策略。同时也达到了本节开始提出的目的——找出一个能够单独衡量每一个 <span class="math inline">\((s, a)\)</span> 好坏的“权重”。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqdcs4up3j30s60dvgnb.jpg" style="zoom:50%;" /></p>
<h2 id="actor-critic算法">Actor-Critic算法</h2>
<p>AC算法的大体框架是这样的：我们定义两个神经网络：一个是用来计算 <span class="math inline">\(V_{\phi}(s_t)\)</span> 价值网络，另一个则是策略网络。我们用策略网络与环境交互产生许多数据集，并用这些数据集同时训练两个网络，提升网络的性能。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqemesrcyj31bs0rqdpk.jpg" /></p>
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="header">
<th>Actor-Critic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>构造并初始化Value net的参数 <span class="math inline">\(w\)</span> 和Policy net的参数 <span class="math inline">\(\phi\)</span></td>
</tr>
<tr class="even">
<td>重复以下步骤：</td>
</tr>
<tr class="odd">
<td>- 通过Policy net与环境交互产生数据集 <span class="math inline">\((s,a,r,s&#39;)\)</span></td>
</tr>
<tr class="even">
<td>- 训练Value net：让 <span class="math inline">\(\phi\)</span> 沿着使损失 <span class="math inline">\(L(\phi)=\left\| V_{\phi}(s)-(r+V_{\phi}(s&#39;)) \right\|^2\)</span> 下降的方向前进</td>
</tr>
<tr class="odd">
<td>- 训练Pollicy net：让 <span class="math inline">\(w\)</span> 沿着梯度 <span class="math inline">\(\nabla_wJ(w)\)</span> 的方向前进</td>
</tr>
<tr class="even">
<td>直到收敛</td>
</tr>
</tbody>
</table>

  </article>

  
    
  <div class="nexmoe-post-copyright">
    <strong>本文作者：</strong>苏苇<br>
    <strong>本文链接：</strong><a href="https://github.com/Su-Lemon/blog.git/2021/02/17/VPG%E4%B8%8EAC%E7%9A%84%E6%80%9D%E6%83%B3%E5%92%8C%E6%8E%A8%E5%AF%BC/" title="https:&#x2F;&#x2F;github.com&#x2F;Su-Lemon&#x2F;blog.git&#x2F;2021&#x2F;02&#x2F;17&#x2F;VPG%E4%B8%8EAC%E7%9A%84%E6%80%9D%E6%83%B3%E5%92%8C%E6%8E%A8%E5%AF%BC&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;github.com&#x2F;Su-Lemon&#x2F;blog.git&#x2F;2021&#x2F;02&#x2F;17&#x2F;VPG%E4%B8%8EAC%E7%9A%84%E6%80%9D%E6%83%B3%E5%92%8C%E6%8E%A8%E5%AF%BC&#x2F;</a><br>
    
      <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
    
  </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
    
        <a class="nexmoefont icon-appstore-fill -link" href="/blog/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
    
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/blog/tags/AC/" rel="tag">AC</a> <a class="nexmoefont icon-tag-fill -none-link" href="/blog/tags/PG/" rel="tag">PG</a> <a class="nexmoefont icon-tag-fill -none-link" href="/blog/tags/RL/" rel="tag">RL</a> <a class="nexmoefont icon-tag-fill -none-link" href="/blog/tags/algorithm/" rel="tag">algorithm</a>
    
</div>

  <div class="nexmoe-post-footer">
    <section class="nexmoe-comment">
    
        <link rel="stylesheet" href=//unpkg.com/gitalk/dist/gitalk.css>
<div id="gitalk"></div>
<script src=//unpkg.com/gitalk/dist/gitalk.min.js></script>
<script src="js/md5.min.js"></script>
<script type="text/javascript">
    // var gitalk = new Gitalk({
    //   clientID: '',
    //   clientSecret: '',
    //   repo: 'gittalk-comments.github.io',
    //   owner: 'Su-Lemon',
    //   admin: 'Su-Lemon',
    //   id: 'location.pathname',
    //   labels: 'Gitalk'.split(',').filter(l => l),
    // //   perPage: '15',
    //   pagerDirection: 'last',
    //   createIssueManually: 'true',
    //   distractionFreeMode: 'false'
    // })
    var gitalk = new Gitalk({
      clientID: '2543206578680a1bdf99',
      clientSecret: '24e25a0277329218d91186c354b4a9d3ec9eaf29',
      repo: 'gittalk-comments',
      owner: 'Su-Lemon',
      admin: 'Su-Lemon',
      id: md5(location.pathname),
      labels: ['Gitalk'],
      perPage: 15,
      pagerDirection: 'last',
      createIssueManually: true,
      distractionFreeMode: false
    })
    gitalk.render('gitalk')
</script>
    
</section>

  </div>
</div>


  
  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <!-- 给MathJax元素添加has-jax class -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        Tex: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
        "HTML-CSS": {linebreaks: {automatic: true}},
        SVG: {linebreaks: {automatic: true}}
    });
  </script>
  <!-- 通过连接CDN加载MathJax的js代码 -->
  <script type="text/javascript" src=//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML>
  </script>




        <div class="nexmoe-post-right">
          
            <div class="nexmoe-fixed">
              <div class="nexmoe-tool">
                <a href="#" aria-label="回到顶部" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
              </div>
            </div>
          
        </div>
    </div>
  </div>
  <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js?v=1"></script>
<script>
	hljs.initHighlightingOnLoad();
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<script src="https://cdn.jsdelivr.net/gh/xtaodada/xtaodada.github.io@0.0.2/copy.js"></script>
 <script src="/blog/js/app.js?v=1613549762567"></script>

<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>

  





</body>

</html>

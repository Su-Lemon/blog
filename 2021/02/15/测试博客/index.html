<!DOCTYPE html>

<html lang="zh-CN">

<head>
  
  <title>测试博客 - SuWei&#39;s Blog</title>
  <meta charset="UTF-8">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  
  

  <link rel="shortcut icon" href="/favicon.ico" type="image/png" />
  <meta name="description" content="VPG与AC的思想与推导">
<meta property="og:type" content="article">
<meta property="og:title" content="测试博客">
<meta property="og:url" content="https://github.com/Su-Lemon/blog.git/2021/02/15/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/index.html">
<meta property="og:site_name" content="SuWei&#39;s Blog">
<meta property="og:description" content="VPG与AC的思想与推导">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfrynopsj30tp0ga77s.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrbg6to5vj318z0ch45l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjq3p6w8muj30k70cxwgf.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfnq05f7j30uv0gzgpc.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqc1z6kmgj321i0n2e2s.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfpvb4qaj30vg0g377r.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqdcs4up3j30s60dvgnb.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqemesrcyj31bs0rqdpk.jpg">
<meta property="article:published_time" content="2021-02-15T06:51:59.000Z">
<meta property="article:modified_time" content="2021-02-15T13:03:18.006Z">
<meta property="article:author" content="苏苇">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/highlight.js@9.15.8/styles/atom-one-dark.css,gh/theme-nexmoe/hexo-theme-nexmoe@latest/source/lib/mdui_043tiny/css/mdui.css,gh/theme-nexmoe/hexo-theme-nexmoe@latest/source/lib/iconfont/iconfont.css,gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css?v=233" crossorigin>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css">
  
  <link rel="stylesheet" href="/blog/css/style.css?v=1613394311201">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>

<body class="mdui-drawer-body-left">
  
  <div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/blog/" title="苏苇" class="mdui-btn mdui-btn-icon"><img src="https://cdn.jsdelivr.net/gh/Su-Lemon/sources/imgs/avatar/IMG_0427.jpeg" alt="苏苇"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/blog/" title="苏苇">
            <img src="https://cdn.jsdelivr.net/gh/Su-Lemon/sources/imgs/avatar/IMG_0427.jpeg" alt="苏苇" alt="苏苇">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>1</div>
        <div><span>标签</span>0</div>
        <div><span>分类</span>0</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/about.html" title="关于博客">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博客
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/blog/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
        <form id="search_form" action_e="https://cn.bing.com/search?q=site:nexmoe.com" onsubmit="return search();">
            <label><input id="search_value" name="q" type="search" placeholder="搜索"></label>
        </form>
    </div>
</div>
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://jq.qq.com/?_wv=1027&k=MivHmcOi" target="_blank" mdui-tooltip="{content: 'QQ群'}" style="color: rgb(249, 174, 8);background-color: rgba(249, 174, 8, .1);">
            <i class="nexmoefont icon-QQ"></i>
        </a><a class="mdui-ripple" href="https://space.bilibili.com/76586290" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/Su-Lemon/" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
  
  

  
  
  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2021 苏苇
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        
    </div>
</div><!-- .nexmoe-drawer -->
  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <div class="nexmoe-post">
  
      <div class="nexmoe-post-cover" style="padding-bottom: 66.66666666666666%;"> 
          <img data-src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg" data-sizes="auto" alt="测试博客" class="lazyload">
          <h1>测试博客</h1>
      </div>
  
  
  <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2021年02月15日</a>
    <a><i class="nexmoefont icon-areachart"></i>3k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 14 分钟</a>
</div>

  <div class="nexmoe-post-right">
    
      <div class="nexmoe-fixed">
        <div class="nexmoe-valign">
            <div class="nexmoe-toc">
                
                
                  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#VPG%E4%B8%8EAC%E7%9A%84%E6%80%9D%E6%83%B3%E4%B8%8E%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.</span> <span class="toc-text">VPG与AC的思想与推导</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RL%E5%AD%A6%E4%B9%A0%E4%BB%80%E4%B9%88"><span class="toc-number">2.</span> <span class="toc-text">RL学习什么</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Vanilla-Policy-Gradient%EF%BC%88VPG%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">Vanilla Policy Gradient（VPG）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E9%80%A0"><span class="toc-number">3.1.</span> <span class="toc-text">策略网络的构造</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E5%AF%BC%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">3.2.</span> <span class="toc-text">推导最基本的策略梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VPG%E7%AE%97%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">VPG算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Actor-Critic"><span class="toc-number">4.</span> <span class="toc-text">Actor-Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#AC%E7%9A%84%E5%87%BA%E5%8F%91%E7%82%B9"><span class="toc-number">4.1.</span> <span class="toc-text">AC的出发点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">4.2.</span> <span class="toc-text">对策略梯度的优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E8%A6%81%E5%8F%97%E8%BF%87%E5%8E%BB%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%88Don%E2%80%99t-Let-the-Past-Distract-You%EF%BC%89"><span class="toc-number">4.2.1.</span> <span class="toc-text">不要受过去的影响（Don’t Let the Past Distract You）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hat-Q-s-t-a-t-%E7%9A%84Baseline"><span class="toc-number">4.2.2.</span> <span class="toc-text">$\hat{Q}(s_t,a_t)$ 的Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-net%E6%80%8E%E4%B9%88%E6%9B%B4%E6%96%B0"><span class="toc-number">4.2.3.</span> <span class="toc-text">Value net怎么更新</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic%E7%AE%97%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">Actor-Critic算法</span></a></li></ol></li></ol>
                
            </div>
        </div>
      </div>
    
  </div>

  <article>
    <p>VPG与AC的思想与推导</p>
<a id="more"></a>
<h1 id="VPG与AC的思想与推导"><a href="#VPG与AC的思想与推导" class="headerlink" title="VPG与AC的思想与推导"></a>VPG与AC的思想与推导</h1><h1 id="RL学习什么"><a href="#RL学习什么" class="headerlink" title="RL学习什么"></a>RL学习什么</h1><ul>
<li><p>动作值函数（Q函数）。</p>
<p>以Q-Learning、DQN为代表，这个系列的算法学习最优动作值函数 $Q^*(s,a)$ 的近似函数 $Q_\theta(s,a)$ 。</p>
<p>Q-learning 智能体的动作由下面的式子给出：</p>
</li>
</ul>
<script type="math/tex; mode=display">
a(s)=\arg\,\max_a\, Q_\theta(s,a)</script><ul>
<li><p>策略（随机或确定的）。</p>
<p>这个系列的方法将策略显示表示为 $\pi<em>{w}(a \mid s)$ ，它们直接对性能目标 $J(\pi</em>{w})$ 进行梯度下降来优化参数 $w$ ，使得我们输入当前的 $s$ 就能输出应该执行的最佳动作 $a$ 。</p>
</li>
<li><p>值函数。</p>
</li>
<li><p>以及/或者环境模型。</p>
</li>
</ul>
<h1 id="Vanilla-Policy-Gradient（VPG）"><a href="#Vanilla-Policy-Gradient（VPG）" class="headerlink" title="Vanilla Policy Gradient（VPG）"></a>Vanilla Policy Gradient（VPG）</h1><p>以下考虑的情况是状态 $s$ 为连续高维变量、动作 $a$ 为分类变量（有限个）的MDP。并且，设环境 $P<em>{s, s^{\prime}}^{a}$ 与 $r</em>{5}^{a}$ 为时齐的，不随时间的变化而变化。（状态与动作都是连续变量的MDP有更高效的DDPG等方法解决，不在VPG里讨论）</p>
<h2 id="策略网络的构造"><a href="#策略网络的构造" class="headerlink" title="策略网络的构造"></a>策略网络的构造</h2><p>在随机且时齐的MDP中，<strong>策略是状态到动作的映射</strong>。由于 $a$ 是分类变量，所以没有办法直接输出 $a$ ，只能输出一个条件分布 $\pi(a \mid s)$ 。为了拟合这个策略，我们定义一个神经网络policy net。网络的输入是 $s$ ，输出是一个 $n$ 维向量，对它进行softmax之后，得到 $n$ 个不同的概率（其和为1），分别对应于最佳动作是各个 $a$ 的概率。设网络的参数为 $w$ ，则可以将网络输出简记为 $\pi_{w}(a \mid s)$ ，它表示在 $s$ 状态下最佳动作是 $a$ 的条件概率。</p>
<p>与我们熟悉的分类网络做比较：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg" alt="" class="lazyload"></p>
<p>我们可以认为分类网络是在用“权重相同的训练集”去训练，而策略网络则是在用“带有不同权重的训练集”去训练。只要我们能够找出衡量 $(s,a)$ 好坏的标准.$v$ ，得到形式 $(s,a,v)$ 的数据，就可以把训练策略网络的过程看成“带权重的监督学习”。但是，我们如何找出这个 $v$ 呢？找出之后具体 $v$ 应该按照什么公式训练呢？下面，要详细地根据定义推导出policy gradient的表达式。</p>
<h2 id="推导最基本的策略梯度"><a href="#推导最基本的策略梯度" class="headerlink" title="推导最基本的策略梯度"></a>推导最基本的策略梯度</h2><p>接下来推导策略梯度的公式及其计算方法。</p>
<p>假设策略网络的参数为 $w$ ，则可以将策略记为 $\pi_w$ 。</p>
<ul>
<li><strong>轨迹的概率</strong>。在不同网络参数 $w$ 下，策略 $\pi<em>w$ 给出的轨迹 $\tau = \left(s</em>{0}, a<em>{0}, r</em>{0}, s<em>{1}, a</em>{1}, r<em>{1}, s</em>{2}, a<em>{2}, r</em>{2}, \ldots, s<em>{t}, a</em>{t}, r<em>{t}\right)$  有不同的分布 $P</em>{\pi<em>{w}}(\tau)$ ，简记为$P</em>{w}(\tau)$ 。</li>
</ul>
<script type="math/tex; mode=display">
P_{w}(\tau)=\Pi_{t=0}^{n} \pi_{w}\left(a_{t} \mid s_{t}\right) \Pi_{t=0}^{T-1} P_{s_{t}, s_{t+1}}^{a_{t}} \Pi_{t=0}^{T} P\left(r_{t} \mid s_{t}, a_{t}\right)
\label{2}</script><ul>
<li><strong>目标函数</strong>。我们的目标是最大化期望回报 $J(w)=E_{w}(r(\tau))$ ，这里假设回报无衰减（ $\gamma=1$ ，对于 $\gamma&lt;1$ 的情况推导过程类似）。</li>
</ul>
<script type="math/tex; mode=display">
J(w) = E_{w}(r(\tau)) = \int_{\tau} P_{w}(\tau) r(\tau) d \tau</script><ul>
<li><strong>策略梯度</strong>。我们想求的“策略梯度”就是 $\nabla<em>{w} J(w)$ 。得到策略梯度就可以通过梯度下降来优化策略 $w</em>{k+1} = w<em>{k} + \alpha\,\nabla</em>{w} J(w)$ 。</li>
</ul>
<script type="math/tex; mode=display">
\nabla_{w} J(w)=\int_{\tau}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau
\label{4}</script><p>​        上面的$\nabla<em>{w} J(w)$仍然是一个积分式，我们很自然地希望将其表示为$\int</em>{\tau} P<em>{w}(\tau) \nabla</em>{w} f(\tau) d \tau$的形式，其中的$f$是某个函数。根据期望的定义，这个积分的结果就是$E<em>{w}\left[\nabla</em>{w} f(\tau)\right]$。这样的形式更加简便，并且也更容易计算——只要我们用当前的策略与环境交互采样很多$\tau$，并计算出梯度$\nabla<em>{w} f(\tau)$的均值，就能将其作为$\nabla</em>{w} J(w)$的一个估计。</p>
<ul>
<li><strong>对数导数技巧</strong>。</li>
</ul>
<script type="math/tex; mode=display">
P_{w}(\tau) \nabla_{w} \log P_{w}(\tau)=P_{w}(\tau) \frac{\nabla_{w} P_{w}(\tau)}{P_{w}(\tau)}=\nabla_{w} P_{w}(\tau)
\label{5}</script><p>​        将 $(\ref{5})$ 带入 $(\ref{4})$ 得：</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
\nabla_{w} J(w) &=& \int_{\gamma}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau \\&=& \int_{\gamma} P_{w}(\tau) \nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau) d \tau \\&=& E_{w}\left[\nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau)\right]
\end{eqnarray*}</script><ul>
<li><strong>环境函数的梯度</strong>。环境不依赖于参数 $w$ ，所以式 ($\ref{2}$) 中 $P<em>{s</em>{i}, s<em>{i+1}}^{a</em>{i}}$ ，$P\left(r<em>{i} \mid s</em>{i}, a_{i}\right)$ 的梯度为零。</li>
<li><strong>轨迹对数概率的梯度</strong>。所以轨迹对数概率的梯度为：</li>
</ul>
<script type="math/tex; mode=display">
\nabla_{w} \log \left(P_{w}(\tau)\right)=\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}(a_{t} \mid s_{t})</script><ul>
<li><strong>简化后的基本策略梯度</strong>。</li>
</ul>
<script type="math/tex; mode=display">
\nabla_{w} J(w)=E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau) \right]
\label{7}</script><p>​        这是一个期望，这意味着我们可以使用样本均值对其进行估计。 如果我们收集一组轨迹 $D = {\left{ \tau<em>i \right}}</em>{i=1,\cdots,N}$ ，        其中每一个轨迹通过让智能体在环境中使用策略 $\pi_w$ 执行操作得到，则策略梯度可以使用以下式子进行估计：</p>
<script type="math/tex; mode=display">
\hat{g} = \frac{1}{N}\sum_{\tau \in D} \sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau)
\label{ref9}</script><h2 id="VPG算法"><a href="#VPG算法" class="headerlink" title="VPG算法"></a>VPG算法</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Policy gradient</th>
</tr>
</thead>
<tbody>
<tr>
<td>构建策略网络 $\pi_{w}(a \mid s)$ ，并随机初始化参数</td>
</tr>
<tr>
<td>重复下面步骤：</td>
</tr>
<tr>
<td>- 用策略 $\pi_{w}(a \mid s)$ 与环境交互，产生大量 $\tau$</td>
</tr>
<tr>
<td>- 计算 $\nabla<em>{w} \log \pi</em>{w}\left(a<em>{t} \mid s</em>{t}\right)r(\tau)$ 的均值，作为策略梯度 $\nabla_wJ(w)$ 的估计</td>
</tr>
<tr>
<td>- 让 $w$ 沿着策略梯度的方向前进：$w = w + \alpha\,\nabla_{w} J(w)$</td>
</tr>
<tr>
<td>直到收敛</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfrynopsj30tp0ga77s.jpg" style="zoom:50%;" /></p>
<h1 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h1><h2 id="AC的出发点"><a href="#AC的出发点" class="headerlink" title="AC的出发点"></a>AC的出发点</h2><p>上述策略梯度式 $(\ref{7})$ 可以理解为：“用带有权重的训练集去训练策略网络”，对于每一步决策，我们用一个能衡量这步决策好坏的“学习权重” $r(\tau)$ 去”促进“或”抑制“当前轨迹 $\tau$ 上的所有决策 $\pi_{w}(a \mid s)$ 。$r(\tau)&gt;0$ ，则”促进“这个轨迹上的所有策略；$r(\tau)&lt;0$ ，则”抑制“这个轨迹上的所有策略。</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrbg6to5vj318z0ch45l.jpg" alt="" class="lazyload"></p>
<p>但是，即使 $r(\tau)&gt;0$ ，轨迹 $\tau$ 上也有可能出现少量差的决策，如果采用上面的方法，会同时”促进“这些差的决策。<strong>在采样样本比较有限的情况下，这可能会导致巨大的均方误差</strong>。</p>
<p>一个最自然的想法是，<strong>我们不应该将一个 $\tau$ 上所有 $(s, a)$ 编成一个batch，用一个统一的“权重” $r(\tau)$ 来衡量它们的好坏。而应该找出一个“权重”能够单独衡量每一个 $(s, a)$ 的好坏</strong>。</p>
<h2 id="对策略梯度的优化"><a href="#对策略梯度的优化" class="headerlink" title="对策略梯度的优化"></a>对策略梯度的优化</h2><h3 id="不要受过去的影响（Don’t-Let-the-Past-Distract-You）"><a href="#不要受过去的影响（Don’t-Let-the-Past-Distract-You）" class="headerlink" title="不要受过去的影响（Don’t Let the Past Distract You）"></a>不要受过去的影响（Don’t Let the Past Distract You）</h3><p>回顾我们的策略梯度表达式 $(\ref{7})$ ，他将“轨迹”上<strong>每个动作</strong>的对数概率都乘了一个“权重” $r(\tau)$ （曾经与将来所有奖励的总和）。</p>
<p>但这没有多大意义。智能体实际上仅应根据其<strong>采取动作后的 <em>结果</em> 强化动作</strong>。采取动作之前获得的奖励与该动作的效果无关。这种直觉体现在数学上，可以证明策略梯度也可以表示为：</p>
<script type="math/tex; mode=display">
\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \sum_{t‘=t}^{T}R(s_{t'},a_{t'},s_{t'+1}) \right] \\= E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \hat{Q}(s_t,a_t) \right]
\label{9}</script><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjq3p6w8muj30k70cxwgf.jpg" style="zoom:60%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfnq05f7j30uv0gzgpc.jpg" style="zoom:50%;" /></p>
<p><strong>为什么这样做会更好</strong>？策略梯度的关键问题是需要多少个样本轨迹才能获得它们的低方差样本估计。 我们从公式开始就包括了与过去的奖励成比例的强化动作的项， 它们均值为零，但方差不为零：导致它们只会给策略梯度的样本估计值增加噪音。 通过删除它们，<strong>减少了所需的样本轨迹数量</strong>。</p>
<h3 id="hat-Q-s-t-a-t-的Baseline"><a href="#hat-Q-s-t-a-t-的Baseline" class="headerlink" title="$\hat{Q}(s_t,a_t)$ 的Baseline"></a>$\hat{Q}(s_t,a_t)$ 的Baseline</h3><p>式 $(\ref{9})$ 用当前动作的”状态-动作“价值函数的估计 $\hat{Q}(s_t,a_t)$ 作为衡量本次决策的”权重“，看起来是很合适的，但是必须考虑这样一种情形：在回报都是大于零的环境中（例如贪吃蛇游戏，把游戏结束时蛇身的长度作为回报），<strong>$\hat{Q}(s_t,a_t)$ 会是恒正的值</strong>，按照上面的思路，<strong>即使是一个很差的决策（例如游戏结束时蛇身长为3），策略梯度也会在”权重“ $\hat{Q}(s_t,a_t)$ 的作用下比较缓慢的”促进“这个决策</strong>。</p>
<p>但是，如果我们采样了 $N$ 条轨迹，就可以<strong>用 $\hat{Q}(s_t,a_t)$ 减去自己的均值，使得比均值小的 $\hat{Q}(s_t,a_t)$ 变为负，比均值大的 $\hat{Q}(s_t,a_t)$ 变为正，”赏罚分明“的进行训练策略</strong>。</p>
<p>那么，给式 $(\ref{9})$ 策略梯度的 $\hat{Q}(s_t,a_t)+b(s_t)$ 后该式还成立吗？可以证明它仍然是成立的。</p>
<p>由ELPG引理可得：</p>
<script type="math/tex; mode=display">
E_{w}\left[\nabla_{w} \log \left(P_{w}(x)\right) b\right]=\int_{x} \nabla_{w} P_{w}(x)b\  dx = b \nabla_{w} \int_{x} P_{w}(x)\  dx=0</script><p>所以：</p>
<script type="math/tex; mode=display">
\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-b(s_t)\right) \right]</script><p>其中 $b(s<em>t)=\frac{1}{N} \sum_i^N Q</em>{i,t}$ ，它可以看作是对状态值函数 $V(s_t)=E_w(Q(s_t,a_t))$ 的估计，因此<strong>策略梯度可以表示为</strong>：</p>
<script type="math/tex; mode=display">
\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-V(s_t)\right) \right]
\label{12}</script><p>而由Bellman Equation知道：$A(s_t,a_t)=Q(s_t,a_t)-V(s_t)$ ，因此策略梯度又可以表示为：</p>
<script type="math/tex; mode=display">
\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) A(s_t,a_t) \right]
\label{13}</script><p>由于：</p>
<script type="math/tex; mode=display">
Q(s_t,a_t) = r(s_t,a_t)+V(s_{t+1}) \\
A(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1})-V(s_{t})</script><p>所以，计算策略梯度式 $(\ref{12})$ 的关键是计算 $V(s<em>t)$ 。实际上，无法精确计算 **$V(s_t)$ ，通常这是通过神经网络 $V</em>{\phi}(s_t)$ 来近似的（Value net）**。该神经网络会与策略同时进行更新（以便价值网络始终近似于最新策略的值函数）。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqc1z6kmgj321i0n2e2s.jpg" style="zoom:20%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfpvb4qaj30vg0g377r.jpg" style="zoom:50%;" /></p>
<h3 id="Value-net怎么更新"><a href="#Value-net怎么更新" class="headerlink" title="Value net怎么更新"></a>Value net怎么更新</h3><p>Value net的目的是估计 $V(s<em>t)$ ，那么最小化它们之间的均方误差就可以作为一个监督信号，用来在神经网络中反向传播学习 $V</em>{\phi}$ 。</p>
<script type="math/tex; mode=display">
\begin{eqnarray*}
L(\phi) &=& \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-V(s_{i,t}) \right\|}^2 \\&=& \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-\left(r(s_{i,t},a_{i,t})+V_{\phi}(s_{i,t+1})\right) \right\|}^2
\end{eqnarray*}</script><p>至此，Value net可以通过训练，很好的估计 $V(s_t)$ 。将它替代VPG中的 $r(\tau)$ 作为”权重“，指引策略网络学习最优策略。同时也达到了本节开始提出的目的——找出一个能够单独衡量每一个 $(s, a)$ 好坏的“权重”。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqdcs4up3j30s60dvgnb.jpg" style="zoom:50%;" /></p>
<h2 id="Actor-Critic算法"><a href="#Actor-Critic算法" class="headerlink" title="Actor-Critic算法"></a>Actor-Critic算法</h2><p>AC算法的大体框架是这样的：我们定义两个神经网络：一个是用来计算 $V_{\phi}(s_t)$ 价值网络，另一个则是策略网络。我们用策略网络与环境交互产生许多数据集，并用这些数据集同时训练两个网络，提升网络的性能。</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqemesrcyj31bs0rqdpk.jpg" alt="" class="lazyload"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Actor-Critic</th>
</tr>
</thead>
<tbody>
<tr>
<td>构造并初始化Value net的参数 $w$ 和Policy net的参数 $\phi$</td>
</tr>
<tr>
<td>重复以下步骤：</td>
</tr>
<tr>
<td>- 通过Policy net与环境交互产生数据集 $(s,a,r,s’)$</td>
</tr>
<tr>
<td>- 训练Value net：让 $\phi$ 沿着使损失 $L(\phi)=\left\</td>
<td>V<em>{\phi}(s)-(r+V</em>{\phi}(s’)) \right\</td>
<td>^2$ 下降的方向前进</td>
</tr>
<tr>
<td>- 训练Pollicy net：让 $w$ 沿着梯度 $\nabla_wJ(w)$ 的方向前进</td>
</tr>
<tr>
<td>直到收敛</td>
</tr>
</tbody>
</table>
</div>

  </article>

  
    
  <div class="nexmoe-post-copyright">
    <strong>本文作者：</strong>苏苇<br>
    <strong>本文链接：</strong><a href="https://github.com/Su-Lemon/blog.git/2021/02/15/%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2/" title="https:&#x2F;&#x2F;github.com&#x2F;Su-Lemon&#x2F;blog.git&#x2F;2021&#x2F;02&#x2F;15&#x2F;%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;github.com&#x2F;Su-Lemon&#x2F;blog.git&#x2F;2021&#x2F;02&#x2F;15&#x2F;%E6%B5%8B%E8%AF%95%E5%8D%9A%E5%AE%A2&#x2F;</a><br>
    
      <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
    
  </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
    
    
</div>

  <div class="nexmoe-post-footer">
    <section class="nexmoe-comment">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css">
<div id="gitalk"></div>
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '2543206578680a1bdf99',
        clientSecret: '24e25a0277329218d91186c354b4a9d3ec9eaf29',
        id: window.location.pathname,
        repo: 'Su-Lemon.github.io',
        owner: 'Su-Lemon',
        admin: 'Su-Lemon'
    })
    gitalk.render('gitalk')
</script>
</section>
  </div>
</div>
        <div class="nexmoe-post-right">
          
            <div class="nexmoe-fixed">
              <div class="nexmoe-tool">
                <a href="#"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
              </div>
            </div>
          
        </div>
    </div>
  </div>
  <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js?v=1"></script>
<script>
	hljs.initHighlightingOnLoad();
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>


<script src="https://cdn.jsdelivr.net/gh/xtaodada/xtaodada.github.io@0.0.2/copy.js"></script>
 <script src="/blog/js/app.js?v=1613394311202"></script>

<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>

  





</body>

</html>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【RL落地方法论6】模型训练</title>
      <link href="/blog/post/6aa0d565.html"/>
      <url>/blog/post/6aa0d565.html</url>
      
        <content type="html"><![CDATA[<p>为了保证DRL算法能够顺利收敛，policy性能达标并具有实用价值，结果有说服力且能复现，需要算法工作者在训练前、训练中和训练后提供全方位一条龙服务。GANs刚火起来的时候，因为训练难度高，有人在GitHub上专门开了repository，总结来自学术界和工业界的最新训练经验，各种经过或未经验证的tricks被堆砌在一起，吸引了全世界AI爱好者的热烈讨论，可谓盛况空前。在玄学方面，DRL算法训练有得一拼。但毕竟在科研领域没有人真的喜欢玄学，只有久经考验的一般化规律才能凝结成知识被更多的人接受和推广。本篇接下来的内容融合了许多参考资料，算是在DRL训练“去玄学”化上做出的一点微不足道的努力。</p><a id="more"></a><h2 id="训练开始前">训练开始前</h2><h3 id="环境可视化">环境可视化</h3><p>如果条件允许，开始训练前最好先可视化一个随机环境，观察是否会出现你希望的状态（即上一篇里的主线事件）。如果靠随机选择action都能以一定概率探索到目标状态，那说明该任务难度比较低，心里就可以更有底；如果从来不会出现目标状态，说明该任务难度较高，需要在状态空间和reward函数设计时特别下功夫，从而更好地引导agent向目标状态前进。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225210423.png" /></p><h3 id="数据预处理">数据预处理</h3><p>你还可以实时打印出state和reward，看看它们是否在合理范围内取值，是否存在幅值过大的情况，如果是则需要增加必要的归一化操作。事实上，我推荐无条件进行状态空间归一化和reward rescale &amp; clipping，实践证明这两个操作无论在收敛速度还是最终性能上都会带来明显提升。前一个操作很好理解，我只介绍一下reward rescale &amp; clipping，该操作尤其适合基于episode的A3C/A2C/PPO算法，参考形式为 <span class="math inline">\(r=clip(r/(std(Return)+\epsilon ),-10,10)\)</span> ，其中 <span class="math inline">\(Return = \sum\limits_{t=0}^{T} \gamma^{t}r_{t}\)</span> ，是一段episode内reward的折扣累加和，也就是V网络拟合的对象，而V网络输出又为policy优化提供参考，使用该值的统计方差对reward进行rescale，可以反过来有效降低Return的variance，有助于V网络和policy网络进行更加无偏地学习。训练过程中通常采用Return的running std来rescale当前reward。最外层的clip操作可以滤除那些绝对值过大的reward，作用类似。</p><p><strong>注意reward只能进行rescale，而不能整体平移（减去均值）</strong>。回报函数中各项reward的符号以及它们之间的相对大小唯一确定了回报函数的实际功能，各项reward的整体缩放对其没有影响，但整体平移会改变这种相对大小，也就改变了回报函数的功能。事实上，哪怕是clip操作也在一定程度上存在这种问题，但通常影响不大。</p><h2 id="训练进行中">训练进行中</h2><h3 id="拥抱不确定性">拥抱不确定性</h3><p>终于要开始调参了！如果你做过CV项目，会发现相对来说DRL训练的不确定性更高，可复现性更差。这是因为DRL算法不仅超参数多，而且对它们非常敏感，这里贴张图给大家感受一下。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225210852.png" /></p><p>上图是三个DRL算法（纵向）在五个Atari游戏（横向）中的得分随学习率变化的趋势。可以看到，以10倍为单位，高性能所对应的学习率区间普遍很窄，要达到最优性能真得靠地毯式搜索。其实原作者本意是想通过这张图表明他们的方法对超参数变化抵抗力较好……，况且这还只是针对学习率这一个超参数。</p><p>当我们刚开始尝试用DRL算法解决一个全新问题时，性能好坏甚至都是其次，能否收敛才是最关键的。大佬OpenAI Five都有所耳闻吧？其团队成员在接受采访时承认，他们第一次将训练跑起来后因为心里实在没底，干脆全体度假去了，回来后打开屏幕发现竟然收敛了，上帝保佑！</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225211559.png" /></p><p>尽管DRL对超参数如此敏感，也没有必要过分悲观。当我们在心理上接受了这一事实，并开始潜心研究时，就会发现DRL训练还是有迹可循的，尤其是随着实际经验的积累和对算法本质理解的不断深入，将每个超参数的作用都了然于胸，那么训练出优秀policy的可能性就会大很多。接下来我以DQN，DDPG和PPO为例，介绍一下其中的主要超参数和调参技巧。</p><h3 id="drl通用超参数">DRL通用超参数</h3><p>有些超参数不是某个算法所特有，而是DRL普遍使用的，而且作用原理和设置依据都差不多，我放到最前面集中介绍。典型的通用超参数包括折扣因子、网络结构和学习率。</p><h4 id="折扣因子">折扣因子</h4><p><strong>1.作用原理</strong></p><p>折扣因子通常以符号γ表示，在强化学习中用来调节近远期影响，即agent做决策时考虑多长远，取值范围 <span class="math inline">\((0,1]\)</span> 。 <span class="math inline">\(\gamma\)</span> 越大agent往前考虑的步数越多，但训练难度也越高；<span class="math inline">\(\gamma\)</span> 越小agent越注重眼前利益，训练难度也越小。我们都希望agent能“深谋远虑”，但过高的折扣因子容易导致算法收敛困难。还以小车导航为例，由于只有到达终点时才有奖励，相比而言惩罚项则多很多，在训练初始阶段负反馈远多于正反馈，一个很高的折扣因子（如0.999）容易使agent过分忌惮前方的“荆棘丛生”，而宁愿待在原地不动；相对而言，一个较低的折扣因子（如0.9）则使agent更加敢于探索环境从而获取抵达终点的成功经验；而一个过低的折扣因子（如0.4），使得稍远一点的反馈都被淹没了，除非离终点很近，agent在大多数情况下根本看不到“光明的未来”，更谈不上为了抵达终点而努力了。</p><p><strong>2.选取方法</strong></p><p>总之，<strong>折扣因子的取值原则是，在算法能够收敛的前提下尽可能大</strong>。在实践中，有个经验公式 <span class="math inline">\(1/(1-\gamma)\)</span> ，可以用来估计agent做决策时往前考虑的步数。根据对特定任务的分析，合理选择 <span class="math inline">\(\gamma\)</span> 值，避免“近视”和“远视”。比如可以根据观察或统计agent到达终点所需的步数分布，选择合适的步数使得agent在该步数内的探索下有一定概率到达终点（正样本），注意这个概率越高训练难度就越小，然后利用经验公式把该步数换算成 <span class="math inline">\(\gamma\)</span> 即可。</p><p><strong>3.Frame Skipping</strong></p><p>上述折扣因子的选择方法并非无往不利，有时我们会面临这样的窘境：无法设置合理的γ在“近视”和“远视”间找到满意折中，常见于“细粒度”复杂任务。复杂决定了agent需要看得很远才能做出合理决策，而“细粒度”指agent决策间隔很短以至于一段较长的episode只对应较少的agent状态变化。仍以小车导航为例，0.1s的决策间隔显然满足了实时防碰撞的机动性要求，但也大大延长了episode长度，即到达终点所需步数。如果agent平均需要 <span class="math inline">\(1min\)</span> 才能到达终点，那就要求向前考虑 <span class="math inline">\(1min/0.1s=600\)</span> 步，按照经验公式计算合理的折扣因子 <span class="math inline">\(\gamma \approx 1-1/600=0.998\)</span> ，如此高的折扣因子+如此长的episode，训练难度可想而知。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225211751.png" /></p><p>假如我们在保证足够机动性的前提下适当延长决策间隔，比如0.5s，中间4帧重复上一次决策的action不变，相当于跳了几帧达到“快进”效果，从而使episode长度大大缩短，训练难度也直线下降，即使采用较高折扣因子也能顺利收敛。通过这个例子我们也可以知道，<strong>在DRL中agent“看得远”表面上指的是向前考虑的步数多，实质上是指agent向前考虑的系统动态演化跨度大</strong>。</p><h4 id="网络结构">网络结构</h4><p>DRL算法中的网络结构也属于超参数，然而与CV任务不同，DRL绝不应该片面追求网络的复杂化，否则你会发现训练根本无法收敛。对于网络结构的选择，DRL有自己的规矩——<strong>契合状态，够用就好</strong>。前者针对网络类型，后者针对网络深度。</p><p><strong>1.网络类型</strong></p><p>网络类型的选择主要取决于状态空间设计，如果状态信息是向量式的，即一组拉成一维的标量，比如位置、角度、速度等，那就适合采用全连接（MLP）网络；如果状态信息是imagelike的，比如图像，或者其他以二维形式重组的信息，就适合采用卷积神经网络（CNN）。实际应用中往往同时包含这两种状态信息，因此网络类型也可以既有CNN也有MLP，处理完各自对应的输入信息后，在高层通过concat操作汇集在一起，再通过若干层全连接，最后输出action或Q/V值。</p><p>对于on-policy算法，episode形式的数据天然适合采用RNN来挖掘更多时序信息，但同时也会显著提高训练难度，用与不用取决于决策对时序相关性的依赖程度。换句话说，如果之前的经验对当前决策很有参考意义（比如Dota）就适合用RNN，反之仅依靠即时信息做应激式决策就足以应付就没必要用RNN。实践中经常采取折中方案，将最近几个step的原始状态信息叠加到一起作为当前时刻的实际状态信息输入policy，既可以挖掘一定范围内的时序信息，又避免增加训练难度。</p><p><strong>2.网络深度</strong></p><p>至于网络深度，千万不要认为越深越好，虽然深层网络的表征能力更强，但训练难度非常高，更适合有监督训练。DRL算法由于数据效率低下又缺乏直接监督信号，并不擅长以end-to-end的方式训练过深的网络，如果还同时采用了RNN结构，那就是相当不擅长了。除非你有DeepMind或OpenAI那样的硬件资源，否则还是现实点好。其实多读几篇DRL方向的paper就会发现，所谓deep往往只是2-3层MLP或4-5层CNN，虚张声势的背后就是这么知趣。</p><p>当然，如果任务逻辑和状态信息确实非常复杂，浅层网络不足以提供所需的特征提取和加工能力，那么可以考虑适当加深网络，但仍应以够用为准则，不可矫枉过正。我曾经试过把已经work的3层MLP改成10层，发现根本就不收敛或收敛极慢，后来在各层中加了类似ResNet的跳线，勉强收敛，但相同训练量下性能还不如3层网络。如果输入状态信息是ImageNet那样的自然图像，可以像视觉检测应用那样，先用有监督方式预训练一个backbone，然后再放到DRL里finetune。</p><h4 id="学习率">学习率</h4><p>从整个AI技术来看，学习率是如此的平淡无奇，相关调参技巧早就被研究透了。毕竟DRL算法里的学习率也遵循同样的基本法——大了收敛快，稳定性差，且后期影响性能；小了收敛慢，浪费时间。学习率常用的淬火操作也同样可以应用到DRL中。当然，对于不同DRL算法而言，学习率也可能有各自的特点，如有必要，我会在下文介绍DRL特色超参数的时候顺带多说几句。</p><h3 id="drl特色超参数">DRL特色超参数</h3><h4 id="dqn">DQN</h4><p>DQN的特色超参数主要有：buffer size，起始训练时间，batchsize，探索时间占比，最终epsilon，目标网络更新频率等。</p><p>Buffer size指的是DQN中用来提高数据效率的replay buffer的大小。通常取 <span class="math inline">\(1e6\)</span> ，但不绝对。Buffer size过小显然是不利于训练的，replay buffer设计的初衷就是为了保证正样本，尤其是稀有正样本能够被多次利用，从而加快模型收敛。对于复杂任务，适当增大buffer size往往能带来性能提升。反过来过大的buffer size也会产生负面作用，由于标准DQN算法是在buffer中均匀采集样本用于训练，新旧样本被采集的概率是相等的，如果旧样本或者无效样本在buffer中存留时间过长，就会阻碍模型的进一步优化。总之，合理的buffer size需要兼顾样本的稳定性和优胜劣汰。顺便说一句，针对“等概率采样”的弊端，学术界有人提出了prioritized replay buffer，通过刻意提高那些loss较大的transition被选中的概率，从而提升性能，这样又会引入新的超参数，这里就不做介绍了。</p><p>起始训练时间的设置仅仅是为了保证replay buffer里有足够的数据供二次采样，因此与batchsize有直接关系，没啥可说的。Batchsize指的是从replay buffer中二次采样并用于梯度计算的batch大小，和CV任务中的设定原则基本一致，即兼顾训练稳定性和训练速度，也没啥好说的。</p><p>探索时间占比和最终 <span class="math inline">\(\epsilon\)</span> 共同决定了DQN探索和利用的平衡。ε-greedy策略在训练开始的时候，随机选择action的概率 <span class="math inline">\(\epsilon=1\)</span> ，探索力度最大；随着训练进行ε逐渐线性下降直至达到最终epsilon保持恒定，之后DQN的训练将以利用为主而只保留少量探索。因此，最终 <span class="math inline">\(\epsilon\)</span> 取值在区间 <span class="math inline">\([0,1]\)</span> 内靠近0的一端。探索时间占比指的是 <span class="math inline">\(\epsilon\)</span> 从1下降到最终 <span class="math inline">\(\epsilon\)</span> 的时间占总训练时间的比例，在 <span class="math inline">\((0,1)\)</span> 内取值，用来调节以探索为主到以利用为主的过渡。通常来说，复杂任务的探索时间占比应设得大一些，以保证充分的探索；最终 <span class="math inline">\(\epsilon\)</span> 不宜过大，否则影响模型最终阶段“好上加好”的性能冲刺，因为最好的状态往往是在足够好的Q网络指导下才能探索到的，训练后期过强的探索干扰了习得知识的利用，也就阻碍了性能的进一步提升。</p><p>标准DQN引入了一个延迟更新的目标网络用来计算Q的目标值，避免Q网络误差的“自激效应”，并借此来提高训练稳定性。目标网络更新频率就是用来控制这个延迟程度的，时间到了就把Q网络的参数整个复制过来。通常情况下根据具体问题，参考Q网络的更新周期设定，比如Q网络每1个step更新一次，目标Q网络可以设定每500个step更新一次。</p><h4 id="ddpg">DDPG</h4><p>DDPG的特色超参数主要包括：buffer size，batchsize，目标网络软更新参数τ，探索噪声等。其中很多超参数与DQN类似，比如buffer size和batchsize，这里就不重复介绍了。</p><p>DDPG也使用了目标网络（目标Q网络和目标Policy网络）稳定训练，不同的是DDPG的目标网络与主网络更新频率相同，稳定效果来自于软更新（soft-update），即 <span class="math inline">\((1-\tau)*target + \tau*main\)</span> ，<span class="math inline">\(\tau\)</span> 取很小的值（DDPG paper中建议0.001）限制每次更新的幅度。</p><p>DDPG值得特别介绍的是探索噪声及其参数。由于policy网络输出确定性action，DDPG的探索依靠在输出action空间叠加噪声来实现。可选的噪声类型主要包括Gaussian噪声和DDPG paper推荐的ou噪声（Ornstein-Uhlenbeck），后者相对于前者主要是增加了噪声强度逐渐衰减的功能。这两种噪声的主要参数是噪声方差，方差越大探索力度越强。虽然论文推荐使用ou噪声，但我在实践中发现ou噪声并不一定比Gaussian噪声效果好，还是要看具体任务。</p><p>后来DeepMind又提出了adaptive parameter noise，抛弃了在输出层叠加噪声的方法，转而采用在policy网络靠近输出的若干层网络参数上叠加噪声，优点是探索更充分，毕竟参数噪声的影响范围更大，而且可以根据实际情况自适应调节探索力度。类似地，我发现参数噪声在有些任务上效果不错，但在另一些任务中不如传统噪声。综上所述，关于不同噪声的优劣没有确定性结论（局限于我的个人经验），具体选择哪种噪声，还要实际试过才知道。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225211849.png" /></p><p>此外，值得一提的是Q网络和policy网络采用了不同的学习率，且一般Q网络的学习率比policy网络大一个数量级，比如前者用 <span class="math inline">\(1e-3\)</span> ，后者用 <span class="math inline">\(1e-4\)</span> 。这样做的原因是，用于更新policy网络的梯度完全来自于Q网络，两者地位不是对等的。</p><h4 id="ppo">PPO</h4><p>作为on-policy方法，PPO与前两种DRL框架有很大不同，无论是算法原理还是超参数设置。PPO的特色超参数包括：采样环境数量，episode长度，entropy系数，V网络系数，GAE factor，PPO cliprange等。</p><p>并行采样的环境数量越多，整体的探索效率越高，绝对收敛时间越快，该参数的设置主要取决于可用的硬件资源。</p><p>PPO的训练基于episode（或trajectory），将其中每个中间state到episode结束时的Return作为目标值拟合一个V网络，并用V网络作为baseline指导policy网络的更新。为了便于训练，通常每个环境都采集固定长度的episode并返回主进程中拼成一个batch。Episode越长，每次计算梯度时的数据量越大，但消耗内存也越多。Episode长度通常取4096, 2048, 1024等2的次幂，原因是更新网络参数时整个batch还会再分成minibatch（2的次幂比较好分），遍历若干个epoch，从而提高数据利用率，注意minibatch不能太大，否则有可能导致“学不动”的现象。在实际应用中，除了考虑内存开销，episode长度选取也跟任务难度息息相关。以小车导航为例，训练刚开始时agent可能需要探索很久才能幸运地抵达终点，episode长度最好能囊括整个探索过程，这样中间状态与理想状态（到终点）间的演进关系就很容易学习到。当然，episode不可能无限长，如果探索难度实在太高，那也只好提前终止探索，把截断的部分放到下一个episode中。</p><p>PPO算法的loss由三部分组成：policy loss，value loss和entropy loss。其中entropy loss项的系数是一个非常重要的超参数，对收敛速度和最终性能有直接影响。我在算法选择篇介绍PPO的探索-利用平衡时，说过随着训练进行policy输出的action分布的variance会越来越小，反映到统计指标上就是entropy越来越小。这本来是一个自然发生的过程，不需要人的干预，然而DRL训练早期往往受到各种local minima的干扰，容易陷入“拣了芝麻丢了西瓜”的怪圈。为了避免模型过早迷失方向，PPO加入了entropy loss用于强迫policy输出不那么“尖锐”的action分布，从而起到加强探索的效果。Entropy系数负责调节这种“强迫”力度，合理的系数既能确保训练早期充分探索从而使模型向正确方向前进，又能使模型在训练中后期充分利用学到的技能从而获得高性能。对于不同任务，最优entropy系数往往各不相同，需要若干次试错才能找到。比如在训练开始后policy entropy快速下降说明模型陷入了局部最优，根本没学到有用技能，这时就应该提升entropy系数；如果训练很长时间policy entropy仍然未下降或者下降缓慢，说明模型探索过头了，学到的知识被随机性淹没，无法进一步用来提升性能，此时应该适当降低entropy系数。</p><p>V网络系数是PPO loss中value loss项的系数，通常取0.5（policy loss系数默认是1），在实践中不太需要修改。</p><p>由于on-policy算法对数据的使用方式是“<strong>现采现用，用完就扔</strong>”。为了防止policy跑偏，在错误道路上越走越远，需要通过特定方法限制其每次参数更新的幅度。PPO与更早的TRPO类似，核心思想都是针对更新前后policy输出的KL散度设定阈值，但PPO通过一个简单的clip操作大大简化了运算，兼顾了效率和性能。PPO相关的参数主要是cliprange，通常取略大于0的小数，代表使policy更新前后KL散度控制在1-cliprange到1+cliprange之间，超出该范围的梯度直接被忽略（相当于对应数据被弃用）。Cliprange越小训练越稳定，越大越节省数据。一般在训练早期取较小的值，比如0.2，保证训练平稳进行；到训练后期可以适当放大，因为此时policy已经足够优秀，所采集数据中正样本比例非常高，可以放心利用。此外，PPO使用了GAE来估计advantage，相应增加了一个超参数GAE factor，用于在bias和variable之间寻求平衡，也是在(0,1]内取值，一般都默认取0.95。</p><h3 id="给drl初学者的建议">给DRL初学者的建议</h3><p><strong>以热门平台作为切入点</strong>。热门平台维护频率高，关注度高，遇到问题容易找到解决方案。比如在OpenAI Gym平台上找些简单的任务多练手，baselines里提供了大量参考代码。使用现成代码训练的最大好处是可以排除bug的干扰，把注意力都放在调参上，这对于初学者非常重要。</p><p><strong>不要迷信默认超参数。</strong>在实际应用中可以先采用参考代码里的默认超参数，但一定要记住它们不是万能的。可以用tensorboard显示出各种训练曲线，理解其中主要曲线的含义和作用，并根据这些曲线判断超参数设置是否合理，应该朝哪个方向调整。</p><p><strong>重视首次成功经验，学会简化问题</strong>。实践多了你会发现，对于特定Domain和特定算法，最优超参数组合分布都各有特点，最困难也最关键的是首次训练，一旦有了成功经验，后续根据观察结果做些微调就能得到高性能模型了。如果原任务训练难度太高，可以先尝试做适当简化，待成功收敛后再逐渐恢复任务难度。比如要实现任务目标A，必须满足条件B,C,D…，可以先暂时去掉对C,D,…的要求，只保留B，这样探索到A的概率就会显著提升，算法训练难度直线下降，待算法收敛并获得一定的经验后再逐步恢复所有条件。</p><p><strong>保持耐心。</strong>DRL训练本来就挺慢的，很多时候除了等待什么都不用做。除非你对相关Domain的算法训练流程已经很熟悉，否则不要轻易断定算法不收敛，可以等等再说。也不要整天一动不动地盯着屏幕发呆，既浪费时间，更浪费生命，间歇式检查一下就好。</p><h2 id="训练收敛后">训练收敛后</h2><p>当你发现policy已经可以如预期的那样完成任务了，先不要忙着喜形于色，你还需要做些检查和分析工作以确保policy性能达到了最优。以A3C为例，你至少应该：</p><ol type="1"><li><p><strong>观察Value网络对Returns拟合的精度如何，value loss是否还有进一步下降的空间？</strong></p><p>Value网络越精确，由其计算得到的advantage越有意义，也就越有利于policy的优化。注意精度和loss都是相对概念，与reward函数中各项的绝对值息息相关。一般说来，在DRL中对reward进行等比例缩放不会改变policy的最终特性，即 <span class="math inline">\((+10,-2,-1,-0.5)\)</span> 与 <span class="math inline">\((+100,-20,-10,-5)\)</span> 的作用是一样的，但体现在value loss上就差了10倍。对拟合精度更可靠的评估标准是explained variance，计算公式是 <span class="math inline">\(1-Var(return-value)/Var(return)\)</span> 取值区间 <span class="math inline">\((-\infty,1]\)</span> ，该值越接近1说明拟合精度越高。建议训练过程中将该值实时打印到tensorboard中，并不断监测Value网络的质量。</p></li><li><p><strong>观察entropy是否处在合理范围内，相对于action维度是否过高或过低？</strong></p></li></ol><p>假如policy输出10维categorical分布，其entropy有两种极端情况：(1) 完全随机，每个维度概率均为0.1，此时entropy最大等于 <span class="math inline">\(10*[-0.1*log(0.1)]=2.3\)</span> ；(2) 完全确定，其中一维为1.0其余都是0.0，此时entropy最小等于0。整个训练过程，entropy从2.3开始逐渐下降，当训练收敛后，entropy应该稳定在较低水平。如果太高则说明policy对决策信心不足，如果不是任务本身太复杂那就是entropy系数过大造成的，应该适当降低该系数增加exploitation的力度，很有可能继续提升模型性能。当然，entropy很少能降到0，除非是极其简单的任务。</p><h2 id="总结">总结</h2><p>经过前后近一个月零零星星的整理，这篇又臭又长的训练篇终于快要结束了，连我自己都觉得枯燥透顶，如果有哪位读者能坚持读到这里，我敬你是个勇士！我也时常怀疑写这些东西到底有没有意义，毕竟包括DRL在内的深度学习调参技巧往往琐碎而不成体系，很难总结得面面俱到，更何况新算法还在源源不断地涌现，旧的知识经验正在迅速“贬值”，就像现在有了Soft Actor-Critic，谁还用DDPG啊。最重要的是，假如读者不经过亲身实践，直接看这些干巴巴的总结，作用真心不大。对我自己来说，就权当备忘吧~</p><p>事实上，当你通过广泛阅读和动手实践，对各种DRL算法原理有了深入理解，对各种超参数的作用了然于胸，自然而然就会形成自己的调参方法论。只要算法收敛，性能达标，项目验收，调参的细节没那么重要。此外，调参工作毕竟只停留在“术”的层面，而我们应该追求的是算法之“道”，孰轻孰重每个人都要心里有数。祝愿每一个算法工程师最终都能做到“<strong>调尽千参，心中无参</strong>”。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python编程】GUI编程tkinter基础，并实现一个目录树遍历工具</title>
      <link href="/blog/post/56c2233b.html"/>
      <url>/blog/post/56c2233b.html</url>
      
        <content type="html"><![CDATA[<p>本文将对图形用户界面（Graphical User Interface，GUI）编程进行简要的介绍。我们将主要使用的GUI 工具包是Python 默认的GUI 库Tk，通过Python 的接口tkinter（“Tk interface”的缩写）可以访问Tk。 Tk 并不是最新和最好的，也没有包含最强大的GUI 构建模块集，但是它足够易用，你可以使用它构建能够运行在大多数平台下的GUI。</p><a id="more"></a><h2 id="引言">引言</h2><h3 id="tcltk和tkinter">Tcl、Tk和tkinter</h3><p>tkinter 是Python 的默认GUI 库。它基于Tk工具包，该工具包最初是为工具命令语言（Tool Command Language，Tcl）设计的。Tk 普及后，被移植到很多其他的脚本语言中，包括Perl（Perl/Tk）、Ruby（Ruby/Tk）和Python（tkinter）。结合Tk 的GUI 开发的可移植性与灵活性，以及与系统语言功能集成的脚本语言的简洁性，可以让你快速开发和实现很多与商业软件品质相当的GUI 应用。</p><h3 id="安装和使用tkinterpython3">安装和使用tkinter（python3）</h3><p>tkinter 在系统中不是默认必须安装的，可以通过在Python 解释器中尝试导入Tkinter 模块（Python 1 和2 版本，在Python 3 中重命名为tkinter）来检查Tkinter 是否可用。</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> tkinter</span></code></pre></div><h2 id="tkinter和python编程">tkinter和python编程</h2><h3 id="tkinter模块添加tk到应用中">tkinter模块：添加Tk到应用中</h3><p>为了让tkinter 成为应用的一部分，你需要做些什么呢？让GUI 程序启动和运行起来需要以下5 个主要步骤：</p><ol type="1"><li>导入Tkinter 模块（或from Tkinter import *）。</li><li>创建一个顶层窗口对象，用于容纳整个GUI 应用。</li><li>在顶层窗口对象之上（或者“其中”）构建所有的GUI 组件（及其功能）。</li><li>通过底层的应用代码将这些GUI 组件连接起来。</li><li>进入主事件循环。</li></ol><h3 id="gui-编程介绍">GUI 编程介绍</h3><p>创建一个GUI 应用就像艺术家作画一样。必须在搭建起画架之后，才能把画布拼装在上面。在tkinter 中，这个“画架”基础称为<strong>顶层窗口对象</strong>。</p><h4 id="窗口和控件">窗口和控件</h4><p>在GUI 编程中，顶层的根窗口对象包含组成GUI 应用的所有小窗口对象。它们可能是文字标签、按钮、列表框等。这些独立的GUI 组件称为控件。所以当我们说创建一个顶层窗口时，只是表示需要一个地方来摆放所有的控件。在Python 中，一般会写成如下语句。</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>top <span class="op">=</span> tkinter.Tk() <span class="co"># or just Tk() with &quot;from Tkinter import *&quot;</span></span></code></pre></div><p>tkinter.Tk()返回的对象通常称为根窗口，这也是一些应用使用root 而不是top 来指代它的原因。顶层窗口是那些在应用中独立显示的部分。GUI 程序中可以有多个顶层窗口，但是其中只能有一个是根窗口。可以选择先把控件全部设计好，再添加功能；也可以边设计控件边添加功能（这意味着上述步骤中的第3 步和第4 步会混合起来做）。</p><p>控件可以独立存在，也可以作为容器存在。如果一个控件包含其他控件，就可以将其认为是那些控件的父控件。相应地，如果一个控件被其他控件包含，则将其认为是那个控件的子控件，而父控件就是下一个直接包围它的容器控件。</p><p>通常，控件有一些相关的行为，比如按下按钮、将文本写入文本框等。这些用户行为称为事件，而GUI 对这类事件的响应称为回调。</p><h4 id="事件驱动处理">事件驱动处理</h4><p>事件可以包括按钮按下（及释放）、鼠标移动、敲击回车键等。一个GUI 应用从开始到结束就是通过整套事件体系来驱动的。这种方式称为事件驱动处理。最简单的鼠标移动就是一个带有回调的事件的例子。假设鼠标指针正停在GUI 应用顶层窗口的某处。如果你将鼠标移动到应用的另一部分，鼠标移动的行为会被复制到屏幕的光标上，于是看起来像是根据你的手移动的。系统必须处理的这些鼠标移动事件可以绘制窗口上的指针移动。当释放鼠标时，不再有事件需要处理，此时屏幕会重新恢复闲置的状态。</p><p>事件驱动的GUI 处理本质上非常适合于客户端/服务端架构。当启动一个GUI 应用时，需要一些启动步骤来准备核心部分的执行，就像网络服务器启动时必须先分配套接字并将其绑定到本地地址上一样。GUI 应用必须先创建所有的GUI 组件，然后将它们绘制在屏幕上。这是布局管理器（geometry manager）的职责所在。当布局管理器排列好所有控件（包括顶层窗口）后，GUI 应用进入其类似服务器的无限循环。这个循环会一直运行，直到出现GUI 事件，进行处理，然后再等待更多的事件去处理。</p><h4 id="布局管理器">布局管理器</h4><p>Tk 有3 种布局管理器来帮助控件集进行定位。最原始的一种称为Placer。它的做法 非常直接：你提供控件的大小和摆放位置，然后管理器就会将其摆放好。问题是你必须对所有控件进行这些操作，这样就会加重编程开发者的负担，因为这些操作本应该是自动完成的。</p><p>第二种布局管理器会是你主要使用的，它叫做Packer，这个命名十分恰当，因为它会把控件填充到正确的位置（即指定的父控件中），然后对于之后的每个控件，会去寻找剩余的空间进行填充。这个处理很像是旅行时往行李箱中填充行李的过程。</p><p>第三种布局管理器是Grid。你可以基于网格坐标，使用Grid 来指定GUI 控件的放置。Grid 会在它们的网格位置上渲染GUI 应用中的每个对象。本章将使用Packer。</p><p>一旦Packer 确定好所有控件的大小和对齐方式，它就会在屏幕上将其放置妥当。</p><p>当所有控件摆放好后，可以让应用进入前述的无限主循环中。在Tkinter 中，代码如下所示。</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>tkinter.mainloop()</span></code></pre></div><p>一般这是程序运行的最后一段代码。当进入主循环后，GUI 就从这里开始接管程序的执行。所有其他行为都会通过回调来处理，甚至包括退出应用。当选择File 菜单并单击Exit 菜单选项，或者直接关闭窗口时，就会调用一个回调函数来结束这个GUI 应用。</p><h3 id="tk控件">Tk控件</h3><table><thead><tr class="header"><th>控 件</th><th>描 述</th></tr></thead><tbody><tr class="odd"><td>Button</td><td>与Label 类似，但提供额外的功能，如鼠标悬浮、按下、释放以及键盘活动/事件</td></tr><tr class="even"><td>Canvas</td><td>提供绘制形状的功能（线段、椭圆、多边形、矩形），可以包含图像或位图</td></tr><tr class="odd"><td>Checkbutton</td><td>一组选框，可以勾选其中的任意个（与HTML 的checkbox 输入类似）</td></tr><tr class="even"><td>Entry</td><td>单行文本框，用于收集键盘输入（与HTML 的文本输入类似）</td></tr><tr class="odd"><td>Frame</td><td>包含其他控件的纯容器</td></tr><tr class="even"><td>Label</td><td>用于包含文本或图像</td></tr><tr class="odd"><td>LabelFrame</td><td>标签和框架的组合，拥有额外的标签属性</td></tr><tr class="even"><td>Listbox</td><td>给用户显示一个选项列表来进行选择</td></tr><tr class="odd"><td>Menu</td><td>按下Menubutton 后弹出的选项列表，用户可以从中选择</td></tr><tr class="even"><td>Menubutton</td><td>用于包含菜单（下拉、级联等）</td></tr><tr class="odd"><td>Message</td><td>消息。与Label 类似，不过可以显示成多行</td></tr><tr class="even"><td>PanedWindow</td><td>一个可以控制其他控件在其中摆放的容器控件</td></tr><tr class="odd"><td>Radiobutton</td><td>一组按钮，其中只有一个可以“按下”（与HTML 的radio 输入类似）</td></tr><tr class="even"><td>Scale</td><td>线性“滑块”控件，根据已设定的起始值和终止值，给出当前设定的精确值</td></tr><tr class="odd"><td>Scrollbar</td><td>为Text、Canvas、Listbox、Enter 等支持的控件提供滚动功能</td></tr><tr class="even"><td>Spinbox Entry</td><td>和Button 的组合，允许对值进行调整</td></tr><tr class="odd"><td>Text</td><td>多行文本框，用于收集（或显示）用户输入的文本（与HTML 的textarea 类似）</td></tr><tr class="even"><td>Toplevel</td><td>与Frame 类似，不过它提供了一个单独的窗口容器</td></tr></tbody></table><h2 id="tkinter基本示例">tkinter基本示例</h2><h3 id="label控件">Label控件</h3><div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="im">import</span> tkinter <span class="im">as</span> tk</span><span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a></span><span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>top <span class="op">=</span> tk.Tk()</span><span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span><span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>label <span class="op">=</span> tk.Label(top, text<span class="op">=</span><span class="st">&#39;hello&#39;</span>)</span><span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>label.pack()</span><span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a>tk.mainloop()</span></code></pre></div><h3 id="button控件">Button控件</h3><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="im">import</span> tkinter <span class="im">as</span> tk</span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span><span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>top <span class="op">=</span> tk.Tk()</span><span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a></span><span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>quit <span class="op">=</span> tk.Button(top, text<span class="op">=</span><span class="st">&#39;start&#39;</span>, command<span class="op">=</span>top.quit)</span><span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>quit.pack()</span><span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>tk.mainloop()</span></code></pre></div><h2 id="目录树遍历工具">目录树遍历工具</h2><p>它会从当前目录开始，提供一个文件列表。双击列表中任意其他目录，就会使得工具切换到新目录中，用新目录中的文件列表代替旧文件列表。 <img src="https://img-blog.csdnimg.cn/20200224105058824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p><div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="im">import</span> os</span><span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="im">from</span> time <span class="im">import</span> sleep</span><span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="im">import</span> tkinter <span class="im">as</span> tk</span><span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a><span class="im">from</span> functools <span class="im">import</span> partial <span class="im">as</span> pto</span><span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a></span><span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a></span><span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a><span class="kw">class</span> DirList(<span class="bu">object</span>):</span><span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    <span class="co">&quot;&quot;&quot;读取并显示指定目录下的文件列表&quot;&quot;&quot;</span></span><span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, initdir<span class="op">=</span><span class="va">None</span>):</span><span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>        <span class="va">self</span>.top <span class="op">=</span> tk.Tk()</span><span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>        <span class="va">self</span>.label <span class="op">=</span> tk.Label(<span class="va">self</span>.top, text<span class="op">=</span><span class="st">&#39;DirList v1.1&#39;</span>)</span><span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>        <span class="va">self</span>.label.pack()</span><span id="cb6-13"><a href="#cb6-13" aria-hidden="true"></a></span><span id="cb6-14"><a href="#cb6-14" aria-hidden="true"></a>        <span class="co"># 写入延迟</span></span><span id="cb6-15"><a href="#cb6-15" aria-hidden="true"></a>        <span class="va">self</span>.cwd <span class="op">=</span> tk.StringVar(<span class="va">self</span>.top)</span><span id="cb6-16"><a href="#cb6-16" aria-hidden="true"></a></span><span id="cb6-17"><a href="#cb6-17" aria-hidden="true"></a>        <span class="va">self</span>.dirl <span class="op">=</span> tk.Label(<span class="va">self</span>.top, fg<span class="op">=</span><span class="st">&#39;blue&#39;</span>, font<span class="op">=</span>(<span class="st">&#39;Helvetica&#39;</span>, <span class="dv">12</span>, <span class="st">&#39;bold&#39;</span>))</span><span id="cb6-18"><a href="#cb6-18" aria-hidden="true"></a>        <span class="va">self</span>.dirl.pack()</span><span id="cb6-19"><a href="#cb6-19" aria-hidden="true"></a></span><span id="cb6-20"><a href="#cb6-20" aria-hidden="true"></a>        <span class="va">self</span>.dirfm <span class="op">=</span> tk.Frame(<span class="va">self</span>.top)</span><span id="cb6-21"><a href="#cb6-21" aria-hidden="true"></a>        <span class="va">self</span>.dirsb <span class="op">=</span> tk.Scrollbar(<span class="va">self</span>.dirfm)</span><span id="cb6-22"><a href="#cb6-22" aria-hidden="true"></a>        <span class="va">self</span>.dirsb.pack(side<span class="op">=</span>tk.RIGHT, fill<span class="op">=</span>tk.Y)</span><span id="cb6-23"><a href="#cb6-23" aria-hidden="true"></a>        <span class="va">self</span>.dirs <span class="op">=</span> tk.Listbox(<span class="va">self</span>.dirfm, height<span class="op">=</span><span class="dv">15</span>, width<span class="op">=</span><span class="dv">50</span>,</span><span id="cb6-24"><a href="#cb6-24" aria-hidden="true"></a>                               yscrollcommand<span class="op">=</span><span class="va">self</span>.dirsb.<span class="bu">set</span>)</span><span id="cb6-25"><a href="#cb6-25" aria-hidden="true"></a>        <span class="va">self</span>.dirs.bind(<span class="st">&#39;&lt;Double-1&gt;&#39;</span>, <span class="va">self</span>.setDirAndGo)</span><span id="cb6-26"><a href="#cb6-26" aria-hidden="true"></a>        <span class="va">self</span>.dirsb.config(command<span class="op">=</span><span class="va">self</span>.dirs.yview)</span><span id="cb6-27"><a href="#cb6-27" aria-hidden="true"></a>        <span class="va">self</span>.dirs.pack(side<span class="op">=</span>tk.LEFT, fill<span class="op">=</span>tk.BOTH)</span><span id="cb6-28"><a href="#cb6-28" aria-hidden="true"></a>        <span class="va">self</span>.dirfm.pack()</span><span id="cb6-29"><a href="#cb6-29" aria-hidden="true"></a></span><span id="cb6-30"><a href="#cb6-30" aria-hidden="true"></a>        <span class="va">self</span>.dirn <span class="op">=</span> tk.Entry(<span class="va">self</span>.top, width<span class="op">=</span><span class="dv">50</span>, textvariable<span class="op">=</span><span class="va">self</span>.cwd)</span><span id="cb6-31"><a href="#cb6-31" aria-hidden="true"></a>        <span class="va">self</span>.dirn.bind(<span class="st">&#39;&lt;Return&gt;&#39;</span>, <span class="va">self</span>.doLS)</span><span id="cb6-32"><a href="#cb6-32" aria-hidden="true"></a>        <span class="va">self</span>.dirn.pack()</span><span id="cb6-33"><a href="#cb6-33" aria-hidden="true"></a></span><span id="cb6-34"><a href="#cb6-34" aria-hidden="true"></a>        <span class="va">self</span>.bfm <span class="op">=</span> tk.Frame(<span class="va">self</span>.top)</span><span id="cb6-35"><a href="#cb6-35" aria-hidden="true"></a>        <span class="va">self</span>.clr <span class="op">=</span> tk.Button(<span class="va">self</span>.bfm, text<span class="op">=</span><span class="st">&#39;Clear&#39;</span>, command<span class="op">=</span><span class="va">self</span>.clrDir,</span><span id="cb6-36"><a href="#cb6-36" aria-hidden="true"></a>                             activeforeground<span class="op">=</span><span class="st">&#39;white&#39;</span>, activebackground<span class="op">=</span><span class="st">&#39;blue&#39;</span>)</span><span id="cb6-37"><a href="#cb6-37" aria-hidden="true"></a>        <span class="va">self</span>.ls <span class="op">=</span> tk.Button(<span class="va">self</span>.bfm, text<span class="op">=</span><span class="st">&#39;List Directory&#39;</span>, command<span class="op">=</span><span class="va">self</span>.doLS,</span><span id="cb6-38"><a href="#cb6-38" aria-hidden="true"></a>                            activeforeground<span class="op">=</span><span class="st">&#39;white&#39;</span>, activebackground<span class="op">=</span><span class="st">&#39;green&#39;</span>)</span><span id="cb6-39"><a href="#cb6-39" aria-hidden="true"></a>        <span class="va">self</span>.quit <span class="op">=</span> tk.Button(<span class="va">self</span>.bfm, text<span class="op">=</span><span class="st">&#39;Quit&#39;</span>, command<span class="op">=</span><span class="va">self</span>.top.quit,</span><span id="cb6-40"><a href="#cb6-40" aria-hidden="true"></a>                              activeforeground<span class="op">=</span><span class="st">&#39;white&#39;</span>, activebackground<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span><span id="cb6-41"><a href="#cb6-41" aria-hidden="true"></a>        <span class="va">self</span>.clr.pack(side<span class="op">=</span>tk.LEFT)</span><span id="cb6-42"><a href="#cb6-42" aria-hidden="true"></a>        <span class="va">self</span>.ls.pack(side<span class="op">=</span>tk.LEFT)</span><span id="cb6-43"><a href="#cb6-43" aria-hidden="true"></a>        <span class="va">self</span>.quit.pack(side<span class="op">=</span>tk.LEFT)</span><span id="cb6-44"><a href="#cb6-44" aria-hidden="true"></a>        <span class="va">self</span>.bfm.pack()</span><span id="cb6-45"><a href="#cb6-45" aria-hidden="true"></a></span><span id="cb6-46"><a href="#cb6-46" aria-hidden="true"></a>        <span class="cf">if</span> initdir:</span><span id="cb6-47"><a href="#cb6-47" aria-hidden="true"></a>            <span class="va">self</span>.cwd.<span class="bu">set</span>(os.curdir)</span><span id="cb6-48"><a href="#cb6-48" aria-hidden="true"></a>            <span class="va">self</span>.doLS()</span><span id="cb6-49"><a href="#cb6-49" aria-hidden="true"></a></span><span id="cb6-50"><a href="#cb6-50" aria-hidden="true"></a>    <span class="kw">def</span> clrDir(<span class="va">self</span>, ev<span class="op">=</span><span class="va">None</span>):</span><span id="cb6-51"><a href="#cb6-51" aria-hidden="true"></a>        <span class="co">&quot;&quot;&quot;清空文件列表&quot;&quot;&quot;</span></span><span id="cb6-52"><a href="#cb6-52" aria-hidden="true"></a>        <span class="va">self</span>.cwd.<span class="bu">set</span>(<span class="st">&#39;&#39;</span>)</span><span id="cb6-53"><a href="#cb6-53" aria-hidden="true"></a></span><span id="cb6-54"><a href="#cb6-54" aria-hidden="true"></a>    <span class="kw">def</span> setDirAndGo(<span class="va">self</span>, ev<span class="op">=</span><span class="va">None</span>):</span><span id="cb6-55"><a href="#cb6-55" aria-hidden="true"></a>        <span class="co">&quot;&quot;&quot;设置文件夹路径并读取&quot;&quot;&quot;</span></span><span id="cb6-56"><a href="#cb6-56" aria-hidden="true"></a>        <span class="va">self</span>.last <span class="op">=</span> <span class="va">self</span>.cwd.get()</span><span id="cb6-57"><a href="#cb6-57" aria-hidden="true"></a>        <span class="va">self</span>.dirs.config(selectbackgroung<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span><span id="cb6-58"><a href="#cb6-58" aria-hidden="true"></a>        check <span class="op">=</span> <span class="va">self</span>.dirs.get(<span class="va">self</span>.dirs.curselection())</span><span id="cb6-59"><a href="#cb6-59" aria-hidden="true"></a>        <span class="cf">if</span> <span class="kw">not</span> check:</span><span id="cb6-60"><a href="#cb6-60" aria-hidden="true"></a>            check <span class="op">=</span> os.curdir</span><span id="cb6-61"><a href="#cb6-61" aria-hidden="true"></a>        <span class="va">self</span>.cwd.<span class="bu">set</span>(check)</span><span id="cb6-62"><a href="#cb6-62" aria-hidden="true"></a>        <span class="va">self</span>.doLS()</span><span id="cb6-63"><a href="#cb6-63" aria-hidden="true"></a></span><span id="cb6-64"><a href="#cb6-64" aria-hidden="true"></a>    <span class="kw">def</span> doLS(<span class="va">self</span>, ev<span class="op">=</span><span class="va">None</span>):</span><span id="cb6-65"><a href="#cb6-65" aria-hidden="true"></a>        error <span class="op">=</span> <span class="st">&#39;&#39;</span></span><span id="cb6-66"><a href="#cb6-66" aria-hidden="true"></a>        tdir <span class="op">=</span> <span class="va">self</span>.cwd.get()</span><span id="cb6-67"><a href="#cb6-67" aria-hidden="true"></a>        <span class="cf">if</span> <span class="kw">not</span> tdir:</span><span id="cb6-68"><a href="#cb6-68" aria-hidden="true"></a>            tdir <span class="op">=</span> os.curdir</span><span id="cb6-69"><a href="#cb6-69" aria-hidden="true"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(tdir):</span><span id="cb6-70"><a href="#cb6-70" aria-hidden="true"></a>            error <span class="op">=</span> tdir <span class="op">+</span> <span class="st">&#39;:no such file&#39;</span></span><span id="cb6-71"><a href="#cb6-71" aria-hidden="true"></a>        <span class="cf">elif</span> <span class="kw">not</span> os.path.isdir(tdir):</span><span id="cb6-72"><a href="#cb6-72" aria-hidden="true"></a>            error <span class="op">=</span> tdir <span class="op">+</span> <span class="st">&#39;:not a directory&#39;</span></span><span id="cb6-73"><a href="#cb6-73" aria-hidden="true"></a></span><span id="cb6-74"><a href="#cb6-74" aria-hidden="true"></a>        <span class="cf">if</span> error:</span><span id="cb6-75"><a href="#cb6-75" aria-hidden="true"></a>            <span class="va">self</span>.cwd.<span class="bu">set</span>(error)</span><span id="cb6-76"><a href="#cb6-76" aria-hidden="true"></a>            <span class="va">self</span>.top.update()</span><span id="cb6-77"><a href="#cb6-77" aria-hidden="true"></a>            sleep(<span class="dv">2</span>)</span><span id="cb6-78"><a href="#cb6-78" aria-hidden="true"></a>            <span class="cf">if</span> <span class="kw">not</span> (<span class="bu">hasattr</span>(<span class="va">self</span>, <span class="st">&#39;last&#39;</span>) <span class="kw">and</span> <span class="va">self</span>.last):</span><span id="cb6-79"><a href="#cb6-79" aria-hidden="true"></a>                <span class="va">self</span>.last <span class="op">=</span> os.curdir</span><span id="cb6-80"><a href="#cb6-80" aria-hidden="true"></a>            <span class="va">self</span>.cwd.<span class="bu">set</span>(<span class="va">self</span>.last)</span><span id="cb6-81"><a href="#cb6-81" aria-hidden="true"></a>            <span class="va">self</span>.dirs.config(selectbackground<span class="op">=</span><span class="st">&#39;LightSkyBlue&#39;</span>)</span><span id="cb6-82"><a href="#cb6-82" aria-hidden="true"></a>            <span class="va">self</span>.top.update()</span><span id="cb6-83"><a href="#cb6-83" aria-hidden="true"></a>            <span class="cf">return</span></span><span id="cb6-84"><a href="#cb6-84" aria-hidden="true"></a></span><span id="cb6-85"><a href="#cb6-85" aria-hidden="true"></a>        <span class="va">self</span>.cwd.<span class="bu">set</span>(<span class="st">&#39;Fetching directory contents...&#39;</span>)</span><span id="cb6-86"><a href="#cb6-86" aria-hidden="true"></a>        <span class="va">self</span>.top.update()</span><span id="cb6-87"><a href="#cb6-87" aria-hidden="true"></a>        dirlist <span class="op">=</span> os.listdir(tdir)</span><span id="cb6-88"><a href="#cb6-88" aria-hidden="true"></a>        dirlist.sort()</span><span id="cb6-89"><a href="#cb6-89" aria-hidden="true"></a>        os.chdir(tdir)</span><span id="cb6-90"><a href="#cb6-90" aria-hidden="true"></a></span><span id="cb6-91"><a href="#cb6-91" aria-hidden="true"></a>        <span class="va">self</span>.dirl.config(text<span class="op">=</span>os.getcwd())</span><span id="cb6-92"><a href="#cb6-92" aria-hidden="true"></a>        <span class="va">self</span>.dirs.delete(<span class="dv">0</span>, tk.END)</span><span id="cb6-93"><a href="#cb6-93" aria-hidden="true"></a>        <span class="va">self</span>.dirs.insert(tk.END, os.curdir)</span><span id="cb6-94"><a href="#cb6-94" aria-hidden="true"></a>        <span class="va">self</span>.dirs.insert(tk.END, os.pardir)</span><span id="cb6-95"><a href="#cb6-95" aria-hidden="true"></a>        <span class="cf">for</span> eachFile <span class="kw">in</span> dirlist:</span><span id="cb6-96"><a href="#cb6-96" aria-hidden="true"></a>            <span class="va">self</span>.dirs.insert(tk.END, eachFile)</span><span id="cb6-97"><a href="#cb6-97" aria-hidden="true"></a>        <span class="va">self</span>.cwd.<span class="bu">set</span>(os.curdir)</span><span id="cb6-98"><a href="#cb6-98" aria-hidden="true"></a>        <span class="va">self</span>.dirs.config(selectbackground<span class="op">=</span><span class="st">&#39;LightSkyBlue&#39;</span>)</span><span id="cb6-99"><a href="#cb6-99" aria-hidden="true"></a></span><span id="cb6-100"><a href="#cb6-100" aria-hidden="true"></a></span><span id="cb6-101"><a href="#cb6-101" aria-hidden="true"></a></span><span id="cb6-102"><a href="#cb6-102" aria-hidden="true"></a><span class="kw">def</span> main():</span><span id="cb6-103"><a href="#cb6-103" aria-hidden="true"></a>    DirList(os.curdir)</span><span id="cb6-104"><a href="#cb6-104" aria-hidden="true"></a>    tk.mainloop()</span><span id="cb6-105"><a href="#cb6-105" aria-hidden="true"></a></span><span id="cb6-106"><a href="#cb6-106" aria-hidden="true"></a></span><span id="cb6-107"><a href="#cb6-107" aria-hidden="true"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span><span id="cb6-108"><a href="#cb6-108" aria-hidden="true"></a>    main()</span></code></pre></div>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 编程实践 </tag>
            
            <tag> GUI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL落地方法论5】回报函数</title>
      <link href="/blog/post/d73b0d23.html"/>
      <url>/blog/post/d73b0d23.html</url>
      
        <content type="html"><![CDATA[<p>回报函数（reward）设计在DRL应用中是极其重要的一环，通过将任务目标具体化和数值化，reward就如同一种特殊语言，实现了目标与算法之间的沟通，算法工作者在这里面承担了翻译的角色，翻译的好坏体现了其对任务逻辑的理解深度，决定了agent最终是否能学到期望的技能，并直接影响算法的收敛速度和最终性能。结合上一篇的内容，我们知道DRL算法中reward负责引导神经网络挖掘状态信息中的决策相关因素并经过提炼后用于action的计算生成。既然reward设计这么重要，想必分析起来又会是像状态空间那样的长篇大论吧。AI时代有了深度神经网络，也不缺数据和算力，难道这点人工就不能省下来吗？</p><a id="more"></a><h2 id="非要手工设计吗">非要手工设计吗？</h2><p>鉴于强化学习算法对优秀reward函数设计的依赖，学术界提出了很多方法改善这一状况。比如逆向强化学习，利用expert demonstration（专家示范）学习到reward函数，再用这个reward函数训练RL策略。此外，还有一大堆模仿学习的方法，干脆抛开reward直接拟合专家策略。以上方法的前提是要有专家数据，不具备普适性，这里就不多说了。</p><p>近年来学术界有个趋势，希望通过深度神经网络自动学习reward函数，从而代替手工设计。其中一篇比较有代表性的工作[1] ，在传统Actor-Critic框架的基础上，又增加了一个Reward网络，输入当前的状（state）和动作（action），输出这一步的reward值。Actor和Critic网络都依据最新的reward网络输出进行优化，而reward网络则依据人类（supervisor）的喜好用有监督的方式进行更新，具体方法是周期性地采集一些episode片段并成对地让人类观看，后者反馈更喜欢哪个片段，再由reward网络拟合这个二分类问题。这篇paper的思想我非常欣赏，现实生活中确实存在很多难以分解和量化的目标，比如让agent学会后空翻，reward就很难设计；有些目标虽然是本身是量化的，但过于笼统难以分解成具体的reward，此时这种方法就很有吸引力。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225212358.png" /></p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225212421.png" /></p><p>可惜啊~~~，我们在现实中遇到的需求不大可能是第一种情况，比起“不能做→能做”，工业界更喜欢“能做→做得更好”，一般都会给出一个明确的指标用来最大化或最小化。假如这个指标能通过少量采样统计出来或体现出优劣关系，那就适合采用这种方法自动学习reward函数，而且不需要人的介入，全程自动化进行。但如果这个指标必须通过大量采样才能统计出来或者获得可靠的优劣关系，该方法的效率就非常低了，这种情况下就得老老实实地手工设计reward，走传统DRL路线了。</p><h2 id="主线reward和稀疏回报问题">主线reward和稀疏回报问题</h2><p>当我们拿到一个任务目标，往往能够简单分析就能找出与该目标紧密联系的主线事件，比如小车到达终点的任务中“到终点”就是这样的事件，拳皇里“KO”也是这样的事件，超级马里奥中“通关”还是这样的事件。此时我们就有了第一个reward项，我把它称之为主线reward，一般是正奖励，当主线事件发生时即反馈给agent。理论上，只要有主线reward就可以用强化学习算法进行训练了。在很多简单任务中，agent在探索过程中靠误打误撞就能以一定概率遇到主线事件，通过正反馈尝到甜头后通过更新policy逐渐提升得到奖励的概率直至收敛。可是当问题稍微复杂一些，通过随机方式探索到主线事件（正样本）的概率变得很低，而强化学习算法本身的数据效率不高，只靠这些少得可怜的正样本，算法难以收敛或收敛很慢。下面我引用伯克利RL大神Pieter Abbeel的课程CS294-40中的例子具体说明。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213337.png" /></p><p>在上图中agent从最左端起始位置S处出发（state=1），目标是到达最右端的G位置(state=5)，允许的动作包括向左一格和向右一格。在图中的reward系统下，agent只有到达G才能得到1分的奖励（主线reward），其他状态都没有任何反馈，那么仅靠随机探索agent是很难到达G的，<strong>因为中间缺乏有效信号来指导agent向正确的方向前进</strong>。在强化学习中，这一类问题被称作稀疏回报问题（Sparse Reward Problem），一直都是DRL领域研究的热点。显然，只定义了主线reward的任务几乎都是稀疏回报问题，对数据效率低下的RL算法而言，学习难度是很大的。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213402.png" /></p><p>当稀疏回报问题遇上高难度探索（Hard Exploration）问题，DRL算法收敛更是难上加难，几乎是不可能完成的任务。比如让机器人学会打开盒子，抓起木块放到盒子里，然后再把盒子盖上，如果只在成功完成这一系列动作的时候才给出奖励，那么用一般的DRL算法和探索策略根本不可能学会目标技能，因为正样本产生的概率跟闭着眼用针尖扎到平面上一点的概率差不多，可以认为是0。</p><p>针对稀疏回报问题，学术界提出了很多方法，比如通过鼓励agent探索未见过的状态，提高正样本利用率，或者干脆用遗传算法或进化策略代替RL学习policy网络。这些方法不在本篇的讨论范围内，我们关心的是如何通过reward设计本身来规避稀疏回报问题，并尽可能提高训练效率和最终性能。</p><p>此外，联系上一篇状态空间设计的内容，由于主线事件通常难以一蹴而就，大部分状态信息相对于主线reward也就都属于间接相关信息，这也从另一个角度解释了为什么在稀疏回报下算法训练难度高。</p><h2 id="目标分解和辅助reward">目标分解和辅助reward</h2><p>既然只有主线reward不行，我们接下来就要将原始任务目标进一步分解成子目标，并分别给予合理的奖励或惩罚，从而达到引导agent趋利避害提高主线事件发生概率的目的。学术界一般称该过程为credit assignment，credit意译过来就是功劳，说的是某个子目标在达成总目标的过程中起了多大作用，是正向作用还是负向作用。这些子目标对应的reward可以称之为辅助reward，它们使reward不再稀疏。通常情况下，<strong>为了保证主线奖励的核心地位和吸引力，各种辅助reward的绝对值都设得相对较小，以免喧宾夺主</strong>。</p><h3 id="目标分解实例">目标分解实例</h3><p>Agent在环境中探索时需要获得反馈，即刚刚的决策好不好，反馈越及时学得越快，理想情况是每一步都有反馈。还以小车导航到终点的应用为例，除了抵达终点+10分，如果每次靠近终点也+1分，那么小车在抵达终点之前就学会主动靠近终点，这样探索到抵达终点的概率也大大提高了，DRL算法收敛速度自然会加快。</p><p>除了抵达终点，小车还要避免与障碍物和其他小车发生碰撞，我们还要对碰撞事件做出惩罚。为了使agent更好地学会避免碰撞，我们除了对已经发生的碰撞事件给予惩罚，还可以再增加一个预防式的靠近惩罚，并利用状态空间里的直接相关信息——与最近邻居的距离，提高算法学习效率，具体可以参考<a href="https://su-lemon.github.io/blog/post/63561af6.html">状态空间篇</a>。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213423.png" /></p><p>辅助reward的设计建立在对任务逻辑的深刻分析和理解之上，有很多细节都会对最终目标的实现产生正向或负向的影响，值得我们深入挖掘。比如，为了使小车尽快到达终点，就要求少绕路，而绕路的典型表现是转弯多，于是可以增加对转弯的惩罚。类似这样的链式思考有助于找到更好的辅助reward，帮助降低学习难度和提升最终性能。</p><p>此外，由于将最终目标分解成了子目标，在设计对应辅助reward时往往很容易找到与之即时联动的直接相关状态信息，或者相关性较强的间接相关信息。事实上，我们每设计一个reward项，就应该回过头去检查状态空间中是否包含了直接或间接相关信息，已经包含的信息是否足够直接高效，有没有改进的空间。</p><h3 id="杜绝异常行为">杜绝异常行为</h3><p>OK，让我们来捋一捋目前的reward项：抵达奖励，靠近终点奖励，碰撞惩罚和转弯惩罚。Reward项一多，我们就要特别注意它们之间的相对大小。首先，应该避免某个（些）reward项的绝对值过大，以至于淹没其他reward项的影响，必要时应使用系数加以调控；其次，应该避免reward项的不合理取值及多项reward之间的不合理相对大小，导致agent学到异常行为。因为不合理reward造成的常见异常行为主要包括三种类型：鲁莽、贪婪和胆怯，怎么感觉我在说炒股呢~~~</p><h4 id="鲁莽">鲁莽</h4><p>鲁莽行为指的是reward中漏掉了针对某个不希望出现的事件的惩罚项或者惩罚力度太小，被其他reward项盖过，导致agent无法学到主动规避该事件或者权衡利弊后仍然选择接受该事件的惩罚以换取更大收益。比如下图中金币搜索任务中，设计者忘了给予进入岩浆的惩罚，结果机器人为了尽快得到金币而甘愿“赴汤蹈火”；在小车导航的例子中，碰撞惩罚相对于远离惩罚过小，小车可能为了尽快到达终点宁愿撞到其他小车上也不愿意绕远。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213439.png" /></p><h4 id="贪婪">贪婪</h4><p>靠近终点奖励使reward变稠密了，但这样做就够了吗？我们说过RL追求的是长期收益，事实上对小车来说收益最高的选择不是尽快抵达终点，而是不断重复“靠近-远离”的动作，如此一点点地累加，收益远超过抵达终点的一锤子买卖！很显然，agent钻了reward设计漏洞的空子，变得不思进取，贪得无厌。为了防止这种情况发生，我们还要对原地不动或远离终点的行为进行惩罚，而且相对于靠近奖励，扣分太少也不行，否则agent仍然会发现钻空子是划算的。一劳永逸的办法是，将靠近终点的正向奖励改成微小惩罚，绝对值小于原地不动或远离惩罚，这样做的好处是不仅不给agent钻空子的机会，而且还能督促小车尽快向终点行驶。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213456.png" /></p><p>再比如在蒙特祖玛的复仇中，要想通关（主线事件，+10）必须先吃到钥匙，为了鼓励agent吃钥匙我们还可以为这个子目标提供专门的奖励（+1）。这种情况下房间只有一把钥匙还好，但如果到处都是钥匙呢？Agent很可能会变得乐不思蜀，一直留在房间里找新的钥匙，忘记通关这回事了。如果真是这样，那就不如不设吃钥匙奖励，只保留主线奖励，由Agent自己去发掘通关与吃钥匙之间的内在联系。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213513.png" /></p><p>实际上，<strong>除了主线reward应该提供正向奖励以外，其他辅助reward最好都设置为惩罚项</strong>。除非某个子目标与主线事件之间存在强关联，而且该子目标的达成是一次性的或者数量可控，否则不应轻易设置额外奖励项，因为这样很容易诱导agent学习到短视的贪婪策略，只捡芝麻，不要西瓜。</p><h4 id="胆怯">胆怯</h4><p>与贪婪相反的另一个异常行为是胆怯，如果惩罚项很多且绝对值相对于主线reward太大，那么agent在探索过程中会收到大量负反馈，从而变得畏首畏尾，学习到各种意想不到的“绥靖”策略。比如在小车到终点的例子中，假如碰撞惩罚和转弯惩罚绝对值过大，agent有可能宁愿选择原地不动，这是因为训练初期policy很差，需要经历大量转弯和碰撞后才可能出现主线事件（到达终点），而收到的负反馈完全湮没了主线奖励，因此在agent看来原地不动的长期累计收益<strong>暂时</strong>不比到终点差，尽管只是暂时的，但agent很可能陷在这个局部最优里出不来了。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213530.png" /></p><p>在上述情况下，你会发现只需要将惩罚项绝对值减小，突出主线奖励的影响，其他什么也不用干，DRL模型就能顺利收敛了。当然，还可以适当降低折扣因子，让agent变成“近视眼”，更多关注眼前利益，忽略长期的负收益期望（靠后的负反馈都被折扣掉了），只要agent“迈开腿”出来探索，就有更大可能遇到主线事件，并在主线reward的奖励下学习到目标技能。折扣因子的设置技巧我将在训练篇中详细介绍。</p><h3 id="reward-shaping">Reward Shaping</h3><p>把任务目标分解得足够细，又避开了上述各种坑，这样就完美了吗？并没有~~~如果你读过Andrew Ng在1999年发表的关于reward shaping的paper，你就会发现还可以让RL算法收敛得更快一些。Reward shaping技术的证明这里就不赘述了，喜欢手推公式的朋友可以去读原著。我大概说下原理，在原有reward基础上增加一项shaping reward，该项代表某种势能函数，与最终目标的差距决定了势能大小。对于我们上一节举过的例子，如果把reward修改为下图中的形式，agent每向右移动一格都会获得奖励，且离G越近奖励越高，那么agent就很容易被引导到G位置，从而大大加速算法收敛。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225213927.png" /></p><p>Ng从理论上证明了理想势能函数就是 <span class="math inline">\(V(s)\)</span> ，这很好解释，我们前边说过 <span class="math inline">\(V(s)\)</span> 是对长期收益的期望，policy就是根据它优化的，如果一开始就把完美的 <span class="math inline">\(V(s)\)</span> 提供给小车，那也就不用学了。对小车到终点的应用而言，用小车当前位置与终点的距离作为势能函数，增加一个惩罚项 <span class="math inline">\(-\alpha \cdot dist\)</span> ，离终点较远时惩罚得多一些，较近时惩罚得小一些，这样就能起到reward shaping的作用。此外，参考上一篇状态空间设计的内容，你会发现增加的惩罚项可以与状态空间中描述小车与终点相对位置的信息即时联动，从而使后者由间接相关信息转变为直接相关信息，这就从另一个角度解释了reward shaping为什么能提高算法训练的效率。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225214007.png" /></p><h2 id="optimal-reward-problem">Optimal Reward Problem</h2><p>看到这里你可能会问，针对特定任务，比如小车导航到终点，是否存在一组最优reward使得DRL算法在同等条件下收敛最快、性能最高？答案是肯定的，但要想找到它是困难的，该问题在学术界被称为Optimal Reward Problem（ORP），解决方案包括暴力搜索[2]、基于在线策略梯度的PGRD[3]、分层强化学习[4]、遗传算法[5]、Bayes方法[6]等等，有兴趣的朋友可以找来相关paper读一读，相关参考文献我列到最后。</p><h2 id="总结">总结</h2><p>总结一下，reward设计的原则是：<strong>尽可能稠密（最好每步都有反馈），能够反映任务目标/子目标逻辑，与状态空间相呼应，控制好各项取值和相对大小，避免异常行为，适时采用reward shaping</strong>。当算法选择好，动作空间定义好，状态空间和回报函数都设计好，接下来就该进入训练环节了。</p><h2 id="参考">参考</h2><ol type="1"><li>Christiano, P. F.; Leike, J.; Brown, T. B.; Martic, M.; Legg, S.; and Amodei, D. 2017. Deep Reinforcement Learning from Human Preferences. Neural Information Processing Systems: 4299-4307.</li><li>Sorg, J.; Singh, S. P.; and Lewis, R. L. 2010. Internal Rewards Mitigate Agent Boundedness. International Conference on Machine Learning: 1007-1014.</li><li>Sorg, J.; Lewis, R. L.; and Singh, S. P. 2010. Reward Design via Online Gradient Ascent. Neural Information Processing Systems: 2190-2198.</li><li>Bratman, J.; Singh, S. P.; Sorg, J.; and Lewis, R. L. 2012. Strong Mitigation: Nesting Search for Good Policies within Search for Good Reward. Adaptive Agents and Multi Agents Systems: 407-414.</li><li>Niekum, S.; Barto, A. G.; and Spector, L. 2010. Genetic Programming for Reward Function Search. IEEE Transactions on Autonomous Mental Development 2(2): 83-90.</li><li>Hadfield-Menell, D.; Milli, S.; Abbeel, P.; Russell, S.; and Dragan, A. D. 2017. Inverse Reward Design. Neural Information Processing Systems: 6765-6774.</li></ol>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Hexo博客】从搭建到部署，快速构建私人博客</title>
      <link href="/blog/post/5bd42e8c.html"/>
      <url>/blog/post/5bd42e8c.html</url>
      
        <content type="html"><![CDATA[<p>有时候我们希望有一个受控的博客，来记录或分享一些东西。这个博客的主题内容由你自己来决定，可以是技术分享(编程、汉化分享等)，也可以是生活感想。</p><p>本文将介绍一个可以迅速搭建并部署的受控博客。</p><a id="more"></a><h2 id="为什么搭建博客">为什么搭建博客</h2><p>在线类博客有很多选择，为什么我们需要从零搭建新的博客呢？自己搭建的博客有什么好处吗？</p><p>首先，前文所提的 <strong>“博客受控”</strong>，指的就是能够自己控制的博客的样式、内容等，自己想怎么改就怎么改。</p><p>内容受控是指我们知道在线类的博客是受平台限制的，这意味着你所发表的内容是需要受审才能发出的，一些敏感的技术词汇，该篇文章都可能会被和谐或被删除。但在自己搭建博客就没有这样的问题，最起码能保留源文件。</p><p>其二，博客的样式是受控的。像著名在线博客<code>CSDN</code>上一些博主的文章确实是有学习参考的价值，但问题的是该站广告是在是太多了，字体和排版的阅读体验并不太好。但如果是自己搭建的博客的话，就可以自己着手优化这些问题。</p><p>但博客的搭建还需要我们从各方面考虑利弊。平台类博客会有相应的推荐系统，会对同类型文章相互引流，在 SEO 方面会做得比我们好。</p><p>个人搭建的博客，刚起步时的浏览量并不高，但是可以通过<code>SEO</code>等方式来逐步增加自己网站的权重。或者提高博客的质量和干货，读者认为文章有价值，自然会收藏起来形成熟客。</p><p>那么博客能写什么东西呢？在日常生活中，有很多知识点是呈碎片状，写博客的本质上就是对自己知识的一种梳理，然后再将这些知识分享出来，可能会有对这方面知识有疑惑，或者想找到解决方案，自身分享出来的东西能给读者做一定的参考。同时这也会是一个良性循环，因为分享的同时，你可能也需要去查询一些资料，同时也可以找到别人遇到过并分享出来的解决方案，是一个相互收益的过程。</p><p>我们的基本需求是梳理与分享，那么更应该把注意力放在内容本身，网站布局的排版样式等则是增加读者阅读体验的问题。因此我们可以使用现成的博客框架快速完成这些事。</p><p>博客框架有很多种选择，笔者选择的是 <a href="https://hexo.io/zh-cn/">Hexo</a>，因为它足够便捷优雅。</p><h2 id="开始">开始</h2><p><code>Hexo</code> 依赖 <a href="https://nodejs.org/en/">Node.js</a> 和 <code>NPM</code>包管理，<code>Node.js</code> 安装后一般会自带<code>NPM</code>。</p><p>我们打开终端(<code>Windows PowerShell / cmd.exe</code>、<code>bash</code>、<code>macOS</code> 里的终端)，输入以下命令：</p><div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co"># 1 检查npm版本</span></span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="ex">npm</span> -v</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a></span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="co"># 2 安装 hexo cli，</span></span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="ex">npm</span> install -g hexo-cli</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a></span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a><span class="co"># 3 检查 hexo 是否安装成功</span></span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a><span class="ex">hexo</span> -v</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a></span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a><span class="co"># 4 查看使用方式</span></span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a><span class="ex">hexo</span> help</span></code></pre></div><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221124920.png" /></p><p>在官网 <a href="https://hexo.io/zh-cn/docs/commands">commands</a> 中可以找到hexo命令完整的解释。</p><h2 id="建站">建站</h2><p>我们可以使用 <code>init</code> 命令来初始化 <code>hexo</code> 项目，但在建站之前我们需要先决定在哪里存放博客源代码。</p><p>笔者使用 <code>iCloud</code> 云文件夹，将源文件在多设备同步。当然根据个人情况也可以选择 <code>OneDrive</code> 等云存储空间。</p><p>该方式只是数据备份与同步的问题，不使用它也不会影响下文的构建。如果读着暂时没有合适的云存储空间，可以直接存储在本地。</p><div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="co"># 1 查看路径（Unix系统）</span></span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="bu">pwd</span></span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="co"># /Users/susu/Library/Mobile Documents/com~apple~CloudDocs</span></span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a></span><span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="co"># 2 初始化文件夹名为 blog</span></span><span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a><span class="ex">hexo</span> init blog</span><span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a></span><span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a><span class="bu">cd</span> blog</span></code></pre></div><p>初始化安装完成后，blog文件夹下的文件目录如下：</p><div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="ex">.</span></span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>├── <span class="ex">_config.yml</span>  # 网站的配置信息</span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>├── <span class="ex">node_modules</span>  # 应用依赖信息</span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>├── <span class="ex">package-lock.json</span></span><span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>├── <span class="ex">package.json</span>  # 依赖包</span><span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>├── <span class="ex">scaffolds</span>  # 模板文件</span><span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>├── <span class="bu">source</span>  # 资源文件夹是存放用户资源的地方</span><span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>│   ├── <span class="ex">_drafts</span>  # 草稿文件夹，刚初始化时可能不存在</span><span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>│   ├── <span class="ex">_posts</span>  # 文章/帖子源码列表</span><span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a>└── <span class="ex">themes</span>  # 博客主题</span></code></pre></div><h2 id="配置">配置</h2><p>建站完成后我们需要进行 <a href="https://hexo.io/zh-cn/docs/configuration">配置</a>，<code>hexo</code> 中主要有两项配置。</p><ul><li><code>站点配置文件</code>，路径为 <code>/_config.yml</code> ；</li><li><code>主题配置文件</code> ，路径是 <code>/themes/(下载的主题)/_config.yml</code> 。</li></ul><p>我们可以先在 <code>站点配置文件</code> 修改以下基础选项：</p><div class="sourceCode" id="cb4"><pre class="sourceCode yml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="co"># Hexo Configuration</span></span><span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a></span><span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="co"># 网站主标题，SEO元素之一</span></span><span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a><span class="fu">title</span><span class="kw">:</span><span class="at"> blog</span></span><span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a></span><span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a><span class="co"># 网站副标题，可选</span></span><span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a><span class="fu">subtitle</span><span class="kw">:</span></span><span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a></span><span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a><span class="co"># 网站描述, SEO元素之一，用于告诉搜索引擎关于这个站点的描述</span></span><span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a><span class="fu">description</span><span class="kw">:</span><span class="at"> </span></span><span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a></span><span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="co"># 网站的关键词</span></span><span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a><span class="fu">keywords</span><span class="kw">:</span><span class="at"> </span></span><span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a></span><span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a><span class="co"># 网站作者</span></span><span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a><span class="fu">author</span><span class="kw">:</span><span class="at"> SuSu</span></span><span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a></span><span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a><span class="co"># 网站使用的语言, 由于 Hexo 具备多语言配置，默认为英文，我们需要修改回中文语言</span></span><span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a><span class="fu">language</span><span class="kw">:</span><span class="at"> zh-CN</span></span></code></pre></div><h2 id="启动">启动</h2><p>初始化项目后默认会安装相关的依赖，接着在命令行输入如下命令来 <strong>运行博客</strong> ：</p><div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co"># 启动服务，默认端口为 4000，启动服务后可以在浏览器输入 `http://localhost:4000` 查看效果</span></span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="ex">hexo</span> server</span><span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co"># 简写方式</span></span><span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="ex">hexo</span> s</span><span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a></span><span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="co"># 还可以使用 -p, 指定 9000 端口</span></span><span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="ex">hexo</span> s -p 9000</span></code></pre></div><h2 id="写作">写作</h2><p>一般我们都会使用 <code>hexo new &lt;title&gt;</code> 来建立文章，这种建立方法会将新文章建立在 <code>source/_posts</code> 目录下，当使用 <code>hexo generate</code> 编译 markdown 文件时，会将其 HTML 结果编译在 <code>public</code> 目录下，之后 <code>hexo deploy</code> 将会把 <code>public</code> 目录下所有文章部署到 GitHub，这是我们熟悉的 Hexo 流程。</p><p>这种建立文章方式的缺点是：若我们同时编辑多篇文章，只要其中一篇文章尚未编辑完成，也会随着 <code>hexo deploy</code> 一起部署到 GitHub，也就是 GitHub 可能会看到我们尚未完成的文章。</p><p>笔者个人的写作习惯是：</p><ol type="1"><li>创建草稿 (<code>drafts</code> )</li><li>在草稿上进行写作</li><li>整理细节并在本地服务器上查看效果(<code>server</code>)</li><li>发布至正式的帖子上</li><li>生成静态文件并部署</li></ol><h3 id="创建草稿">创建草稿</h3><div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="co"># hexo new draft &lt;title&gt;</span></span><span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="ex">hexo</span> new draft ”博文示例“</span></code></pre></div><p>Hexo 提供 <code>draft</code> 机制，它将新文章将建立在 <code>source/_drafts</code> 目录下。<code>博文示例.md</code> 是一个 <code>markdown</code> 文件，默认的内容如下：</p><div class="sourceCode" id="cb7"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="co">---</span></span><span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="an">title:</span><span class="co"> 博文示例</span></span><span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="an">tags:</span><span class="co"> </span></span><span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="co">---</span></span></code></pre></div><h3 id="在本地服务器预览草稿">在本地服务器预览草稿</h3><div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="ex">hexo</span> s --draft</span></code></pre></div><p>Hexo 的 <code>Hexo server</code> 另外提供 <code>--draft</code> 参数，这让我们可以达到一边编辑 <code>markdown</code> 文章，一边使用浏览器预览的目的。</p><h3 id="发布草稿">发布草稿</h3><p>如果我们在本地服务器上校队完草稿细节后，可以将草稿发布为文章，否则在后续生成博客静态文件时不会被打包出来：</p><div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="co"># hexo publish [layout] &lt;filename&gt;</span></span><span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="ex">hexo</span> publish post 博文示例</span></code></pre></div><p>输入命令后你可以发现发布的文章被转移到了<code>source/_posts/</code>上，这样就完成了本地的文章发布。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221155150.jpg" /></p><h3 id="生成静态文件">生成静态文件</h3><p><code>Hexo</code>框架的一项工作就是将源文件 <code>markdown</code> 最后生成为 <code>HTML</code>：</p><div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="co"># 生成文件</span></span><span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="ex">hexo</span> g</span><span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a></span><span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a><span class="co"># 监控文件变化，并生成静态文件</span></span><span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a><span class="ex">hexo</span> g --watch</span><span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a></span><span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a><span class="co"># 生成文件并部署(部署后面单独章节来讲)</span></span><span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a><span class="ex">hexo</span> g -d</span></code></pre></div><h2 id="主题">主题</h2><p>熟悉了博客系统的操作后，接下来就是美化博客。<code>Hexo</code> 支持主题，我们可以根据<a href="https://hexo.io/zh-cn/docs/themes.html">官网的创建主题教程</a>自己来设计，也可以直接在<a href="https://hexo.io/themes/">主题商城</a> 中找现成的主题。这里以笔者的主题为例，演示安装主题的步骤。（主题配置及自定义各种功能，在随后的系列文章讲解）</p><p>安装主题可以通过 <code>git clone</code> 克隆至 <code>blog/theme/</code> 目录下：</p><div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="bu">pwd</span></span><span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="co"># /Users/susu/Library/Mobile Documents/com~apple~CloudDocs</span></span><span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a></span><span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a><span class="co"># 启动主题前需要清除缓存与已部署的文件</span></span><span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a><span class="ex">hexo</span> clean</span><span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a></span><span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a><span class="co"># clone 主题（在主题商城中，每个主题都提供了各自的安装说明）</span></span><span id="cb11-8"><a href="#cb11-8" aria-hidden="true"></a><span class="fu">git</span> clone <span class="op">&lt;</span>主题的GitHub仓库地址<span class="op">&gt;</span> themes/next</span></code></pre></div><p>接着在 <code>站点配置文件(/_config.yml)</code> 中启动 <code>theme</code> 。</p><div class="sourceCode" id="cb12"><pre class="sourceCode yml"><code class="sourceCode yaml"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="co"># _config.yml</span></span><span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a><span class="kw">-</span><span class="at"> </span><span class="fu">theme</span><span class="kw">:</span><span class="at"> landscape</span></span><span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a><span class="fu">+ theme</span><span class="kw">:</span><span class="at"> 下载的主题名称</span></span></code></pre></div><h2 id="部署">部署</h2><p>我们使用<code>git</code>进行部署，可以将网站部署至私人服务器、也可以部署到免费的<code>github pages</code>上。本文将介绍部署至<code>github</code>的方法，如果你还没有<code>github</code>账号的话，那你需要先<a href="https://anran758.github.io/blog/2019/08/19/hexo-blog/github.com">注册一个账号</a>。</p><ol type="1"><li><p>注册 <code>github</code> 账号</p></li><li><p>创建一个仓库，这里我们先创建一个<code>blog</code>。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221155508.png" /></p><p>接着</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221155836.png" /></p></li><li><p>复制仓库链接，拷贝至 <code>站点配置文件(/_config.yml)</code> 里。同时安装 <code>hexo-deployer-git</code> 的依赖：</p><div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="ex">npm</span> install hexo-deployer-git --save</span></code></pre></div><div class="sourceCode" id="cb14"><pre class="sourceCode yml"><code class="sourceCode yaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a><span class="fu">url</span><span class="kw">:</span><span class="at"> https://yourname.github.io/blog</span><span class="co">  # 修改为 github io 的地址</span></span><span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a><span class="fu">root</span><span class="kw">:</span><span class="at"> /blog/</span><span class="co">  # 要将资源映射到仓库名</span></span><span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a></span><span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a><span class="fu">deploy</span><span class="kw">:</span></span><span id="cb14-5"><a href="#cb14-5" aria-hidden="true"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> git</span></span><span id="cb14-6"><a href="#cb14-6" aria-hidden="true"></a><span class="at">  </span><span class="fu">repo</span><span class="kw">:</span><span class="at"> https://github.com/yourname/blog.git</span><span class="co">  # blog 的 git 地址</span></span><span id="cb14-7"><a href="#cb14-7" aria-hidden="true"></a><span class="at">  </span><span class="fu">branch</span><span class="kw">:</span><span class="at"> gh-pages</span><span class="co">  # 发布至 gh-pages 分支，如果该分支不存在，就会自动创建它</span></span></code></pre></div></li><li><p>接着开始部署。如果你还没配置<code>git</code>账号的话，会提示你输入账号密码，输入正确的账号密码后就部署成功了。</p><div class="sourceCode" id="cb15"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="ex">hexo</span> g -d</span></code></pre></div></li><li><p>接着在github创建的 <code>blog</code> 下进入 <code>settings</code> 项，设置 <code>github pages</code> 为 <code>gh-pages</code> 也就是之前在配置里设置的分支即可。</p></li><li><p><code>github pages</code> 绿色底色处提示我们博客的地址，点击就可以线上查看啦~</p></li></ol><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221160720.png" /></p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221160639.png" /></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> 博客部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL算法】VPG与AC的思想和推导</title>
      <link href="/blog/post/46d87670.html"/>
      <url>/blog/post/46d87670.html</url>
      
        <content type="html"><![CDATA[<p>策略梯度与Actor-Critic的思想和推导</p><a id="more"></a><h1 id="rl学习什么">RL学习什么</h1><ul><li><p>动作值函数（Q函数）。</p><p>以Q-Learning、DQN为代表，这个系列的算法学习最优动作值函数 <span class="math inline">\(Q^*(s,a)\)</span> 的近似函数 <span class="math inline">\(Q_\theta(s,a)\)</span> 。</p><p>Q-learning 智能体的动作由下面的式子给出：</p></li></ul><p><span class="math display">\[\begin{equation}a(s)=\arg\,\max_a\, Q_\theta(s,a)\label{222}\end{equation}\]</span></p><ul><li><p>策略（随机或确定的）。</p><p>这个系列的方法将策略显示表示为 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，它们直接对性能目标 <span class="math inline">\(J(\pi_{w})\)</span> 进行梯度下降来优化参数 <span class="math inline">\(w\)</span> ，使得我们输入当前的 <span class="math inline">\(s\)</span> 就能输出应该执行的最佳动作 <span class="math inline">\(a\)</span> 。</p></li><li><p>值函数。</p></li><li><p>以及/或者环境模型。</p></li></ul><h1 id="vanilla-policy-gradientvpg">Vanilla Policy Gradient（VPG）</h1><p>以下考虑的情况是状态 <span class="math inline">\(s\)</span> 为连续高维变量、动作 <span class="math inline">\(a\)</span> 为分类变量（有限个）的MDP。并且，设环境 <span class="math inline">\(P_{s, s^{\prime}}^{a}\)</span> 与 <span class="math inline">\(r_{5}^{a}\)</span> 为时齐的，不随时间的变化而变化。（状态与动作都是连续变量的MDP有更高效的DDPG等方法解决，不在VPG里讨论）</p><h2 id="策略网络的构造">策略网络的构造</h2><p>在随机且时齐的MDP中，<strong>策略是状态到动作的映射</strong>。由于 <span class="math inline">\(a\)</span> 是分类变量，所以没有办法直接输出 <span class="math inline">\(a\)</span> ，只能输出一个条件分布 <span class="math inline">\(\pi(a \mid s)\)</span> 。为了拟合这个策略，我们定义一个神经网络policy net。网络的输入是 <span class="math inline">\(s\)</span> ，输出是一个 <span class="math inline">\(n\)</span> 维向量，对它进行softmax之后，得到 <span class="math inline">\(n\)</span> 个不同的概率（其和为1），分别对应于最佳动作是各个 <span class="math inline">\(a\)</span> 的概率。设网络的参数为 <span class="math inline">\(w\)</span> ，则可以将网络输出简记为 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，它表示在 <span class="math inline">\(s\)</span> 状态下最佳动作是 <span class="math inline">\(a\)</span> 的条件概率。</p><p>与我们熟悉的分类网络做比较：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrducvd24j31d80om4ge.jpg" /></p><p>我们可以认为分类网络是在用“权重相同的训练集”去训练，而策略网络则是在用“带有不同权重的训练集”去训练。只要我们能够找出衡量 <span class="math inline">\((s,a)\)</span> 好坏的标准.<span class="math inline">\(v\)</span> ，得到形式 <span class="math inline">\((s,a,v)\)</span> 的数据，就可以把训练策略网络的过程看成“带权重的监督学习”。但是，我们如何找出这个 <span class="math inline">\(v\)</span> 呢？找出之后具体 <span class="math inline">\(v\)</span> 应该按照什么公式训练呢？下面，要详细地根据定义推导出policy gradient的表达式。</p><h2 id="推导最基本的策略梯度">推导最基本的策略梯度</h2><p>接下来推导策略梯度的公式及其计算方法。</p><p>假设策略网络的参数为 <span class="math inline">\(w\)</span> ，则可以将策略记为 <span class="math inline">\(\pi_w\)</span> 。</p><ul><li><strong>轨迹的概率</strong>。在不同网络参数 <span class="math inline">\(w\)</span> 下，策略 <span class="math inline">\(\pi_w\)</span> 给出的轨迹 <span class="math inline">\(\tau = \left(s_{0}, a_{0}, r_{0}, s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2}, \ldots, s_{t}, a_{t}, r_{t}\right)\)</span> 有不同的分布 <span class="math inline">\(P_{\pi_{w}}(\tau)\)</span> ，简记为<span class="math inline">\(P_{w}(\tau)\)</span> 。</li></ul><p><span class="math display">\[P_{w}(\tau)=\Pi_{t=0}^{n} \pi_{w}\left(a_{t} \mid s_{t}\right) \Pi_{t=0}^{T-1} P_{s_{t}, s_{t+1}}^{a_{t}} \Pi_{t=0}^{T} P\left(r_{t} \mid s_{t}, a_{t}\right)\label{2}\]</span></p><ul><li><strong>目标函数</strong>。我们的目标是最大化期望回报 <span class="math inline">\(J(w)=E_{w}(r(\tau))\)</span> ，这里假设回报无衰减（ <span class="math inline">\(\gamma=1\)</span> ，对于 <span class="math inline">\(\gamma&lt;1\)</span> 的情况推导过程类似）。</li></ul><p><span class="math display">\[J(w) = E_{w}(r(\tau)) = \int_{\tau} P_{w}(\tau) r(\tau) d \tau\]</span></p><ul><li><strong>策略梯度</strong>。我们想求的“策略梯度”就是 <span class="math inline">\(\nabla_{w} J(w)\)</span> 。得到策略梯度就可以通过梯度下降来优化策略 <span class="math inline">\(w_{k+1} = w_{k} + \alpha\,\nabla_{w} J(w)\)</span> 。</li></ul><p><span class="math display">\[\nabla_{w} J(w)=\int_{\tau}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau\label{4}\]</span></p><p>​ 上面的<span class="math inline">\(\nabla_{w} J(w)\)</span>仍然是一个积分式，我们很自然地希望将其表示为<span class="math inline">\(\int_{\tau} P_{w}(\tau) \nabla_{w} f(\tau) d \tau\)</span>的形式，其中的<span class="math inline">\(f\)</span>是某个函数。根据期望的定义，这个积分的结果就是<span class="math inline">\(E_{w}\left[\nabla_{w} f(\tau)\right]\)</span>。这样的形式更加简便，并且也更容易计算——只要我们用当前的策略与环境交互采样很多<span class="math inline">\(\tau\)</span>，并计算出梯度<span class="math inline">\(\nabla_{w} f(\tau)\)</span>的均值，就能将其作为<span class="math inline">\(\nabla_{w} J(w)\)</span>的一个估计。</p><ul><li><strong>对数导数技巧</strong>。</li></ul><p><span class="math display">\[P_{w}(\tau) \nabla_{w} \log P_{w}(\tau)=P_{w}(\tau) \frac{\nabla_{w} P_{w}(\tau)}{P_{w}(\tau)}=\nabla_{w} P_{w}(\tau)\label{5}\]</span></p><p>​ 将 <span class="math inline">\((\ref{5})\)</span> 带入 <span class="math inline">\((\ref{4})\)</span> 得： <span class="math display">\[\begin{eqnarray*}\nabla_{w} J(w) &amp;=&amp; \int_{\gamma}\left(\nabla_{w} P_{w}(\tau)\right) r(\tau) d \tau \\&amp;=&amp; \int_{\gamma} P_{w}(\tau) \nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau) d \tau \\&amp;=&amp; E_{w}\left[\nabla_{w} \log \left(P_{w}(\tau)\right) r(\tau)\right]\end{eqnarray*}\]</span></p><ul><li><strong>环境函数的梯度</strong>。环境不依赖于参数 <span class="math inline">\(w\)</span> ，所以式 (<span class="math inline">\(\ref{2}\)</span>) 中 <span class="math inline">\(P_{s_{i}, s_{i+1}}^{a_{i}}\)</span> ，<span class="math inline">\(P\left(r_{i} \mid s_{i}, a_{i}\right)\)</span> 的梯度为零。</li><li><strong>轨迹对数概率的梯度</strong>。所以轨迹对数概率的梯度为：</li></ul><p><span class="math display">\[\nabla_{w} \log \left(P_{w}(\tau)\right)=\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}(a_{t} \mid s_{t})\]</span></p><ul><li><strong>简化后的基本策略梯度</strong>。</li></ul><p><span class="math display">\[\nabla_{w} J(w)=E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau) \right]\label{7}\]</span></p><p>​ 这是一个期望，这意味着我们可以使用样本均值对其进行估计。 如果我们收集一组轨迹 <span class="math inline">\(D = {\left\{ \tau_i \right\}}_{i=1,\cdots,N}\)</span> ， 其中每一个轨迹通过让智能体在环境中使用策略 <span class="math inline">\(\pi_w\)</span> 执行操作得到，则策略梯度可以使用以下式子进行估计： <span class="math display">\[\hat{g} = \frac{1}{N}\sum_{\tau \in D} \sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau)\label{ref9}\]</span></p><h2 id="vpg算法">VPG算法</h2><table><colgroup><col style="width: 100%" /></colgroup><thead><tr class="header"><th>Policy gradient</th></tr></thead><tbody><tr class="odd"><td>构建策略网络 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> ，并随机初始化参数</td></tr><tr class="even"><td>重复下面步骤：</td></tr><tr class="odd"><td>- 用策略 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> 与环境交互，产生大量 <span class="math inline">\(\tau\)</span></td></tr><tr class="even"><td>- 计算 <span class="math inline">\(\nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right)r(\tau)\)</span> 的均值，作为策略梯度 <span class="math inline">\(\nabla_wJ(w)\)</span> 的估计</td></tr><tr class="odd"><td>- 让 <span class="math inline">\(w\)</span> 沿着策略梯度的方向前进：<span class="math inline">\(w = w + \alpha\,\nabla_{w} J(w)\)</span></td></tr><tr class="even"><td>直到收敛</td></tr></tbody></table><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfrynopsj30tp0ga77s.jpg" style="zoom:50%;" /></p><h1 id="actor-critic">Actor-Critic</h1><h2 id="ac的出发点">AC的出发点</h2><p>上述策略梯度式 <span class="math inline">\((\ref{7})\)</span> 可以理解为：“用带有权重的训练集去训练策略网络”，对于每一步决策，我们用一个能衡量这步决策好坏的“学习权重” <span class="math inline">\(r(\tau)\)</span> 去”促进“或”抑制“当前轨迹 <span class="math inline">\(\tau\)</span> 上的所有决策 <span class="math inline">\(\pi_{w}(a \mid s)\)</span> 。<span class="math inline">\(r(\tau)&gt;0\)</span> ，则”促进“这个轨迹上的所有策略；<span class="math inline">\(r(\tau)&lt;0\)</span> ，则”抑制“这个轨迹上的所有策略。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrbg6to5vj318z0ch45l.jpg" /></p><p>但是，即使 <span class="math inline">\(r(\tau)&gt;0\)</span> ，轨迹 <span class="math inline">\(\tau\)</span> 上也有可能出现少量差的决策，如果采用上面的方法，会同时”促进“这些差的决策。<strong>在采样样本比较有限的情况下，这可能会导致巨大的均方误差</strong>。</p><p>一个最自然的想法是，<strong>我们不应该将一个 <span class="math inline">\(\tau\)</span> 上所有 <span class="math inline">\((s, a)\)</span> 编成一个batch，用一个统一的“权重” <span class="math inline">\(r(\tau)\)</span> 来衡量它们的好坏。而应该找出一个“权重”能够单独衡量每一个 <span class="math inline">\((s, a)\)</span> 的好坏</strong>。</p><h2 id="对策略梯度的优化">对策略梯度的优化</h2><h3 id="不要受过去的影响dont-let-the-past-distract-you">不要受过去的影响（Don’t Let the Past Distract You）</h3><p>回顾我们的策略梯度表达式 <span class="math inline">\((\ref{7})\)</span> ，他将“轨迹”上<strong>每个动作</strong>的对数概率都乘了一个“权重” <span class="math inline">\(r(\tau)\)</span> （曾经与将来所有奖励的总和）。</p><p>但这没有多大意义。智能体实际上仅应根据其<strong>采取动作后的 <em>结果</em> 强化动作</strong>。采取动作之前获得的奖励与该动作的效果无关。这种直觉体现在数学上，可以证明策略梯度也可以表示为： <span class="math display">\[\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \sum_{t‘=t}^{T}R(s_{t&#39;},a_{t&#39;},s_{t&#39;+1}) \right] \\= E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \hat{Q}(s_t,a_t) \right]\label{9}\]</span> <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjq3p6w8muj30k70cxwgf.jpg" style="zoom:60%;" /></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfnq05f7j30uv0gzgpc.jpg" style="zoom:50%;" /></p><p><strong>为什么这样做会更好</strong>？策略梯度的关键问题是需要多少个样本轨迹才能获得它们的低方差样本估计。 我们从公式开始就包括了与过去的奖励成比例的强化动作的项， 它们均值为零，但方差不为零：导致它们只会给策略梯度的样本估计值增加噪音。 通过删除它们，<strong>减少了所需的样本轨迹数量</strong>。</p><h3 id="hatqs_ta_t-的baseline"><span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 的Baseline</h3><p>式 <span class="math inline">\((\ref{9})\)</span> 用当前动作的”状态-动作“价值函数的估计 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 作为衡量本次决策的”权重“，看起来是很合适的，但是必须考虑这样一种情形：在回报都是大于零的环境中（例如贪吃蛇游戏，把游戏结束时蛇身的长度作为回报），<strong><span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 会是恒正的值</strong>，按照上面的思路，<strong>即使是一个很差的决策（例如游戏结束时蛇身长为3），策略梯度也会在”权重“ <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 的作用下比较缓慢的”促进“这个决策</strong>。</p><p>但是，如果我们采样了 <span class="math inline">\(N\)</span> 条轨迹，就可以<strong>用 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 减去自己的均值，使得比均值小的 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 变为负，比均值大的 <span class="math inline">\(\hat{Q}(s_t,a_t)\)</span> 变为正，”赏罚分明“的进行训练策略</strong>。</p><p>那么，给式 <span class="math inline">\((\ref{9})\)</span> 策略梯度的 <span class="math inline">\(\hat{Q}(s_t,a_t)+b(s_t)\)</span> 后该式还成立吗？可以证明它仍然是成立的。</p><p>由ELPG引理可得： <span class="math display">\[E_{w}\left[\nabla_{w} \log \left(P_{w}(x)\right) b\right]=\int_{x} \nabla_{w} P_{w}(x)b\  dx = b \nabla_{w} \int_{x} P_{w}(x)\  dx=0\]</span> 所以： <span class="math display">\[\nabla_{w} J(w) = E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-b(s_t)\right) \right]\]</span> 其中 <span class="math inline">\(b(s_t)=\frac{1}{N} \sum_i^N Q_{i,t}\)</span> ，它可以看作是对状态值函数 <span class="math inline">\(V(s_t)=E_w(Q(s_t,a_t))\)</span> 的估计，因此<strong>策略梯度可以表示为</strong>： <span class="math display">\[\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) \left(\hat{Q}(s_t,a_t)-V(s_t)\right) \right]\label{12}\]</span></p><p>而由Bellman Equation知道：<span class="math inline">\(A(s_t,a_t)=Q(s_t,a_t)-V(s_t)\)</span> ，因此策略梯度又可以表示为： <span class="math display">\[\nabla_{w} J(w) \approx E_{w} \left[\sum_{t=0}^{T} \nabla_{w} \log \pi_{w}\left(a_{t} \mid s_{t}\right) A(s_t,a_t) \right]\label{13}\]</span> 由于： <span class="math display">\[Q(s_t,a_t) = r(s_t,a_t)+V(s_{t+1}) \\A(s_t,a_t) \approx r(s_t,a_t)+V(s_{t+1})-V(s_{t})\]</span> 所以，计算策略梯度式 <span class="math inline">\((\ref{12})\)</span> 的关键是计算 <span class="math inline">\(V(s_t)\)</span> 。实际上，无法精确计算 <strong><span class="math inline">\(V(s_t)\)</span> ，通常这是通过神经网络 <span class="math inline">\(V_{\phi}(s_t)\)</span> 来近似的（Value net）</strong>。该神经网络会与策略同时进行更新（以便价值网络始终近似于最新策略的值函数）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqc1z6kmgj321i0n2e2s.jpg" style="zoom:20%;" /></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqfpvb4qaj30vg0g377r.jpg" style="zoom:50%;" /></p><h3 id="value-net怎么更新">Value net怎么更新</h3><p>Value net的目的是估计 <span class="math inline">\(V(s_t)\)</span> ，那么最小化它们之间的均方误差就可以作为一个监督信号，用来在神经网络中反向传播学习 <span class="math inline">\(V_{\phi}\)</span> 。 <span class="math display">\[\begin{eqnarray*}L(\phi) &amp;=&amp; \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-V(s_{i,t}) \right\|}^2 \\&amp;=&amp; \frac{1}{N}\sum_{i}^{N} {\left\| V_{\phi}(s_{i,t})-\left(r(s_{i,t},a_{i,t})+V_{\phi}(s_{i,t+1})\right) \right\|}^2\end{eqnarray*}\]</span> 至此，Value net可以通过训练，很好的估计 <span class="math inline">\(V(s_t)\)</span> 。将它替代VPG中的 <span class="math inline">\(r(\tau)\)</span> 作为”权重“，指引策略网络学习最优策略。同时也达到了本节开始提出的目的——找出一个能够单独衡量每一个 <span class="math inline">\((s, a)\)</span> 好坏的“权重”。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqdcs4up3j30s60dvgnb.jpg" style="zoom:50%;" /></p><h2 id="actor-critic算法">Actor-Critic算法</h2><p>AC算法的大体框架是这样的：我们定义两个神经网络：一个是用来计算 <span class="math inline">\(V_{\phi}(s_t)\)</span> 价值网络，另一个则是策略网络。我们用策略网络与环境交互产生许多数据集，并用这些数据集同时训练两个网络，提升网络的性能。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjqemesrcyj31bs0rqdpk.jpg" /></p><table><colgroup><col style="width: 100%" /></colgroup><thead><tr class="header"><th>Actor-Critic</th></tr></thead><tbody><tr class="odd"><td>构造并初始化Value net的参数 <span class="math inline">\(w\)</span> 和Policy net的参数 <span class="math inline">\(\phi\)</span></td></tr><tr class="even"><td>重复以下步骤：</td></tr><tr class="odd"><td>- 通过Policy net与环境交互产生数据集 <span class="math inline">\((s,a,r,s&#39;)\)</span></td></tr><tr class="even"><td>- 训练Value net：让 <span class="math inline">\(\phi\)</span> 沿着使损失 <span class="math inline">\(L(\phi)=\left\| V_{\phi}(s)-(r+V_{\phi}(s&#39;)) \right\|^2\)</span> 下降的方向前进</td></tr><tr class="odd"><td>- 训练Pollicy net：让 <span class="math inline">\(w\)</span> 沿着梯度 <span class="math inline">\(\nabla_wJ(w)\)</span> 的方向前进</td></tr><tr class="even"><td>直到收敛</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> algorithm </tag>
            
            <tag> PG </tag>
            
            <tag> AC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL落地方法论4】状态空间</title>
      <link href="/blog/post/63561af6.html"/>
      <url>/blog/post/63561af6.html</url>
      
        <content type="html"><![CDATA[<p>DRL的状态信息代表了agent所感知到的环境信息，以及因自身的action带来的变化。状态信息是agent制定决策和评估其长期收益的依据，而状态设计的好坏直接决定了DRL算法能否收敛、收敛速度以及最终性能，兹事体大，不可不察。</p><a id="more"></a><h2 id="前言">前言</h2><p>DRL的状态信息代表了agent所感知到的环境信息，以及因自身的action带来的变化。状态信息是agent制定决策和评估其长期收益的依据，而状态设计的好坏直接决定了DRL算法能否收敛、收敛速度以及最终性能，兹事体大，不可不察。通常在一些公共平台，如Gym，大部分domain的状态空间都是现成的，学者们在上边比的是谁的算法收敛快、性能好；然而，在实际项目中，状态空间设计工作却要自己来，根据我的个人经验，增加一个优秀的新状态信息所带来的性能提升明显高于其他方面的工作（如调参），性价比非常高，因此状态空间的优化工作几乎贯彻项目始终。</p><p>大家注意到我多次使用“设计”这个词，这不就是特征工程（feature engineering）嘛，9102年都快过完了，怎么还搞这一套？直接上深度神经网络呀！真要那么简单就好了……把所有原始信息一股脑堆砌起来，让神经网络去挑选其中有用的成分并学习它们与决策间的相关性，原理上是没毛病的，可端到端的DRL学习效率实在不太给力，比有监督学习差老远了，即使经过大量训练神经网络能够最终提取到有用信息，因为训练时间的延长也会导致算法实用性的下降。更糟糕的是，一些不相关的干扰信息还会起到反作用。因此，要想在可控时间内得到比较好的policy，的确需要人为筛选出一些好的状态信息，可以是raw information，也可以是经过二次加工的信息，帮助神经网络更轻松准确地建立起决策相关性。</p><h2 id="状态设计的四个步骤">状态设计的四个步骤</h2><p>那么具体该如何做呢？我把状态空间设计的精髓总结成以下4个步骤：<strong>任务分析，相关信息筛选，统一性考虑，效果验证</strong>。</p><h3 id="任务分析">任务分析</h3><p>任务分析是状态设计的灵魂，好的状态信息建立在对任务逻辑的深入理解之上。客户提出最终目标，优秀的算法工程师需要把这个目标进一步分解，研究该目标的本质是什么，要实现它涉及到哪些重要环节，每个环节有哪些影响因素，每个因素又由哪些信息体现。对任务逻辑的深入分析也有助于我们设计优秀的回报函数（reward），并反哺状态空间的设计。对一个复杂任务的理解，除非天赋异禀或者相关经验丰富，一般都是要经过一段时间的摸爬滚打后才会深入到一定程度，期间还可能不断推翻之前的错误认知，更伴随瞬间顿悟的喜悦，因此要保持足够的耐心。</p><figure><img src="https://pic4.zhimg.com/80/v2-01caa34cfa02775aea2338356345965f_1440w.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>为了便于说明，这里引入一个简单任务场景：在一个遍布障碍物的平面区域内有若干辆小车在随机位置待命，现在要求它们以最短时间行驶到各自的终点位置停下来，期间避免与障碍物或者其他小车发生碰撞。我们可以把上述目标分解成三部分：1.达到终点，2.避免碰撞，3.用时短。到达终点要求小车知道自己在哪，还要知道终点在哪；避免碰撞要求小车知道附近障碍物的位置，自己和周围其他小车的位置及运动状态；用时短要求小车少绕路，行驶速度快，尽量避免减速和刹车。</p><h3 id="相关信息筛选">相关信息筛选</h3><p>带着以上分析，我们就可以进入下一个环节——相关信息筛选。顾名思义，就是在所有可用信息中找出与任务目标、子目标有关的那些。我们都知道RL任务逻辑最终是以回报函数（reward）为载体呈现的，而RL算法优化的则是该reward系统下的长期累计收益。神经网络的作用是将原始状态信息经过层层非线性提炼后转化为与长期收益高度关联的形式，并进一步指导生成action决策。理想情况下，状态空间应该完全由筛选出的相关信息组成。某个状态信息所代表的事件在越短时间内得到反馈，神经网络就越容易学会如何对其进行加工并建立起决策相关性。按照这个反馈时间的长短，我们还可以粗略地将这些相关信息分为直接相关信息和间接相关信息。</p><h4 id="直接相关信息">直接相关信息</h4><p>所谓直接相关信息，就是与某个reward奖励项或惩罚项即时联动的信息。比如为了更有效地避免小车之间发生碰撞，回报函数里设计了“最近小车距离反比”惩罚项 <span class="math inline">\(-\alpha\cdot max(D-d_{min})\)</span> ，其中 <span class="math inline">\(D\)</span> 是靠近惩罚阈值，当agent与周围最近小车距离 <span class="math inline">\(d_{min}\)</span> 小于 <span class="math inline">\(D\)</span> 时，即开始反比惩罚，靠得越近罚得越多。这里的 <span class="math inline">\(d_{min}\)</span> 相对于该惩罚项就属于直接相关信息，agent在每一步都能收到与 <span class="math inline">\(d_{min}\)</span> 线性相关的反馈，很容易建立起决策相关性。再举一个例子，我们希望小车在电量不足时主动停止工作转而去指定地点充电，除了在reward中设置电量不足的惩罚项，同时也应该在状态空间中增加当前剩余电量，小于设定阈值时即在每步做惩罚。这也属于直接相关信息，没有这个信息，小车是无法建立起电量不足与该惩罚的相关性的，也就无从学会主动去充电。</p><figure><img src="https://pic4.zhimg.com/80/v2-693ce767b89268e291f373e7f1baa443_1440w.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>直接相关信息不仅对DRL算法学习很友好，在有对口reward奖励/惩罚项的前提下，对算法工作者来说也更容易设计。事实上，DRL的状态空间设计往往和reward设计同时进行，为了达到某个目的需要增加一项奖励/惩罚，并相应增加一个或多个直接相关状态信息，帮助模型识别现象与反馈之间的因果关系，这一设计理念很直观也很有效。</p><h4 id="间接相关信息">间接相关信息</h4><p>间接相关信息指的是reward中没有即时联动项的状态信息，其所代表的事件需要一段时间后才得到反馈。相对于直接相关信息，DRL利用它们建立决策相关性的难度更高，学习效率更差。比如下图中的游戏画面，agent要通关必须先吃钥匙，假如reward没有专门设置“吃钥匙”的奖励项，那么吃钥匙的好处要等到通关的时候才能体现出来。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221220441.png" /></p><p>再比如小车要学会到达终点，当前位置坐标、朝向、速度、加速度、终点坐标、周围障碍物的分布似乎都与到达终点这一目标有关，假如我们暂时没有针对这个目标做更细化的credit assignment，而只设置了到达终点奖励，那么这些信息就都属于间接相关信息，agent只有经过充分探索后才能发现某时刻这些信息的变化与最终达到终点之间的联系。</p><p>间接相关信息通过某些手段可以转化为直接相关信息，从而提高DRL的学习效率。最简单的方法是对任务目标做更详细的credit assignment并增加相应的reward奖励/惩罚项，如果某状态信息恰好与之即时联动，相应状态信息就成为了直接相关信息。还以小车为例，如果在reward中增加靠近终点奖励或远离终点惩罚，那么小车的朝向（配合小车当前坐标和终点坐标）就成为了直接相关信息。更多关于reward函数设计的内容我在下一篇中再详细介绍。</p><h4 id="相关信息预处理">相关信息预处理</h4><p>无论是直接相关还是间接相关，原始信息都要经过神经网络的提炼才能转化为action输出，提炼难度与学习效率和最终性能呈反向相关。如果我们提前对原始信息做些二次加工，人为提炼出与学习目标更相关的因素，相当于替神经网络干了一部分活儿，虽然不那么elegant，但往往能收到奇效。举个极端例子，直接告诉agent钥匙的相对坐标在哪儿，一定比神经网络通过原始图像更容易学到吃钥匙的操作。由于强化学习的优化目标是折扣累加的长期收益，这使得reward起作用的方式较为间接，无法像有监督学习那样为神经网络的feature extraction提供很好的指导，这也是DRL训练效率低下的根本原因。因此，我们在状态空间上多下一点功夫，DRL学习的难度就降低一点。在资源有限的情况下这很有可能就是训不出来和训得出来的区别，也有可能是性能不达标与性能达标的区别。</p><h3 id="统一性考虑">统一性考虑</h3><p>当我们已经筛选出了所有相关信息，接下来该以何种形式使用它们呢？把他们拉成一组向量塞到神经网络里行不行？当然可以，但那样做只能得到适用于当前特定场景的policy。比如状态信息中包含了其他小车的信息，则训练出的policy只适合特定数量小车的任务，假如车数增加或减少，输入向量维度随之变化，policy就没法用了。因此，我们必须合理设计状态信息使其对环境主要因素的改变有最起码的兼容性，我把它称之为统一性考虑。具体地，这里的统一性又包含形式统一和逻辑统一。</p><h4 id="形式统一">形式统一</h4><p>为了保证输入向量长度恒定，我们需要找到一种统一形式把不同信息填到对应的位置。比如小车周围装了一圈测距雷达，按固定顺序输出一维距离向量，那么无论把小车放到什么地方，这些信息所代表的含义也不会变；或者采用imagelike的状态表示方式，把地图信息网格化，无论是作为二维channel或拉成一维向量，都能保持外在形式的统一。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221220745.png" /></p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210221220554.png" /></p><p>针对上述第二种方案，为了将信息离散化到网格点上，不可避免地会带来精度损失。但在实际应用中，只要网格尺寸合理，这样的精度损失是可以接受的。事实上，离散化操作本身会在一定程度上降低学习难度从而带来性能的提升，有paper报告在Montezuma's Revenge游戏里通过离散化agent和环境的信息，DRL模型性能不仅没有下降，反倒提升了30%多。</p><h4 id="逻辑统一">逻辑统一</h4><p>状态空间只做到外在形式统一是不够的。比如我们把小车当前位置 <img src="https://www.zhihu.com/equation?tex=%5Cleft%28x_%7B0%7D%2Cy_%7B0%7D%5Cright%29" alt="[公式]" /> 和终点位置坐标 <img src="https://www.zhihu.com/equation?tex=%5Cleft%28x_%7B1%7D%2Cy_%7B1%7D%5Cright%29" alt="[公式]" /> 作为状态信息同时输入网络，按照DRL的过拟合天性，神经网络最终会记住这张地图每个坐标处的特征以及在这里通行的最佳路线，policy在这幅地图里测试性能会很高，但换幅地图就完全不能用了。通常情况下，我们并不希望DRL用这种方式获得高性能，而是希望它能学会应对不同地形的通用知识，即使换张地图也至少能达到“勉强能用”的地步，再通过在新地图中finetune即可快速具备实用价值。因此，更合理的方式是将两个绝对坐标合并为一个相对坐标<img src="https://www.zhihu.com/equation?tex=%5Cleft%28x_%7B1%7D-x_%7B0%7D%2Cy_%7B1%7D-y_%7B0%7D%5Cright%29" alt="[公式]" /> ，即终点位置在小车坐标系中的坐标，这样就可以使policy与具体地图“脱钩”，从而学习到更加通用的导航知识。可见，<strong>要想让网络学到我们希望它学到的知识，前提是输入正确形式的状态信息</strong>。</p><p>再说回上一节的自动充电任务，如果输入的是绝对电量，而不包含低电量阈值（预警电量），DRL模型需要通过大量探索，根据当前电量与是否被惩罚的经验去摸索出预警电量是多少，并用于指导action的生成。这个隐性阈值会固化到网络参数中，如果客户后续希望提升预警电量，policy就又要用新阈值重新训练了。为了避免这种情况，我们可以把绝对电量改为相对电量（绝对电量/预警电量），能够直接反映当前电量与预警电量的关系，即使预警电量被改变也不影响模型的使用，因为此时固化到网络参数中的知识不再是某个电量阈值而是比例阈值。</p><h3 id="效果验证">效果验证</h3><p>当我们设计好状态空间或对原状态空间进行修改后，接下来需要通过实验验证其是否达到预期效果。验证方法可以分为三类：模仿学习验证，直接验证和缺省验证。</p><h4 id="模仿学习验证">模仿学习验证</h4><p>如果项目已经有一个较好的baseline，可以搭建一个policy网络，专门模仿该baseline在各种状态下的action，如果状态中包含了正确决策所需的相关信息，那么得到的policy性能就会越接近baseline。考虑到有监督学习的高效性，这是验证状态信息有效性的一种较快方式，尤其适用于项目初期一片懵懂的时候。</p><h4 id="直接验证">直接验证</h4><p>如果没有这样的baseline，那就只能用直接验证了，即用DRL训练一个policy并验证其效果。为了提升效率，可以只比较训练中途（固定步数、固定数据量）的性能，因为一般情况下好状态和差状态的won-lost关系在较早的时候就确定了。另外由于DQN收敛速度相对较快，可以优先考虑用来验证新状态。</p><p><img src="../../../../../../Documents/1img/blog/%E6%88%AA%E5%B1%8F2021-02-21%20%E4%B8%8B%E5%8D%8810.08.12.png" /></p><h4 id="缺省验证">缺省验证</h4><p>当我们已经训练得到一个不错的policy时，可以用缺省的方式验证每个状态信息的作用大小，即正常输入其他信息，而将目标信息取合理区间内的定值（如区间中点），测试性能损失的百分比。损失越大说明该状态信息越关键，反之则说明作用越边缘化，有时候甚至会发现性能不降反升，说明该信息有干扰作用，还是去掉的好。缺省验证的意义在于，剔除那些无用或起反作用的状态，为进一步优化关键状态和弱作用状态提供指导。</p><h2 id="总结">总结</h2><p>与学术研究不同，在DRL落地工作中，状态空间设计是如此的重要，所以我用了很长的篇幅探讨了其中各种细节。此外，尽管我已经十分克制，但仍然不得不引入了大量关于回报函数（reward）的描述和设计理念，这是因为在实践中，状态空间和回报函数的设计几乎是水乳交融的，很难做到泾渭分明，往往修改了其中一个，另一个也需要相应做出改变。在下一篇中，我将集中介绍回报函数的设计，当然难免也会涉及到一些状态空间设计的内容，总之，一起服用效果更佳~</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL落地方法论3】动作空间</title>
      <link href="/blog/post/82d3f6a1.html"/>
      <url>/blog/post/82d3f6a1.html</url>
      
        <content type="html"><![CDATA[<p>在将DRL应用于实际项目时，可能最轻松愉快的部分就是动作空间定义了。倒不是因为这项工作简单，而是agent的控制方式往往早就定死了，留给我们发挥的空间很小，就好像我们无法决定DOTA里允许多少种操作，也无法改变一台机器人的关节数量和各自的角度范围，Gym用户甚至从来都不用为这个问题操心，action空间有多少维，连续还是离散，各种domain早就都定义好了，我们根据这些性质判断任务的难度，仅此而已。选择困难症患者表示松了一口气有木有~~~当然咯，如果运气足够好，agent提供了多种控制选项并允许我们自由选择时，一定要珍惜这种机会。</p><a id="more"></a><h2 id="对动作空间的两个要求">对动作空间的两个要求</h2><h3 id="完备性">完备性</h3><p>动作空间首先要提供实现预期目标的可能性，比如一辆车必须具备加减速、转弯和刹车功能才可以实现导航和防撞任务。动作空间应尽可能简单高效，为了降低训练难度提升算法性能，尽量选择<strong>离散动作空间，即互斥、可穷举，能够表示成one-hot形式</strong>，这样无论用DQN或A3C训练都很方便。即使agent本身是连续控制方式，也可以尝试将区间离散化，担心精度就分得细一些，这样做是有可能取得比连续控制更好的性能的。如果必须采用连续动作空间，须注意把各维度区间归一化，即映射到[-1,1]之间，在policy网络输出层加个tanh激活（DDPG就是这么做的），从而避免因实际数值不统一造成的学习困难。</p><h3 id="合法性">合法性</h3><p>还有一点值得注意，在DRL应用中并不是所有action在任何state下都有效，比如AlphaGo就不能在棋盘上已经被占据的位置落子，骑自行车时也最好别倒蹬车，自动驾驶车辆遇到行人时绝对不能撞上去。<strong>对于特定状态下规则不允许出现的action或者引发严重后果的action，我们应该直接屏蔽掉</strong>。DRL与其他AI算法一样，都属于统计学范畴，我们在理解policy输出时也应该使用概率思维，即使agent学会在99.99%的情况下输出合法action，但仍存在0.01%的可能性输出非法action，与其寄希望于DRL完全学会遵守规则，不如加一层“硬保险”来得靠谱。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225214617.jpeg" /></p><h2 id="结语">结语</h2><p>关于动作空间的内容不多，DRL算法工作者的主战场在其他方面，比如下一篇我们将要介绍的重头戏——状态空间设计。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL落地方法论2】算法分析</title>
      <link href="/blog/post/3a9bccd9.html"/>
      <url>/blog/post/3a9bccd9.html</url>
      
        <content type="html"><![CDATA[<p>虽然每年RL方向的paper满天飞，但真正具有普遍实用价值的突破性工作实在不多，大多数还是在经典框架基础上的改进和扩展。DRL常规武器库里的存货主要还是老三样：DQN，DDPG和A3C，它们是深度学习时代最成熟、最能体现智慧结晶的三个DRL框架，我们可以在GitHub上找到无数相关代码，有OpenAI，DeepMind和Nvidia这些大公司的，也有个人爱好者的。对于DRL初学者，它们是最佳的敲门砖；对于算法研究者，它们是最厚实的“巨人肩膀”；对于算法工程师，它们是最顺手的试金石。当然，这三个算法框架都有各自的特点和适用domain，结合对项目的分析，是可以提前评估最合适的算法的。</p><a id="more"></a><h2 id="强化学习探索和利用的平衡游戏">强化学习——探索和利用的平衡游戏</h2><p>总体来说，<strong>强化学习是一个探索（Exploration）和利用（Exploitation）的平衡游戏</strong>，前者使agent充分遍历环境中的各种可能性，从而有机会找到最优解；后者利用学到的经验指导agent做出更合理的选择。两者之间可以说是相爱相杀的关系：</p><ol type="1"><li>充分的探索才能带来有效的利用，从而使RL走在正确的道路上。对于那些难度特别高的任务，改进探索策略是性价比最高的手段，比如AlphaGo使用蒙特卡洛决策树征服了围棋，Go-Explore利用状态回访打爆了Montezuma's Revenge；</li><li>充分的利用才能探索到更好的状态，agent往往需要掌握基本技能，才能解锁更高级的技能。就好像小孩先要学会站起来，才能学会走，然后才能学会跑。这种从易到难、循序渐进的思想在RL中也很受用，著名的Curriculum Learning就是由此而来；</li><li>过量的探索阻碍及时的利用。如果随机探索噪声强度过高，已经学到的知识会被噪声淹没，而无法指导agent解锁更好的状态，导致RL模型的性能停滞不前；</li><li>机械的利用误导探索的方向。如果刚刚学到一点知识就无条件利用，agent有可能被带偏，从而陷入局部最优，在错误道路上越走越远，在训练早期就扼杀了最好的可能性。</li></ol><p><strong>强化学习的训练过程其实就是从以探索为主到以利用为主的过渡过程</strong>，训练早期通过广泛试错找准一个方向，然后沿着该方向一路试探下去直到达到最优。请牢牢记住这“两点一线”，因为这是所有RL算法的主要内容，任何RL算法都能以此为切入点进行解构，有助于不断加深对算法的理解。接下来就谈谈对三个主流DRL框架的一些浅见。</p><h2 id="dqn">DQN</h2><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225135416.png" /></p><p>DQN是借助AlphaGo最早成名的深度强化学习算法，其核心思想是利用Bellman公式的bootstrap特性，不断迭代优化一个 <span class="math inline">\(Q(s,a)\)</span> 函数，并据此在各种状态下选择action。其中 <span class="math inline">\(Q(s,a)\)</span> 函数拟合的是一对状态-动作的长期收益评估，该算法没有显式的policy。DQN探索和利用的平衡靠的是一种称为<code>ε-greedy</code>的策略，针对最新的 <span class="math inline">\(Q(s,a)\)</span> 函数和当前的输入状态 <span class="math inline">\(s\)</span> ，agent做决策时以概率 <span class="math inline">\(p\)</span> 随机选择action，而以 <span class="math inline">\(1-p\)</span> 的概率选择使 <span class="math inline">\(Q(s,a)\)</span> 最大的action，随着 <span class="math inline">\(p\)</span> 从大到小变化，DQN也相应地从“强探索弱利用”过渡到“弱探索强利用”。DQN的原理使其天然地适合那些具备有限action集合的任务，说白了就是action可以穷举，比如走迷宫的agent只允许前后左右4个动作，下围棋的AlphaGo只允许 <span class="math inline">\(19*19=361\)</span> 个落子位置（实际还要排除已经落子的网格点）。这是一个重要的特征，如果是一个连续控制任务，action在某区间内有无数种可能，那就不适合用DQN了。当然，我们可以选择把区间离散化，这样就可以应用DQN了，也曾有paper报告这样做在某些任务中可以比连续控制取得更好的性能。</p><p>DQN属于off-policy方法，所谓off-policy是指用于计算梯度的数据不一定是当前policy/Q下采集的，DQN使用一个叫replay buffer的FIFO结构，用于存储transition，即 <span class="math inline">\((s,a,s&#39;,r,q)\)</span> ，每次随机从buffer中拿出一个batch用于梯度计算和参数更新。Replay buffer是稳定DQN训练的重要措施，历史数据的重复使用也提高了其数据利用率，对于那些数据比较“贵”的任务，比如Google的抓取应用，这一点非常重要，事实上Google除了replay buffer，还专门搞了个数据库，把之前存储的另一个抓取应用采集的数据拿出来做预训练，精打细算到了极致，真是比你有钱，还比你节约~。</p><p>DQN的缺点挺多，有些是RL的通病，比如对超参数敏感，笔者在训练篇会详细介绍；另一些是DQN所独有的，比如overestimation造成的训练不稳定问题，近些年学术界有不少工作是围绕这一点做出改进。此外，DQN还有off-policy方法的通病，对历史数据的重复利用虽然可以提高数据效率，但有个前提条件是环境model不能发生变化，single agent任务较易满足这个条件，但multi agent场景就未必了，对任意agent而言，其他agent也是环境的一部分，而他们的学习进化会改变这个环境，从而使历史数据失效，这就是MARL领域著名的环境不稳定问题，此时off-policy方法的性能往往不如on-policy方法。 <span class="math display">\[loss = (r + \gamma \max\limits_{a&#39;}Q(s&#39;,a&#39;)-Q(s,a))^2\]</span> 如果超参数设置合适，DQN收敛速度相对是比较快的，因为更新公式里有个<span class="math inline">\(max\)</span>操作，意思是计算目标值时s’状态下使用使<span class="math inline">\(Q(s’,a)\)</span>最大的a从而达到加速收敛的目的（上边说到的overestimation问题也源自于此……）。因此，DQN可以作为在新项目场景中快速测试效果的首选框架，比如测试一个新状态信息或一个新reward项是否有用。</p><h2 id="ddpg">DDPG</h2><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225141747.png" /></p><p>针对DQN无法处理连续控制任务的空白，DDPG在DQN的基础上做了改进，引入了一个输出连续action的显式policy，与 <span class="math inline">\(Q\)</span> 函数组成Actor-Critic结构，更新policy网络的梯度完全来自于 <span class="math inline">\(Q\)</span> 网络，目标是最大化当前的 <span class="math inline">\(Q\)</span> 函数。 <span class="math inline">\(Q\)</span> 函数的更新与DQN类似，只是计算 <span class="math inline">\(s’\)</span> 状态下目标值时放弃了 <span class="math inline">\(\max\)</span> 操作，而采用当前policy网络的输出<span class="math inline">\(π(a|s&#39;)\)</span>。DDPG名字里的第一个<span class="math inline">\(D\)</span>是Deterministic的缩写，意思是确定性的，这是有意与正宗Actor-Critic方法（如A2C，A3C等）区分开，后者policy输出的是action的概率分布，而DDPG输出的就是确定性的action。正因为如此，DDPG采用了独特的探索方式，即在action输出直接加上一个noise，该noise的强弱决定了探索力度，本质上相当于以当前action为中心形成了一个概率分布，每次更新都使policy向该分布中更好的方向演化，直到action达到了最优，此时对应分布内其他方向都是更差的方向，policy输出也就稳定在最优action附近了，从而实现了探索和利用的平衡。</p><figure><img src="https://pic3.zhimg.com/v2-8e4654480ae1bd802293347ad8cd4902_b.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>能用于连续控制任务自然是招人喜欢的，毕竟实际控制任务的变量往往都是连续取值的，比如角度、位移、速度、加速度、电流、电压等等。学者们把DDPG用在MuJoCo上，解决了很多连续domain的任务，后来也有人把它用到真实的软体章鱼机器人上，用两只触角实现了向前运动。然而，在连续区间上找到最优的确定性action输出本身是一件非常困难的事，导致<strong>DDPG在action维度较高的复杂任务中表现不佳</strong>，比如KUKA iiwa机器人有7个自由度，使得探索空间一下大了很多，训练难度陡升。同时policy网络的梯度完全来自于<span class="math inline">\(Q\)</span>网络，<span class="math inline">\(Q\)</span>函数的拟合误差都直接传导给了policy，致使DDPG的训练稳定性也不足。在Google的抓取paper中，干脆抛弃了独立policy网络，做决策时随机在区间里取16个点输入<span class="math inline">\(Q\)</span>网络，然后选择<span class="math inline">\(Q\)</span>值最大的那个作为action，实验结果表明如此粗糙的做法却大大提升了训练稳定性，且性能显著优于DDPG，有点尴尬……</p><p>总结一下，如果我们面对的问题是连续控制任务，action维度又不高，可以尝试用DDPG解决，但也不要忘了离散化动作空间并用DQN训练得到更高性能的可能性。如果action维度很高，那还是别用DDPG的好。如果数据很“贵”不得不用off-policy方法的话，那就向Google学习，拿掉policy网络，直接用Q网络+启发式搜索选择action。如果数据廉价又追求高性能，我推荐用A3C框架——个人比较偏爱的一种框架。</p><h2 id="a3c">A3C</h2><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225142104.png" /></p><p>喜爱A3C的原因很简单，它带给我的成功经验最多。A3C是Actor-Critic框架A2C的扩展。A2C脱胎于经典的REINFORCE梯度方法，其policy输出的不是action，而是关于action的概率分布，因此梯度无法直接从Critic（又称为V网络）流到policy网络，只能用在线样本统计出一个近似梯度。原始REINFORCE梯度形式如 <span class="math inline">\(V(s)\nabla logπ(a|s)\)</span> ，直观解释就是使状态 <span class="math inline">\(s\)</span> 下返回高 <span class="math inline">\(V\)</span> 值的action出现概率更大。缺点是 <span class="math inline">\(V\)</span> 值绝对值不可控（与reward等因素有关），variance很大，造成训练不稳定。A2C把梯度改成 <span class="math inline">\(A(s,a)\nabla logπ(a|s)\)</span> ，<span class="math inline">\(A(s,a)\)</span> 是在线episode计算出的一对<span class="math inline">\(s,a\)</span>的Value值与当前<span class="math inline">\(V\)</span>网络估计值之差，并经过normalization操作，简单理解就是把 <span class="math inline">\(log\)</span> 前的部分做了一个居中 <span class="math inline">\(+\)</span> 归一化，variance降低，训练稳定性显著提升，这里的 <span class="math inline">\(A(s,a)\)</span> 称为 <span class="math inline">\(a\)</span> 在 <span class="math inline">\(s\)</span> 下的advantage，是A2C名字里的第一个‘A’。既然policy输出的是action概率分布，那么探索就很容易实现——按照这个分布采样即可，训练初期分布variance比较大，探索力度强，随着policy不断改善，分布variance越来越小，代表policy对所选action越来越自信，这就实现了对经验的利用。</p><p>A3C在A2C的基础上增加了对并行采样的支持，从而有效利用多核资源，在不同CPU上并行运行不同的环境种子，显著提升了训练稳定性、收敛速度以及最终性能，A3C比A2C名字里多了一个‘A’，代表asynchronous的缩写。A3C支持多种action概率分布，如果action空间是DQN那样的离散集合，可以用Categorical分布；如果是像DDPG那样的多维连续分布，可以用Multivariate Gaussian分布，此外A3C还支持伯努利分布，如果action的每一维都是非此即彼的二值选项，或者one-hot向量太长想改用二进制表示，那就是它了。可见，A3C在通用性上是显著优于DQN和DDPG的，几乎所有任务都能拿A3C跑一跑。此外，A3C作为on-policy方法，每次更新policy的梯度都由当前policy采集的样本计算，这使得A3C在Multi-agent任务里对环境不稳定性的抵抗能力比DQN和DDPG更强。</p><p>和很多paper的实验结果一致，笔者在实际应用中发现A3C+PPO的组合（PPO是一种稳定训练的算法，笔者会在训练篇详细介绍）在连续控制任务里性能显著优于DDPG，对超参数的敏感度也比DDPG低，因此训练起来更加得心应手。因此，笔者推荐在解决连续任务时首选A3C，资源不够就A2C，DDPG的优先级往后放就是了。很多离散action空间的任务也值得用A3C跑一下，跟DQN比一比。</p><h2 id="其他算法">其他算法</h2><p>以上三个DRL框架是基础，大多数情况下都至少能得到一个“能用”的policy。然而，也不应奢望它们能解决一切问题。DRL领域是个大坑，里边有太多需要解决的问题和值得挖掘的方向，比如：高难度探索，稀疏reward，数据效率，训练稳定性，快速适应新环境等等，类似MARL这样的子领域还有自己特有的问题，如环境不稳定性，scalability等等。算法工作者一定要保持开放的态度，及时跟踪学术界的新趋势新方法。每当算法性能遇到瓶颈，首先要沉下心来分析关键制约因素在哪里，如果是上述这些普遍意义上的问题造成，那就去相关方向最新paper中寻找灵感。比如探索不够充分时，可以用count-based exploration或者parameter noise来加强探索；DQN训练不稳定时，可以尝试Double-Q网络，每次选择较小Q值计算目标值，从而抑制overestimation；DQN或DDPG数据效率低时，可以用prioritized replay buffer；MARL里为了改善环境不稳定问题，可以尝试DIMAPG，……。问题无常势，算法无常形，群众智慧是无穷的，博采众长才能攻无不克。</p><p>关于算法选择就先写到这里，不同算法在训练时还有各种各样的trick和注意事项，笔者在训练篇里再详细介绍。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【RL落地方法论1】需求分析</title>
      <link href="/blog/post/b901f1b6.html"/>
      <url>/blog/post/b901f1b6.html</url>
      
        <content type="html"><![CDATA[<p>任何机器学习方法都不是包治百病的灵丹妙药，它们也有各自的“舒适圈”，有时候还相当挑剔。强化学习，无论前面带不带“深度”二字，也同样有其鲜明的优势和局限性，务必要具体问题具体分析。不是所有需求都适合用DRL做，适合用DRL做的需求也未必能超越传统方法。</p><a id="more"></a><p>在我看来，算法工程师的核心能力可以总结成以下三点：</p><ol type="1"><li>对各种算法本质及其能力边界的深刻理解</li><li>对问题内在逻辑的深入分析</li><li>对两者结合点的敏锐直觉</li></ol><p>一个优秀算法工程师的高光时刻从拒绝不合理的需求开始，其他的都是后话。不经慎重评估而盲目上马的项目不仅是对资源的巨大浪费，更让每个参与者陷在深坑中痛不欲生。知道一种算法不能干什么与知道它能干什么同样重要，对DRL而言，即使在最理想的外部条件下，也有其绕不过去的七寸——泛化无能。这是DRL的基本原理决定的，任何在这一点上提出过高要求的应用都不适合用DRL解决。</p><h2 id="drl的过拟合天性">DRL的过拟合天性</h2><p>DRL解决的是从过去经验中学习有用知识，并用于后续决策的问题。有别于纯视觉应用，DRL不仅仅满足于识别和定位，而是要根据这些信息采取针对性的行动以获取最大长期收益。从本质上说，DRL就是一种依赖过拟合的算法，说白了就是通过暴力搜索把其中的成功经验记下来，并用以指导后续决策。别嫌露骨，别怕尴尬，岂不闻学术界某大牛的辛辣讽刺仍余音绕梁——强化学习是唯一被允许在训练集上测试的算法。由于缺乏直接监督信号用于训练，DRL还特别“费数据”，以至于需要专门的模拟器源源不断地产生数据供其挥霍。好不容易训出来的policy在训练环境用得好好的，换个环境立马歇菜。</p><p>等等，不是说好了DNN有泛化能力吗？ResNet明明能在一张没见过的图片中识别出阿猫阿狗的呀。这是因为任务层次不同，泛化的定义和要求自然也不同。视觉识别层面的泛化可以理解为深度网络学习到了通用的高层语义信息，以至于只要看到类似的像素结构就能与高层语义对应起来。这个层次的泛化能力DRL也可以有，可惜远远不够。我们前边说过，DRL是要根据识别到的信息做出决策以最大化长期收益的，具体地，DRL拟合了一个特定环境、特定reward函数和当前policy下，从特定输入状态（state）到最终收益（episode结束时）的Value函数，再反过来根据这个函数去优化policy，使其输出的action最大化该Value函数。这是一个交替更新的过程，Value函数和policy你中有我我中有你，直到抵达某种纳什均衡。假设探索足够充分，我们可以认为最终的Value函数只由环境特性和reward函数决定。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225214923.png" /></p><p>OK，整理一下DRL的逻辑链条：Value函数过拟合环境特性和reward函数，Policy又过拟合Value函数。一朝天子一朝臣，一种环境一种policy，环境换了，policy废了。在训练环境中agent看到家猫，发现喂食可以得到高回报，于是用policy记了下来，在测试环境中，家猫换成野猫，虽然识别到的还是猫，agent继续喂食，然后就被猫抓了……相信看到这里，你就能明白为什么DRL被允许在训练集上测试了，也明白为什么DRL泛化无能了，因为这个level的泛化要求太TM高了！即使是人类在缺乏相关经验的情况下也会踩坑，但人类能做到吃一堑长一智，甚至从此主动避开所有可能有危险的动物，这得益于人类的两大核心优势：高效学习（只需极少样本）和终身学习（举一反三、融会贯通），而现阶段DRL却只会低效地死记硬背，训练也是一锤子买卖，这是真正的智能对人工智能的降维碾压。</p><h2 id="适用drl的五大特征">适用DRL的五大特征</h2><p>于是我们产生了两个问题：1.是不是DRL就真的一点泛化能力都没有？2.这样的DRL到底有没有实用价值？关于第一个问题我建议读者去了解一些Meta-RL（元强化学习）方向的工作，利用DNN的表征能力，agent能够利用有限的环境探索不断在线学习该环境的特性和reward函数，并据此调整policy的输出使其表现出对新环境的适应能力。但目前也仅能解决一类相似任务间的泛化问题，而代价是训练难度进一步提升，数据效率进一步下降，愿景很美好，实用有点早。第二个问题才是目前我们最关注的，换个问法，到底什么样的任务适合用DRL解决呢?答曰：<strong>场景固定，目标明确，数据廉价，过程复杂，自由度高</strong>。依次解读如下：</p><h3 id="场景固定">场景固定</h3><p>场景固定是指决定系统动态演化趋势的主要因素保持恒定。听起来可能有点抽象，举个例子，agent在环境中遇到一条峡谷，跳过去的过程中有30%的概率被落石击中，这里的30%就属于这类因素，无论在训练和工作的时候都不能改变。用符号表示：状态s=面对峡谷，动作a=跳过去，下一个状态s'的概率分布p(s'|s,a)是明确的，即安全着陆70%，壮烈牺牲30%，这个概率在RL中叫状态转移概率，很多paper又称其为环境的model，对于任何s,a和s'，p(s'|s,a)都明确而恒定的决策过程又称为Markov Decision Process或MDP，RL的理论基础即建立在MDP之上，Value函数和policy就是通过隐式（model-free）或显式(model-based)地对环境model建模得到的。model变了，policy就废了，上边举的家猫和野猫的例子就是这个道理。另一个不满足场景固定的典型例子是DOTA更换地图，基于训练地图得到的局势演化预期在新地图里不成立了。又比如临时要求围棋棋盘里某几个位置不准落子，AlphaGo大概率是要跪的。你要是还不理解，就记住：训练环境尽可能做到与工作（测试）环境相同。</p><h3 id="目标明确">目标明确</h3><p>目标明确很好理解，任务要达到何种效果清晰具体，最好可以量化。工业界的需求一般都是优化某个指标（效率、能耗、胜算等），基本满足这个条件。目标越明确，设计优质的reward函数就越容易，从而训练得到更接近预期的policy。</p><h3 id="数据廉价">数据廉价</h3><p>数据廉价对RL至关重要，毕竟挥霍数据是RL与生俱来的属性，没办法。我们知道视频游戏领域很容易满足这个条件，所以我们最常听说DRL在XX游戏里碾压、吊打、秒杀人类玩家。然而这个要求对牵涉到硬件的应用却相当不友好，Google可以用7台KUKA iiwa机器人（单价80万rmb体会一下）日夜不停跑上几个月训练抓取技能，其他公司怕是连8千的设备撞坏了都心疼，那就只剩下模拟器这一条路了。所谓模拟器，就是将真实场景中的各种物理模型（即上文提到的model）在软件环境中仿真，从而生成无限量的高仿数据。这里有一个reality gap的问题，即这些仿真model与真实世界的误差，如果太大则训练出的policy无法直接应用。一个逼真的模拟器也是要花功夫（钱）的，像MuJoCo这样的优秀仿真平台收费也是合情合理的。</p><p><img src="https://raw.githubusercontent.com/Su-Lemon/sources/master/imgs/blog/20210225215143.png" /></p><p>有心的朋友已经发现，对模拟器精度的要求其实与上文的“场景固定”逻辑上是一致的，之所以分开介绍，是因为廉价还包含了另一层意思——采样速率。AlphaGo和OpenAI Five在宣传的时候动不动就说他们的agent学习了相当于人类XX万年的经验，显然没有高速模拟器是不可能做到的。总之，如果非要做硬件相关的应用，先尽最大努力做出逼真的高速模拟器吧！</p><h3 id="过程复杂">过程复杂</h3><p>如果说前三个特征决定了“能不能”，那么接下来两个特征决定了“值不值”。我们用DRL的目的无非是看中了其处理复杂场景的能力，人类看不透，DRL来凑。如果任务太简单，依靠规则和启发式就能解决问题了，相当于拿到了“解析解”，还用神经网络拟合个什么劲儿。这里介绍一个技巧，请熟悉业务流程的甲方人员结合Domain Knowledge，分析一下阻碍性能提升的主要瓶颈在哪里，如果对方回答是过程太复杂难以掌握规律或其他类似的答复，那就说明DRL值得一试。</p><h3 id="自由度高">自由度高</h3><p>自由度高指的是选择空间大、限制少，我们人类之所以有“选择困难症”，正是因为选择太多了，这时候DRL的优势就体现出来了，通过大量探索总能拟合出不错的value函数指导policy做选择。自由度越高，DRL优势越明显，自由度越低，越有利于规则。因此在决定用DRL之前，一定要认真评估任务场景是否有足够的优化空间，千万不要拎着锤子找钉子，否则即使训出了policy，性能也不如传统算法，白忙活一场。</p><h2 id="总结">总结</h2><p>关于需求分析就先写到这里，基本涵盖了需求评估的主要方面，如果经仔细了解后发现不满足以上五个特征，就要小心了。这时候要么直接向对方提出不可行，要么在大需求里找出符合以上要求的子任务用强化学习解决，一样可以改善总体性能，切不可盲目追求大而全。好了，万里长征走完了第一步也是最重要的一步，接下来我们就假设已经拿到了一个适合又值得用DRL解决的任务，讨论如何选择合适的算法。</p>]]></content>
      
      
      <categories>
          
          <category> RL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> 方法论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Mac】超详细的个性化终端颜色及vim颜色配置</title>
      <link href="/blog/post/835dea57.html"/>
      <url>/blog/post/835dea57.html</url>
      
        <content type="html"><![CDATA[<p>Mac终端默认风格为Basic，白底黑字（黑暗模式下黑底白字）。导致输入很多命令后，想要寻找之前的命令提示行瞅瞎眼。</p><p>这里记录了终端个性化配色的设置过程，打造你的个性终端。</p><a id="more"></a><h2 id="更新"><strong>更新</strong></h2><p><strong>macOS Catalina</strong>终端默认为zsh，<strong>以下第三节</strong> 开始的配置会遇到无法应用的问题，解决方案有两种：</p><ul><li>切换为bash：系统偏好设置 -&gt; 用户与群组 -&gt; 点击左下角小黄锁图标，以解锁允许设置 -&gt; 在左侧列表单击你的用户名，出现"高级选项" -&gt; 打开高级选项，在login shell一栏选择/bin/Bash -&gt; 应用修改。</li><li>使用zsh应用下列配置（由于zsh与bash差异较大，该方法配置过程可能遇到其它问题）：在终端依次执行下列指令</li></ul><div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="bu">cd</span> ~</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="fu">touch</span> .zshrc</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="ex">open</span> .zshrc</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="co"># 在打开的.zshrc中添加source .bash_profile</span></span></code></pre></div><h2 id="默认设置的缺陷">默认设置的缺陷</h2><p>Mac终端默认风格为Basic，白底黑字（黑暗模式下黑底白字）。导致输入很多命令后，想要寻找之前的命令提示行瞅瞎眼。</p><p>这里记录了终端个性化配色的设置过程，从 <strong>偏好设置</strong> 开始，到<strong>自定义命令提示行颜色</strong>，<strong>自定义不同文件按类别显示颜色</strong>和<strong>自定义vim编辑器配色</strong>。 ## 终端偏好设置 打开终端，快捷键command + , 打开，或者菜单 “终端” – &gt;“偏好设置” -&gt; “描述文件”，打开如下设置窗口： <img src="https://img-blog.csdnimg.cn/20200313104129580.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p><p>选择了Pro主题，并修改了一些默认配置： 1. 字体为SF Mono，大小调整为14号。 2. 文本不使用粗体，而使用粗体高亮。 3. ANSI颜色，单击任一颜色可以打开调色板，修改了蓝色（明亮）的RGB等（不然之后文件夹显示颜色为默认蓝色，在黑色背景下难以看清）。 <img src="https://img-blog.csdnimg.cn/20200313105124914.png#pic_center" alt="在这里插入图片描述" /></p><ol start="4" type="1"><li>光标勾选了闪动，方便看清光标位置。</li></ol><h2 id="命令提示行设置">命令提示行设置</h2><h3 id="实现个性化配置">实现个性化配置</h3><p>先展示效果，清爽的命令提示行&gt;_&lt;，当前文件夹用红色突出显示： <img src="https://img-blog.csdnimg.cn/20200313105458643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></p><p><a href="https://download.csdn.net/download/BreakingDawn0/12246627">打包资源传送门</a>，但建议一步步自己配置。</p><p>配置过程如下：</p><ol type="1"><li><p>按顺序输入上图命令，返回用户目录，显示所有文件（包括隐藏文件）。</p><div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="bu">cd</span> ~</span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="fu">ls</span> -ah</span></code></pre></div></li><li><p>可以看到，博主已经有了一个 <strong>.bash_profile</strong>文件，它的作用是设置一些环境变量，当用户登录时，该文件仅仅执行一次！<strong>如果系统默认有该文件，那我们之后的操作都追加在原有内容之后，如果没有，就要自己创建了</strong>。</p></li><li><p>输入如下指令，进入vim编辑器后按“i”进入插入模式。（还没习惯在vim编辑的话，可以接着按ESC :wq！保存退出，然后输入指令open .bash_profile在Mac自带的文本编辑里操作）。</p><div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="ex">vim</span> .bash_profile</span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co"># 退出vim后</span></span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="ex">open</span> .bash_profile</span></code></pre></div></li><li><p>把下面ANSI转码控制的颜色配置粘贴到.bash_profile文件中，先实现效果，具体原理下一节说明。</p><div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="bu">export</span> <span class="va">PS1=</span><span class="st">&#39;\[\033[01;32;01m\]\h:\[\033[01;31;01m\]\W \[\033[01;32;01m\]\u$\[\033[01;00;00m\] &#39;</span>  # 终端命令提示行格式及颜色</span></code></pre></div></li><li><p>退出（command + q）重启Terminal就可以看到和博主一样的命令提示行配色了，（<strong>这里配置了背景透明，万一终端背景色发生了变化，也不会在命令提示行出现一个尴尬的底色</strong>）。</p></li></ol><h3 id="配置的原理">配置的原理</h3><ul><li>Mac中的一个重要的配置文件：<ul><li><code>/etc/profile</code>：每个用户登录时都会运行的环境变量设置。</li><li><code>~/.bash_profile</code>：专用于某个用户自己使用的shell信息，当用户登录时，该文件仅仅执行一次，默认情况下，它设置一些环境变量。</li></ul></li><li>要修改命令行提示符，我们通过环境变量PS1来达到目的（正如我们在3.1中所做的那样，PS1=……）。<ul><li>命令列表的参数，这些参数可以在我们上面配置的环境变量中找到。</li></ul></li></ul><table><thead><tr class="header"><th>参数</th><th>作用</th></tr></thead><tbody><tr class="odd"><td>\u</td><td>显示当前用户的用户名（user name）</td></tr><tr class="even"><td>\h</td><td>显示主机名（host name）</td></tr><tr class="odd"><td>\W</td><td>显示当前工作目录的名字（work directory）</td></tr><tr class="even"><td>$</td><td>显示$符作为提示符，如果用户是root的话，则显示#号</td></tr></tbody></table><ul><li>我们来查看一下当前的PS1，就能理解上面的参数了，输入如下指令<code>$echo echo $PS1</code>，<strong>注意，这里要在我们还没有按照3.1修改.bash_profile时的情况下操作（注释掉个性化修改），不然输出的结果就是我们修改的内容</strong>：</li></ul><div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="ex">localhost</span>:~ lemon$ <span class="va">$echo</span> echo <span class="va">$PS1</span></span><span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span><span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a><span class="co"># Mac默认配置输出的结果是：</span></span><span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="co"># \h:\W \u\$</span></span><span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a><span class="co"># 带代表Mac默认的终端命令提示行格式是：</span></span><span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a><span class="co"># 主机名:目录 用户名$ </span></span></code></pre></div><ul><li>从默认的PS1输出看到，是没有文本颜色配置的。而文本终端的颜色可以使用“ANSI非常规字符序列”来生成，（正如3.1中 <code>\[\033[01;32;40m\]\h</code>等）。<ul><li><code>[</code> 和 <code>]</code> 序列被用来封装这些非打印字符。</li><li>一个 ANSI 转义编码以一个八进制<code>033</code>（这个编码是由 退出按键产生的）开头。</li><li>其后跟着一个可选的字符属性<code>（00</code>：正常、<code>01</code>：黑体、<code>04</code>：下划线、<code>05</code>：闪烁、<code>07</code>：反转前景背景色）</li><li>最后是一个指令。前景色；背景色。</li></ul><div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="st">&#39;\[\033[01;32;01m\]\h&#39;</span></span><span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a>意思就是： </span><span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="st">&#39;\[这是一个ANSI编码[正常；前景色绿色；背景色透明 结束ANSI序列\]用户名&#39;</span></span></code></pre></div></li></ul><table><thead><tr class="header"><th>字符序列</th><th>作用</th></tr></thead><tbody><tr class="odd"><td>[、]</td><td>封装这些非打印字符</td></tr><tr class="even"><td>\033</td><td>引导非常规字符序列</td></tr><tr class="odd"><td>字符属性：00,01,04,05,07</td><td>正常，黑提，下划线，闪烁，反转前景背景色</td></tr><tr class="even"><td>m</td><td>设置属性，然后结束非常规字符序列</td></tr></tbody></table><p>这里，给出大家一些颜色序列，<strong>可以修改3.1中的配色方案</strong>，设置自己喜欢的颜色。替换字符属性、前景色、背景色对应位置的字符即可。</p><table><thead><tr class="header"><th>前景色</th><th>字符属性为<strong>00</strong></th><th>字符属性为<strong>01</strong></th></tr></thead><tbody><tr class="odd"><td>30</td><td>黑色</td><td>深灰</td></tr><tr class="even"><td>31</td><td>红色</td><td>浅红</td></tr><tr class="odd"><td>32</td><td>绿色</td><td>浅绿</td></tr><tr class="even"><td>33</td><td>棕色</td><td>黄色</td></tr><tr class="odd"><td>34</td><td>蓝色</td><td>浅蓝</td></tr><tr class="even"><td>35</td><td>粉色</td><td>浅粉</td></tr><tr class="odd"><td>36</td><td>青色</td><td>浅青</td></tr><tr class="even"><td>37</td><td>浅灰</td><td>白色</td></tr></tbody></table><table><thead><tr class="header"><th>背景色</th><th>字符属性为<strong>00</strong></th><th>背景色</th><th>字符属性为<strong>01</strong></th></tr></thead><tbody><tr class="odd"><td>40</td><td>黑色</td><td>44</td><td>蓝色</td></tr><tr class="even"><td>41</td><td>红色</td><td>45</td><td>粉色</td></tr><tr class="odd"><td>42</td><td>绿色</td><td>46</td><td>青色</td></tr><tr class="even"><td>43</td><td>棕色</td><td>47</td><td>浅灰</td></tr></tbody></table><h2 id="不同文件类型显示颜色配置">不同文件类型显示颜色配置</h2><p><a href="https://download.csdn.net/download/BreakingDawn0/12246627">打包资源传送门</a>，但建议一步步自己配置。</p><p>同样是在打开的.bash_profile文件中，粘贴下面的配置：</p><div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="bu">export</span> <span class="va">CLICOLOR=</span><span class="st">&#39;Yes&#39;</span>   # 是否输出颜色</span><span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="co"># export LS_OPTIONS=&#39;--color=auto&#39;  # 不同文件类型颜色配置，自动选择颜色</span></span><span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="bu">export</span> <span class="va">LSCOLORS=</span><span class="st">&#39;ExGxFxdaCxDaDahbadacec&#39;</span>    # 指定颜色，<span class="va">Linux</span>配色方案</span></code></pre></div><figure><img src="https://img-blog.csdnimg.cn/20200313132950333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>这里完全沿用了Linux的配色方案。</p><table><thead><tr class="header"><th>颜色</th><th>文件类型</th></tr></thead><tbody><tr class="odd"><td>蓝色</td><td>文件夹</td></tr><tr class="even"><td>绿色</td><td>可执行文件</td></tr><tr class="odd"><td>白色</td><td>普通文件</td></tr><tr class="even"><td>青色</td><td>链接文件</td></tr><tr class="odd"><td>...</td><td>...</td></tr></tbody></table><p>想要修改配色方案的话需要修改上述文件中的 <strong>LSCOLORS</strong> 变量。该变量由22个字符组成，每2个一组，分别代表一种文件类型的前景色和背景色。 这11种文件类型按顺序分别为：</p><div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="st">&#39;</span></span><span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a><span class="st">Directory</span></span><span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a><span class="st">Symbolic Link</span></span><span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a><span class="st">Socket</span></span><span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a><span class="st">Pipe</span></span><span id="cb8-6"><a href="#cb8-6" aria-hidden="true"></a><span class="st">Executable</span></span><span id="cb8-7"><a href="#cb8-7" aria-hidden="true"></a><span class="st">Block Special</span></span><span id="cb8-8"><a href="#cb8-8" aria-hidden="true"></a><span class="st">Character Special</span></span><span id="cb8-9"><a href="#cb8-9" aria-hidden="true"></a><span class="st">Executable with Setuid Bit Set</span></span><span id="cb8-10"><a href="#cb8-10" aria-hidden="true"></a><span class="st">Executable with Setgid Bit Set</span></span><span id="cb8-11"><a href="#cb8-11" aria-hidden="true"></a><span class="st">Directory Writable to Others, with Sticky Bit</span></span><span id="cb8-12"><a href="#cb8-12" aria-hidden="true"></a><span class="st">Directory Writable to Others, without Sticky Bit</span></span><span id="cb8-13"><a href="#cb8-13" aria-hidden="true"></a><span class="st">&#39;</span></span></code></pre></div><p>颜色字符为：</p><table><thead><tr class="header"><th>字符</th><th>颜色</th><th>字符</th><th>颜色</th></tr></thead><tbody><tr class="odd"><td>a</td><td>黑色</td><td>A</td><td>粗体黑色</td></tr><tr class="even"><td>b</td><td>红色</td><td>B</td><td>粗体红色</td></tr><tr class="odd"><td>c</td><td>绿色</td><td>C</td><td>粗体绿色</td></tr><tr class="even"><td>d</td><td>棕色</td><td>D</td><td>粗体棕色</td></tr><tr class="odd"><td>e</td><td>蓝色</td><td>E</td><td>粗体蓝色</td></tr><tr class="even"><td>f</td><td>洋红</td><td>F</td><td>粗体洋红</td></tr><tr class="odd"><td>g</td><td>青色</td><td>G</td><td>粗体青色</td></tr><tr class="even"><td>h</td><td>浅灰</td><td>H</td><td>粗体浅灰</td></tr><tr class="odd"><td>x</td><td>默认颜色</td><td></td><td></td></tr></tbody></table><h2 id="vim配色">vim配色</h2><p><a href="https://download.csdn.net/download/BreakingDawn0/12246627">打包资源传送门</a>，但建议一步步自己配置。</p><p>Mac默认的vim配色也是黑底白字，没有任何高亮，需要我们自行配置，这里配置vim人气超高的molokai配色方案。先上效果： <img src="https://img-blog.csdnimg.cn/2020031314585185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p><ol type="1"><li><p>进入主目录下的.vim文件夹</p><div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="bu">cd</span> ~/.vim</span></code></pre></div></li><li><p>如果没有.vim文件夹，则在主目录下创建.vim</p><div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="bu">cd</span> ~</span><span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="fu">mkdir</span> .vim</span><span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a><span class="bu">cd</span> .vim</span></code></pre></div></li><li><p>下载molokai配色方案（<strong>没有git命令的话，手动去下面链接下载，把colors文件夹放到.vim文件夹下</strong>）</p><div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="fu">git</span> clone https://github.com/tomasr/molokai.git</span><span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="fu">cp</span> -r molokai/colors ./</span></code></pre></div></li><li><p><strong>检查一下，colors文件夹（里面有molokai）是否在.vim文件夹下，一般都是这个问题。</strong></p></li><li><p>回到主目录，创建.vimrc文件。</p><div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="bu">cd</span> ~</span><span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a><span class="ex">vim</span> .vimrc</span></code></pre></div></li><li><p>将下面内容粘贴到.vimrc中，第三行colorscheme molokai即选择配色方案。保存并退出（ESC :wq!）</p></li></ol><div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">nocompatible</span> <span class="st">&quot; 关闭 vi 兼容模式</span></span><span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a><span class="st">syntax on &quot;</span> 自动语法高亮</span><span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a></span><span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a><span class="ex">colorscheme</span> molokai <span class="st">&quot; 设定配色方案</span></span><span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a><span class="st">set number &quot;</span> 显示行号</span><span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">cursorline</span> <span class="st">&quot; 突出显示当前行</span></span><span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a><span class="st">set ruler &quot;</span> 打开状态栏标尺</span><span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a><span class="kw">set</span> <span class="va">shiftwidth=</span>4 <span class="st">&quot; 设定 &lt;&lt; 和 &gt;&gt; 命令移动时的宽度为 4</span></span><span id="cb13-9"><a href="#cb13-9" aria-hidden="true"></a><span class="st">set softtabstop=4 &quot;</span> 使得按退格键时可以一次删掉 <span class="ex">4</span> 个空格</span><span id="cb13-10"><a href="#cb13-10" aria-hidden="true"></a><span class="kw">set</span> <span class="va">tabstop=</span>4 <span class="st">&quot; 设定 tab 长度为 4</span></span><span id="cb13-11"><a href="#cb13-11" aria-hidden="true"></a><span class="st">set nobackup &quot;</span> 覆盖文件时不备份</span><span id="cb13-12"><a href="#cb13-12" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">autochdir</span> <span class="st">&quot; 自动切换当前目录为当前文件所在的目录</span></span><span id="cb13-13"><a href="#cb13-13" aria-hidden="true"></a><span class="st">filetype plugin indent on &quot;</span> 开启插件</span><span id="cb13-14"><a href="#cb13-14" aria-hidden="true"></a><span class="kw">set</span> <span class="va">backupcopy=</span>yes <span class="st">&quot; 设置备份时的行为为覆盖</span></span><span id="cb13-15"><a href="#cb13-15" aria-hidden="true"></a><span class="st">set ignorecase smartcase &quot;</span> 搜索时忽略大小写，但在有一个或以上大写字母时仍保持对大小写敏感</span><span id="cb13-16"><a href="#cb13-16" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">nowrapscan</span> <span class="st">&quot; 禁止在搜索到文件两端时重新搜索</span></span><span id="cb13-17"><a href="#cb13-17" aria-hidden="true"></a><span class="st">set incsearch &quot;</span> 输入搜索内容时就显示搜索结果</span><span id="cb13-18"><a href="#cb13-18" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">hlsearch</span> <span class="st">&quot; 搜索时高亮显示被找到的文本</span></span><span id="cb13-19"><a href="#cb13-19" aria-hidden="true"></a><span class="st">set noerrorbells &quot;</span> 关闭错误信息响铃</span><span id="cb13-20"><a href="#cb13-20" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">novisualbell</span> <span class="st">&quot; 关闭使用可视响铃代替呼叫</span></span><span id="cb13-21"><a href="#cb13-21" aria-hidden="true"></a><span class="st">set t_vb= &quot;</span> 置空错误铃声的终端代码</span><span id="cb13-22"><a href="#cb13-22" aria-hidden="true"></a><span class="st">&quot; set showmatch &quot;</span> 插入括号时，短暂地跳转到匹配的对应括号</span><span id="cb13-23"><a href="#cb13-23" aria-hidden="true"></a><span class="st">&quot; set matchtime=2 &quot;</span> 短暂跳转到匹配括号的时间</span><span id="cb13-24"><a href="#cb13-24" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">magic</span> <span class="st">&quot; 设置魔术</span></span><span id="cb13-25"><a href="#cb13-25" aria-hidden="true"></a><span class="st">set hidden &quot;</span> 允许在有未保存的修改时切换缓冲区，此时的修改由 vim 负责保存</span><span id="cb13-26"><a href="#cb13-26" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">guioptions-</span>=T <span class="st">&quot; 隐藏工具栏</span></span><span id="cb13-27"><a href="#cb13-27" aria-hidden="true"></a><span class="st">set guioptions-=m &quot;</span> 隐藏菜单栏</span><span id="cb13-28"><a href="#cb13-28" aria-hidden="true"></a><span class="kw">set</span> <span class="ex">smartindent</span> <span class="st">&quot; 开启新行时使用智能自动缩进</span></span><span id="cb13-29"><a href="#cb13-29" aria-hidden="true"></a><span class="st">set backspace=indent,eol,start</span></span><span id="cb13-30"><a href="#cb13-30" aria-hidden="true"></a><span class="st">&quot;</span> 不设定在插入状态无法用退格键和 Delete 键删除回车符</span><span id="cb13-31"><a href="#cb13-31" aria-hidden="true"></a><span class="kw">set</span> <span class="va">cmdheight=</span>1 <span class="st">&quot; 设定命令行的行数为 1</span></span><span id="cb13-32"><a href="#cb13-32" aria-hidden="true"></a><span class="st">set laststatus=2 &quot;</span> 显示状态栏 <span class="kw">(</span>默认值为 <span class="ex">1</span>, 无法显示状态栏<span class="kw">)</span></span><span id="cb13-33"><a href="#cb13-33" aria-hidden="true"></a><span class="kw">set</span> <span class="va">statusline=</span><span class="dt">\ </span><span class="ex">%</span><span class="op">&lt;</span>%F[%1*%M%*%n%R%H]%=<span class="dt">\ </span>%y<span class="dt">\ </span>%0(%<span class="dt">&#123;&amp;fileformat&#125;\ </span>%<span class="dt">&#123;&amp;encoding&#125;\ </span>%c:%l/%L%)<span class="ex">\ </span></span><span id="cb13-34"><a href="#cb13-34" aria-hidden="true"></a><span class="st">&quot; 设置在状态行显示的信息</span></span><span id="cb13-35"><a href="#cb13-35" aria-hidden="true"></a><span class="st">set foldenable &quot;</span> 开始折叠</span><span id="cb13-36"><a href="#cb13-36" aria-hidden="true"></a><span class="kw">set</span> <span class="va">foldmethod=</span>syntax <span class="st">&quot; 设置语法折叠</span></span><span id="cb13-37"><a href="#cb13-37" aria-hidden="true"></a><span class="st">set foldcolumn=0 &quot;</span> 设置折叠区域的宽度</span><span id="cb13-38"><a href="#cb13-38" aria-hidden="true"></a><span class="ex">setlocal</span> foldlevel=1 <span class="st">&quot; 设置折叠层数为</span></span><span id="cb13-39"><a href="#cb13-39" aria-hidden="true"></a><span class="st">&quot;</span> set foldclose=all <span class="st">&quot; 设置为自动关闭折叠 </span></span><span id="cb13-40"><a href="#cb13-40" aria-hidden="true"></a><span class="st">&quot;</span> nnoremap <span class="op">&lt;</span>space<span class="op">&gt;</span> @=((foldclosed(line(<span class="st">&#39;.&#39;</span>)) <span class="op">&lt;</span> <span class="ex">0</span>) <span class="ex">?</span> <span class="st">&#39;zc&#39;</span> : <span class="st">&#39;zo&#39;</span>)<span class="op">&lt;</span><span class="ex">CR</span><span class="op">&gt;</span></span><span id="cb13-41"><a href="#cb13-41" aria-hidden="true"></a><span class="st">&quot; 用空格键来开关折叠</span></span><span id="cb13-42"><a href="#cb13-42" aria-hidden="true"></a></span><span id="cb13-43"><a href="#cb13-43" aria-hidden="true"></a></span><span id="cb13-44"><a href="#cb13-44" aria-hidden="true"></a><span class="st">&quot;</span> <span class="bu">return</span> OS type, eg: windows, or linux, mac, et.st..</span><span id="cb13-45"><a href="#cb13-45" aria-hidden="true"></a><span class="kw">function</span>! <span class="fu">MySys()</span></span><span id="cb13-46"><a href="#cb13-46" aria-hidden="true"></a><span class="kw">if</span> <span class="ex">has</span>(<span class="st">&quot;win16&quot;</span>) <span class="kw">||</span> <span class="ex">has</span>(<span class="st">&quot;win32&quot;</span>) <span class="kw">||</span> <span class="ex">has</span>(<span class="st">&quot;win64&quot;</span>) <span class="kw">||</span> <span class="ex">has</span>(<span class="st">&quot;win95&quot;</span>)</span><span id="cb13-47"><a href="#cb13-47" aria-hidden="true"></a><span class="bu">return</span> <span class="st">&quot;windows&quot;</span></span><span id="cb13-48"><a href="#cb13-48" aria-hidden="true"></a><span class="ex">elseif</span> has(<span class="st">&quot;unix&quot;</span>)</span><span id="cb13-49"><a href="#cb13-49" aria-hidden="true"></a><span class="bu">return</span> <span class="st">&quot;linux&quot;</span></span><span id="cb13-50"><a href="#cb13-50" aria-hidden="true"></a><span class="ex">endif</span></span><span id="cb13-51"><a href="#cb13-51" aria-hidden="true"></a><span class="ex">endfunction</span></span><span id="cb13-52"><a href="#cb13-52" aria-hidden="true"></a></span><span id="cb13-53"><a href="#cb13-53" aria-hidden="true"></a><span class="st">&quot; 用户目录变量</span><span class="va">$VIMFILES</span></span><span id="cb13-54"><a href="#cb13-54" aria-hidden="true"></a><span class="st">if MySys() ** &quot;</span><span class="ex">windows</span><span class="st">&quot;</span></span><span id="cb13-55"><a href="#cb13-55" aria-hidden="true"></a><span class="st">let </span><span class="va">$VIMFILES</span><span class="st"> = </span><span class="va">$VIM</span><span class="st">.&#39;/vimfiles&#39;</span></span><span id="cb13-56"><a href="#cb13-56" aria-hidden="true"></a><span class="st">elseif MySys() ** &quot;</span>linux<span class="st">&quot;</span></span><span id="cb13-57"><a href="#cb13-57" aria-hidden="true"></a><span class="st">let </span><span class="va">$VIMFILES</span><span class="st"> = </span><span class="va">$HOME</span><span class="st">.&#39;/.vim&#39;</span></span><span id="cb13-58"><a href="#cb13-58" aria-hidden="true"></a><span class="st">endif</span></span><span id="cb13-59"><a href="#cb13-59" aria-hidden="true"></a></span><span id="cb13-60"><a href="#cb13-60" aria-hidden="true"></a><span class="st">&quot;</span> 设定doc文档目录</span><span id="cb13-61"><a href="#cb13-61" aria-hidden="true"></a><span class="bu">let</span> helptags=<span class="va">$VIMFILES</span>.<span class="st">&#39;/doc&#39;</span></span><span id="cb13-62"><a href="#cb13-62" aria-hidden="true"></a></span><span id="cb13-63"><a href="#cb13-63" aria-hidden="true"></a><span class="st">&quot; 设置字体 以及中文支持</span></span><span id="cb13-64"><a href="#cb13-64" aria-hidden="true"></a><span class="st">if has(&quot;</span><span class="ex">win32</span><span class="st">&quot;)</span></span><span id="cb13-65"><a href="#cb13-65" aria-hidden="true"></a><span class="st">set guifont=Inconsolata:h12:cANSI</span></span><span id="cb13-66"><a href="#cb13-66" aria-hidden="true"></a><span class="st">endif</span></span><span id="cb13-67"><a href="#cb13-67" aria-hidden="true"></a></span><span id="cb13-68"><a href="#cb13-68" aria-hidden="true"></a><span class="st">&quot;</span> 配置多语言环境</span><span id="cb13-69"><a href="#cb13-69" aria-hidden="true"></a><span class="kw">if</span> <span class="ex">has</span>(<span class="st">&quot;multi_byte&quot;</span>)</span><span id="cb13-70"><a href="#cb13-70" aria-hidden="true"></a><span class="st">&quot; UTF-8 编码</span></span><span id="cb13-71"><a href="#cb13-71" aria-hidden="true"></a><span class="st">set encoding=utf-8</span></span><span id="cb13-72"><a href="#cb13-72" aria-hidden="true"></a><span class="st">set termencoding=utf-8</span></span><span id="cb13-73"><a href="#cb13-73" aria-hidden="true"></a><span class="st">set formatoptions+=mM</span></span><span id="cb13-74"><a href="#cb13-74" aria-hidden="true"></a><span class="st">set fencs=utf-8,gbk</span></span><span id="cb13-75"><a href="#cb13-75" aria-hidden="true"></a></span><span id="cb13-76"><a href="#cb13-76" aria-hidden="true"></a><span class="st">if v:lang =~? &#39;^\(zh\)\|\(ja\)\|\(ko\)&#39;</span></span><span id="cb13-77"><a href="#cb13-77" aria-hidden="true"></a><span class="st">set ambiwidth=double</span></span><span id="cb13-78"><a href="#cb13-78" aria-hidden="true"></a><span class="st">endif</span></span><span id="cb13-79"><a href="#cb13-79" aria-hidden="true"></a></span><span id="cb13-80"><a href="#cb13-80" aria-hidden="true"></a><span class="st">if has(&quot;</span><span class="ex">win32</span><span class="st">&quot;)</span></span><span id="cb13-81"><a href="#cb13-81" aria-hidden="true"></a><span class="st">source </span><span class="va">$VIMRUNTIME</span><span class="st">/delmenu.vim</span></span><span id="cb13-82"><a href="#cb13-82" aria-hidden="true"></a><span class="st">source </span><span class="va">$VIMRUNTIME</span><span class="st">/menu.vim</span></span><span id="cb13-83"><a href="#cb13-83" aria-hidden="true"></a><span class="st">language messages zh_CN.utf-8</span></span><span id="cb13-84"><a href="#cb13-84" aria-hidden="true"></a><span class="st">endif</span></span><span id="cb13-85"><a href="#cb13-85" aria-hidden="true"></a><span class="st">else</span></span><span id="cb13-86"><a href="#cb13-86" aria-hidden="true"></a><span class="st">echoerr &quot;</span>Sorry, this version of (g)<span class="ex">vim</span> was not compiled with +multi_byte<span class="st">&quot;</span></span><span id="cb13-87"><a href="#cb13-87" aria-hidden="true"></a><span class="st">endif</span></span><span id="cb13-88"><a href="#cb13-88" aria-hidden="true"></a></span><span id="cb13-89"><a href="#cb13-89" aria-hidden="true"></a><span class="st">&quot;</span> Buffers操作快捷方式!</span><span id="cb13-90"><a href="#cb13-90" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-RETURN<span class="op">&gt;</span> :bnext<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-91"><a href="#cb13-91" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-S-RETURN<span class="op">&gt;</span> :bprevious<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-92"><a href="#cb13-92" aria-hidden="true"></a></span><span id="cb13-93"><a href="#cb13-93" aria-hidden="true"></a><span class="st">&quot; Tab操作快捷方式!</span></span><span id="cb13-94"><a href="#cb13-94" aria-hidden="true"></a><span class="st">nnoremap &lt;C-TAB&gt; :tabnext&lt;CR&gt;</span></span><span id="cb13-95"><a href="#cb13-95" aria-hidden="true"></a><span class="st">nnoremap &lt;C-S-TAB&gt; :tabprev&lt;CR&gt;</span></span><span id="cb13-96"><a href="#cb13-96" aria-hidden="true"></a></span><span id="cb13-97"><a href="#cb13-97" aria-hidden="true"></a><span class="st">&quot;</span>关于<span class="ex">tab</span>的快捷键</span><span id="cb13-98"><a href="#cb13-98" aria-hidden="true"></a><span class="st">&quot; map tn :tabnext&lt;cr&gt;</span></span><span id="cb13-99"><a href="#cb13-99" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">map</span> tp :tabprevious<span class="op">&lt;</span>cr<span class="op">&gt;</span></span><span id="cb13-100"><a href="#cb13-100" aria-hidden="true"></a><span class="st">&quot; map td :tabnew .&lt;cr&gt;</span></span><span id="cb13-101"><a href="#cb13-101" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">map</span> te :tabedit</span><span id="cb13-102"><a href="#cb13-102" aria-hidden="true"></a><span class="st">&quot; map tc :tabclose&lt;cr&gt;</span></span><span id="cb13-103"><a href="#cb13-103" aria-hidden="true"></a></span><span id="cb13-104"><a href="#cb13-104" aria-hidden="true"></a><span class="st">&quot;</span>窗口分割时,进行切换的按键热键需要连接两次,比如从下方窗口移动</span><span id="cb13-105"><a href="#cb13-105" aria-hidden="true"></a><span class="st">&quot;光标到上方窗口,需要&lt;c-w&gt;&lt;c-w&gt;k,非常麻烦,现在重映射为&lt;c-k&gt;,切换的</span></span><span id="cb13-106"><a href="#cb13-106" aria-hidden="true"></a><span class="st">&quot;</span>时候会变得非常方便<span class="ex">.</span></span><span id="cb13-107"><a href="#cb13-107" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-h<span class="op">&gt;</span> <span class="op">&lt;</span>C-w<span class="op">&gt;</span>h</span><span id="cb13-108"><a href="#cb13-108" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-j<span class="op">&gt;</span> <span class="op">&lt;</span>C-w<span class="op">&gt;</span>j</span><span id="cb13-109"><a href="#cb13-109" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-k<span class="op">&gt;</span> <span class="op">&lt;</span>C-w<span class="op">&gt;</span>k</span><span id="cb13-110"><a href="#cb13-110" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>C-l<span class="op">&gt;</span> <span class="op">&lt;</span>C-w<span class="op">&gt;</span>l</span><span id="cb13-111"><a href="#cb13-111" aria-hidden="true"></a></span><span id="cb13-112"><a href="#cb13-112" aria-hidden="true"></a><span class="st">&quot;一些不错的映射转换语法（如果在一个文件中混合了不同语言时有用）</span></span><span id="cb13-113"><a href="#cb13-113" aria-hidden="true"></a><span class="st">nnoremap &lt;leader&gt;1 :set filetype=xhtml&lt;CR&gt;</span></span><span id="cb13-114"><a href="#cb13-114" aria-hidden="true"></a><span class="st">nnoremap &lt;leader&gt;2 :set filetype=css&lt;CR&gt;</span></span><span id="cb13-115"><a href="#cb13-115" aria-hidden="true"></a><span class="st">nnoremap &lt;leader&gt;3 :set filetype=javascript&lt;CR&gt;</span></span><span id="cb13-116"><a href="#cb13-116" aria-hidden="true"></a><span class="st">nnoremap &lt;leader&gt;4 :set filetype=php&lt;CR&gt;</span></span><span id="cb13-117"><a href="#cb13-117" aria-hidden="true"></a></span><span id="cb13-118"><a href="#cb13-118" aria-hidden="true"></a><span class="st">&quot;</span> <span class="kw">set</span> <span class="va">fileformats=</span>unix,dos,mac</span><span id="cb13-119"><a href="#cb13-119" aria-hidden="true"></a><span class="st">&quot; nmap &lt;leader&gt;fd :se fileformat=dos&lt;CR&gt;</span></span><span id="cb13-120"><a href="#cb13-120" aria-hidden="true"></a><span class="st">&quot;</span> <span class="fu">nmap</span> <span class="op">&lt;</span>leader<span class="op">&gt;</span>fu :se fileformat=unix<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-121"><a href="#cb13-121" aria-hidden="true"></a></span><span id="cb13-122"><a href="#cb13-122" aria-hidden="true"></a><span class="st">&quot; use Ctrl+[l|n|p|cc] to list|next|previous|jump to count the result</span></span><span id="cb13-123"><a href="#cb13-123" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">map</span> <span class="op">&lt;</span>C-x<span class="op">&gt;</span>l <span class="op">&lt;</span>ESC<span class="op">&gt;</span>:cl<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-124"><a href="#cb13-124" aria-hidden="true"></a><span class="st">&quot; map &lt;C-x&gt;n &lt;ESC&gt;:cn&lt;CR&gt;</span></span><span id="cb13-125"><a href="#cb13-125" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">map</span> <span class="op">&lt;</span>C-x<span class="op">&gt;</span>p <span class="op">&lt;</span>ESC<span class="op">&gt;</span>:cp<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-126"><a href="#cb13-126" aria-hidden="true"></a><span class="st">&quot; map &lt;C-x&gt;c &lt;ESC&gt;:cc&lt;CR&gt;</span></span><span id="cb13-127"><a href="#cb13-127" aria-hidden="true"></a></span><span id="cb13-128"><a href="#cb13-128" aria-hidden="true"></a></span><span id="cb13-129"><a href="#cb13-129" aria-hidden="true"></a><span class="st">&quot;</span> 让 <span class="ex">Tohtml</span> 产生有 CSS 语法的 html</span><span id="cb13-130"><a href="#cb13-130" aria-hidden="true"></a><span class="st">&quot; syntax/2html.vim，可以用:runtime! syntax/2html.vim</span></span><span id="cb13-131"><a href="#cb13-131" aria-hidden="true"></a><span class="st">let html_use_css=1</span></span><span id="cb13-132"><a href="#cb13-132" aria-hidden="true"></a></span><span id="cb13-133"><a href="#cb13-133" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">Python</span> 文件的一般设置，比如不要 tab 等</span><span id="cb13-134"><a href="#cb13-134" aria-hidden="true"></a><span class="ex">autocmd</span> FileType python set tabstop=4 shiftwidth=4 expandtab</span><span id="cb13-135"><a href="#cb13-135" aria-hidden="true"></a><span class="ex">autocmd</span> FileType python map <span class="op">&lt;</span>F12<span class="op">&gt;</span> :!python %<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-136"><a href="#cb13-136" aria-hidden="true"></a></span><span id="cb13-137"><a href="#cb13-137" aria-hidden="true"></a><span class="st">&quot; 选中状态下 Ctrl+c 复制</span></span><span id="cb13-138"><a href="#cb13-138" aria-hidden="true"></a><span class="st">vmap &lt;C-c&gt; &quot;</span><span class="ex">+y</span></span><span id="cb13-139"><a href="#cb13-139" aria-hidden="true"></a></span><span id="cb13-140"><a href="#cb13-140" aria-hidden="true"></a><span class="st">&quot; 打开javascript折叠</span></span><span id="cb13-141"><a href="#cb13-141" aria-hidden="true"></a><span class="st">let b:javascript_fold=1</span></span><span id="cb13-142"><a href="#cb13-142" aria-hidden="true"></a><span class="st">&quot;</span> 打开<span class="ex">javascript</span>对dom、html和css的支持</span><span id="cb13-143"><a href="#cb13-143" aria-hidden="true"></a><span class="bu">let</span> javascript_enable_domhtmlcss=1</span><span id="cb13-144"><a href="#cb13-144" aria-hidden="true"></a><span class="st">&quot; 设置字典 ~/.vim/dict/文件的路径</span></span><span id="cb13-145"><a href="#cb13-145" aria-hidden="true"></a><span class="st">autocmd filetype javascript set dictionary=</span><span class="va">$VIMFILES</span><span class="st">/dict/javascript.dict</span></span><span id="cb13-146"><a href="#cb13-146" aria-hidden="true"></a><span class="st">autocmd filetype css set dictionary=</span><span class="va">$VIMFILES</span><span class="st">/dict/css.dict</span></span><span id="cb13-147"><a href="#cb13-147" aria-hidden="true"></a><span class="st">autocmd filetype php set dictionary=</span><span class="va">$VIMFILES</span><span class="st">/dict/php.dict</span></span><span id="cb13-148"><a href="#cb13-148" aria-hidden="true"></a></span><span id="cb13-149"><a href="#cb13-149" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-150"><a href="#cb13-150" aria-hidden="true"></a><span class="st">&quot; plugin - bufexplorer.vim Buffers切换</span></span><span id="cb13-151"><a href="#cb13-151" aria-hidden="true"></a><span class="st">&quot;</span> \<span class="ex">be</span> 全屏方式查看全部打开的文件列表</span><span id="cb13-152"><a href="#cb13-152" aria-hidden="true"></a><span class="st">&quot; \bv 左右方式查看 \bs 上下方式查看</span></span><span id="cb13-153"><a href="#cb13-153" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-154"><a href="#cb13-154" aria-hidden="true"></a></span><span id="cb13-155"><a href="#cb13-155" aria-hidden="true"></a></span><span id="cb13-156"><a href="#cb13-156" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-157"><a href="#cb13-157" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> - taglist.vim 查看函数列表，需要ctags程序</span><span id="cb13-158"><a href="#cb13-158" aria-hidden="true"></a><span class="st">&quot; F4 打开隐藏taglist窗口</span></span><span id="cb13-159"><a href="#cb13-159" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-160"><a href="#cb13-160" aria-hidden="true"></a><span class="kw">if</span> <span class="fu">MySys()</span> <span class="ex">**</span> <span class="st">&quot;windows&quot;</span> <span class="st">&quot; 设定windows系统中ctags程序的位置</span></span><span id="cb13-161"><a href="#cb13-161" aria-hidden="true"></a><span class="st">let Tlist_Ctags_Cmd = &#39;&quot;&#39;.$VIMRUNTIME.&#39;</span>/ctags.exe<span class="st">&quot;&#39;</span></span><span id="cb13-162"><a href="#cb13-162" aria-hidden="true"></a><span class="st">elseif MySys() ** &quot;</span>linux<span class="st">&quot; &quot;</span> 设定windows系统中ctags程序的位置</span><span id="cb13-163"><a href="#cb13-163" aria-hidden="true"></a><span class="bu">let</span> Tlist_Ctags_Cmd = <span class="st">&#39;/usr/bin/ctags&#39;</span></span><span id="cb13-164"><a href="#cb13-164" aria-hidden="true"></a><span class="ex">endif</span></span><span id="cb13-165"><a href="#cb13-165" aria-hidden="true"></a><span class="ex">nnoremap</span> <span class="op">&lt;</span>silent<span class="op">&gt;&lt;</span>F4<span class="op">&gt;</span> :TlistToggle<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-166"><a href="#cb13-166" aria-hidden="true"></a><span class="bu">let</span> Tlist_Show_One_File = 1 <span class="st">&quot; 不同时显示多个文件的tag，只显示当前文件的</span></span><span id="cb13-167"><a href="#cb13-167" aria-hidden="true"></a><span class="st">let Tlist_Exit_OnlyWindow = 1 &quot;</span> 如果taglist窗口是最后一个窗口，则退出vim</span><span id="cb13-168"><a href="#cb13-168" aria-hidden="true"></a><span class="bu">let</span> Tlist_Use_Right_Window = 1 <span class="st">&quot; 在右侧窗口中显示taglist窗口</span></span><span id="cb13-169"><a href="#cb13-169" aria-hidden="true"></a><span class="st">let Tlist_File_Fold_Auto_Close=1 &quot;</span> 自动折叠当前非编辑文件的方法列表</span><span id="cb13-170"><a href="#cb13-170" aria-hidden="true"></a><span class="bu">let</span> Tlist_Auto_Open = 0</span><span id="cb13-171"><a href="#cb13-171" aria-hidden="true"></a><span class="bu">let</span> Tlist_Auto_Update = 1</span><span id="cb13-172"><a href="#cb13-172" aria-hidden="true"></a><span class="bu">let</span> Tlist_Hightlight_Tag_On_BufEnter = 1</span><span id="cb13-173"><a href="#cb13-173" aria-hidden="true"></a><span class="bu">let</span> Tlist_Enable_Fold_Column = 0</span><span id="cb13-174"><a href="#cb13-174" aria-hidden="true"></a><span class="bu">let</span> Tlist_Process_File_Always = 1</span><span id="cb13-175"><a href="#cb13-175" aria-hidden="true"></a><span class="bu">let</span> Tlist_Display_Prototype = 0</span><span id="cb13-176"><a href="#cb13-176" aria-hidden="true"></a><span class="bu">let</span> Tlist_Compact_Format = 1</span><span id="cb13-177"><a href="#cb13-177" aria-hidden="true"></a></span><span id="cb13-178"><a href="#cb13-178" aria-hidden="true"></a></span><span id="cb13-179"><a href="#cb13-179" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-180"><a href="#cb13-180" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> - mark.vim 给各种tags标记不同的颜色，便于观看调式的插件。</span><span id="cb13-181"><a href="#cb13-181" aria-hidden="true"></a><span class="st">&quot; \m mark or unmark the word under (or before) the cursor</span></span><span id="cb13-182"><a href="#cb13-182" aria-hidden="true"></a><span class="st">&quot;</span> \<span class="ex">r</span> manually input a regular expression. 用于搜索.</span><span id="cb13-183"><a href="#cb13-183" aria-hidden="true"></a><span class="st">&quot; \n clear this mark (i.e. the mark under the cursor), or clear all highlighted marks .</span></span><span id="cb13-184"><a href="#cb13-184" aria-hidden="true"></a><span class="st">&quot;</span> <span class="dt">\*</span> 当前<span class="ex">MarkWord</span>的下一个 \# 当前MarkWord的上一个</span><span id="cb13-185"><a href="#cb13-185" aria-hidden="true"></a><span class="st">&quot; \/ 所有MarkWords的下一个 \? 所有MarkWords的上一个</span></span><span id="cb13-186"><a href="#cb13-186" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-187"><a href="#cb13-187" aria-hidden="true"></a></span><span id="cb13-188"><a href="#cb13-188" aria-hidden="true"></a></span><span id="cb13-189"><a href="#cb13-189" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-190"><a href="#cb13-190" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> - NERD_tree.vim 以树状方式浏览系统中的文件和目录</span><span id="cb13-191"><a href="#cb13-191" aria-hidden="true"></a><span class="st">&quot; :ERDtree 打开NERD_tree :NERDtreeClose 关闭NERD_tree</span></span><span id="cb13-192"><a href="#cb13-192" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">o</span> 打开关闭文件或者目录 t 在标签页中打开</span><span id="cb13-193"><a href="#cb13-193" aria-hidden="true"></a><span class="st">&quot; T 在后台标签页中打开 ! 执行此文件</span></span><span id="cb13-194"><a href="#cb13-194" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">p</span> 到上层目录 P 到根目录</span><span id="cb13-195"><a href="#cb13-195" aria-hidden="true"></a><span class="st">&quot; K 到第一个节点 J 到最后一个节点</span></span><span id="cb13-196"><a href="#cb13-196" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">u</span> 打开上层目录 m 显示文件系统菜单（添加、删除、移动操作）</span><span id="cb13-197"><a href="#cb13-197" aria-hidden="true"></a><span class="st">&quot; r 递归刷新当前目录 R 递归刷新当前根目录</span></span><span id="cb13-198"><a href="#cb13-198" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-199"><a href="#cb13-199" aria-hidden="true"></a><span class="st">&quot; F3 NERDTree 切换</span></span><span id="cb13-200"><a href="#cb13-200" aria-hidden="true"></a><span class="st">map &lt;F3&gt; :NERDTreeToggle&lt;CR&gt;</span></span><span id="cb13-201"><a href="#cb13-201" aria-hidden="true"></a><span class="st">imap &lt;F3&gt; &lt;ESC&gt;:NERDTreeToggle&lt;CR&gt;</span></span><span id="cb13-202"><a href="#cb13-202" aria-hidden="true"></a></span><span id="cb13-203"><a href="#cb13-203" aria-hidden="true"></a></span><span id="cb13-204"><a href="#cb13-204" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-205"><a href="#cb13-205" aria-hidden="true"></a><span class="st">&quot; plugin - NERD_commenter.vim 注释代码用的，</span></span><span id="cb13-206"><a href="#cb13-206" aria-hidden="true"></a><span class="st">&quot;</span> [<span class="ex">count</span>],cc 光标以下count行逐行添加注释(7,cc)</span><span id="cb13-207"><a href="#cb13-207" aria-hidden="true"></a><span class="st">&quot; [count],cu 光标以下count行逐行取消注释(7,cu)</span></span><span id="cb13-208"><a href="#cb13-208" aria-hidden="true"></a><span class="st">&quot;</span> [<span class="ex">count</span>],cm 光标以下count行尝试添加块注释(7,cm)</span><span id="cb13-209"><a href="#cb13-209" aria-hidden="true"></a><span class="st">&quot; ,cA 在行尾插入 ,并且进入插入模式。 这个命令方便写注释。</span></span><span id="cb13-210"><a href="#cb13-210" aria-hidden="true"></a><span class="st">&quot;</span> 注：<span class="ex">count</span>参数可选，无则默认为选中行或当前行</span><span id="cb13-211"><a href="#cb13-211" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-212"><a href="#cb13-212" aria-hidden="true"></a><span class="st">let NERDSpaceDelims=1 &quot;</span> 让注释符与语句之间留一个空格</span><span id="cb13-213"><a href="#cb13-213" aria-hidden="true"></a><span class="bu">let</span> NERDCompactSexyComs=1 <span class="st">&quot; 多行注释时样子更好看</span></span><span id="cb13-214"><a href="#cb13-214" aria-hidden="true"></a></span><span id="cb13-215"><a href="#cb13-215" aria-hidden="true"></a></span><span id="cb13-216"><a href="#cb13-216" aria-hidden="true"></a><span class="st">&quot;</span>-----------------------------------------------------------------</span><span id="cb13-217"><a href="#cb13-217" aria-hidden="true"></a><span class="st">&quot; plugin - DoxygenToolkit.vim 由注释生成文档，并且能够快速生成函数标准注释</span></span><span id="cb13-218"><a href="#cb13-218" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-219"><a href="#cb13-219" aria-hidden="true"></a><span class="bu">let</span> g:DoxygenToolkit_authorName=<span class="st">&quot;Asins - asinsimple AT gmail DOT com&quot;</span></span><span id="cb13-220"><a href="#cb13-220" aria-hidden="true"></a><span class="bu">let</span> g:DoxygenToolkit_briefTag_funcName=<span class="st">&quot;yes&quot;</span></span><span id="cb13-221"><a href="#cb13-221" aria-hidden="true"></a><span class="ex">map</span> <span class="op">&lt;</span>leader<span class="op">&gt;</span>da :DoxAuthor<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-222"><a href="#cb13-222" aria-hidden="true"></a><span class="ex">map</span> <span class="op">&lt;</span>leader<span class="op">&gt;</span>df :Dox<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-223"><a href="#cb13-223" aria-hidden="true"></a><span class="ex">map</span> <span class="op">&lt;</span>leader<span class="op">&gt;</span>db :DoxBlock<span class="op">&lt;</span>CR<span class="op">&gt;</span></span><span id="cb13-224"><a href="#cb13-224" aria-hidden="true"></a><span class="ex">map</span> <span class="op">&lt;</span>leader<span class="op">&gt;</span>dc a <span class="op">&lt;</span>LEFT<span class="op">&gt;&lt;</span>LEFT<span class="op">&gt;&lt;</span>LEFT<span class="op">&gt;</span></span><span id="cb13-225"><a href="#cb13-225" aria-hidden="true"></a></span><span id="cb13-226"><a href="#cb13-226" aria-hidden="true"></a></span><span id="cb13-227"><a href="#cb13-227" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-228"><a href="#cb13-228" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> – ZenCoding.vim 很酷的插件，HTML代码生成</span><span id="cb13-229"><a href="#cb13-229" aria-hidden="true"></a><span class="st">&quot; 插件最新版：http://github.com/mattn/zencoding-vim</span></span><span id="cb13-230"><a href="#cb13-230" aria-hidden="true"></a><span class="st">&quot;</span> 常用命令可看：<span class="ex">http</span>://nootn.com/blog/Tool/23/</span><span id="cb13-231"><a href="#cb13-231" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-232"><a href="#cb13-232" aria-hidden="true"></a></span><span id="cb13-233"><a href="#cb13-233" aria-hidden="true"></a></span><span id="cb13-234"><a href="#cb13-234" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-235"><a href="#cb13-235" aria-hidden="true"></a><span class="st">&quot; plugin – checksyntax.vim JavaScript常见语法错误检查</span></span><span id="cb13-236"><a href="#cb13-236" aria-hidden="true"></a><span class="st">&quot;</span> 默认快捷方式为 <span class="ex">F5</span></span><span id="cb13-237"><a href="#cb13-237" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-238"><a href="#cb13-238" aria-hidden="true"></a><span class="st">let g:checksyntax_auto = 0 &quot;</span> 不自动检查</span><span id="cb13-239"><a href="#cb13-239" aria-hidden="true"></a></span><span id="cb13-240"><a href="#cb13-240" aria-hidden="true"></a></span><span id="cb13-241"><a href="#cb13-241" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-242"><a href="#cb13-242" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> - NeoComplCache.vim 自动补全插件</span><span id="cb13-243"><a href="#cb13-243" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-244"><a href="#cb13-244" aria-hidden="true"></a><span class="st">let g:AutoComplPop_NotEnableAtStartup = 1</span></span><span id="cb13-245"><a href="#cb13-245" aria-hidden="true"></a><span class="st">let g:NeoComplCache_EnableAtStartup = 1</span></span><span id="cb13-246"><a href="#cb13-246" aria-hidden="true"></a><span class="st">let g:NeoComplCache_SmartCase = 1</span></span><span id="cb13-247"><a href="#cb13-247" aria-hidden="true"></a><span class="st">let g:NeoComplCache_TagsAutoUpdate = 1</span></span><span id="cb13-248"><a href="#cb13-248" aria-hidden="true"></a><span class="st">let g:NeoComplCache_EnableInfo = 1</span></span><span id="cb13-249"><a href="#cb13-249" aria-hidden="true"></a><span class="st">let g:NeoComplCache_EnableCamelCaseCompletion = 1</span></span><span id="cb13-250"><a href="#cb13-250" aria-hidden="true"></a><span class="st">let g:NeoComplCache_MinSyntaxLength = 3</span></span><span id="cb13-251"><a href="#cb13-251" aria-hidden="true"></a><span class="st">let g:NeoComplCache_EnableSkipCompletion = 1</span></span><span id="cb13-252"><a href="#cb13-252" aria-hidden="true"></a><span class="st">let g:NeoComplCache_SkipInputTime = &#39;0.5&#39;</span></span><span id="cb13-253"><a href="#cb13-253" aria-hidden="true"></a><span class="st">let g:NeoComplCache_SnippetsDir = </span><span class="va">$VIMFILES</span><span class="st">.&#39;/snippets&#39;</span></span><span id="cb13-254"><a href="#cb13-254" aria-hidden="true"></a><span class="st">&quot;</span> <span class="op">&lt;</span><span class="ex">TAB</span><span class="op">&gt;</span> completion.</span><span id="cb13-255"><a href="#cb13-255" aria-hidden="true"></a><span class="ex">inoremap</span> <span class="op">&lt;</span>expr<span class="op">&gt;&lt;</span>TAB<span class="op">&gt;</span> pumvisible() <span class="ex">?</span> <span class="st">&quot;\&lt;C-n&gt;&quot;</span> : <span class="st">&quot;\&lt;TAB&gt;&quot;</span></span><span id="cb13-256"><a href="#cb13-256" aria-hidden="true"></a><span class="st">&quot; snippets expand key</span></span><span id="cb13-257"><a href="#cb13-257" aria-hidden="true"></a><span class="st">imap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand)</span></span><span id="cb13-258"><a href="#cb13-258" aria-hidden="true"></a><span class="st">smap &lt;silent&gt; &lt;C-e&gt; &lt;Plug&gt;(neocomplcache_snippets_expand)</span></span><span id="cb13-259"><a href="#cb13-259" aria-hidden="true"></a></span><span id="cb13-260"><a href="#cb13-260" aria-hidden="true"></a></span><span id="cb13-261"><a href="#cb13-261" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-262"><a href="#cb13-262" aria-hidden="true"></a><span class="st">&quot; plugin - matchit.vim 对%命令进行扩展使得能在嵌套标签和语句之间跳转</span></span><span id="cb13-263"><a href="#cb13-263" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">%</span> 正向匹配 g% 反向匹配</span><span id="cb13-264"><a href="#cb13-264" aria-hidden="true"></a><span class="st">&quot; [% 定位块首 ]% 定位块尾</span></span><span id="cb13-265"><a href="#cb13-265" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-266"><a href="#cb13-266" aria-hidden="true"></a></span><span id="cb13-267"><a href="#cb13-267" aria-hidden="true"></a></span><span id="cb13-268"><a href="#cb13-268" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-269"><a href="#cb13-269" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> - vcscommand.vim 对%命令进行扩展使得能在嵌套标签和语句之间跳转</span><span id="cb13-270"><a href="#cb13-270" aria-hidden="true"></a><span class="st">&quot; SVN/git管理工具</span></span><span id="cb13-271"><a href="#cb13-271" aria-hidden="true"></a><span class="st">&quot;</span><span class="ex">-----------------------------------------------------------------</span></span><span id="cb13-272"><a href="#cb13-272" aria-hidden="true"></a></span><span id="cb13-273"><a href="#cb13-273" aria-hidden="true"></a></span><span id="cb13-274"><a href="#cb13-274" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span><span id="cb13-275"><a href="#cb13-275" aria-hidden="true"></a><span class="st">&quot;</span> <span class="ex">plugin</span> – a.vim</span><span id="cb13-276"><a href="#cb13-276" aria-hidden="true"></a><span class="st">&quot;-----------------------------------------------------------------</span></span></code></pre></div><h2 id="参考文献">参考文献</h2><p>https://www.cnblogs.com/RyanJin/p/10066389.html</p><p>https://www.cnblogs.com/puyangsky/p/5442153.html</p>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mac </tag>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【MAC】抹盘重装教程</title>
      <link href="/blog/post/62abf324.html"/>
      <url>/blog/post/62abf324.html</url>
      
        <content type="html"><![CDATA[<p>一年不抹盘就手痒痒系列&gt;_&lt; 在这里记录自己抹盘重装macOS Mojave的历程，引导器（USB启动磁盘）制作是一个大坑，目前的官方教程丢了一个小细节。</p><a id="more"></a><h2 id="制作可引导的macos安装器">制作可引导的macOS安装器</h2><h3 id="下载macos">下载macOS</h3><p>各个 macOS 版本的下载链接移步官网，这里以我安装的Mojave为例，<a href="https://support.apple.com/zh-cn/HT210190">Mojave</a>：</p><ul><li><p>macOS High Sierra 以上的系统会以 App 的形式直接下载到“应用程序”文件夹，例如“安装 macOS Mojave”或“安装 macOS High Sierra”。</p></li><li><p>其他版本可能会下载为.pkg文件，打开以后按指示操作，最终也会是xxx.app。</p></li></ul><h2 id="在终端中使用createinstallmedia命令">在“终端”中使用“createinstallmedia”命令</h2><ol type="1"><li><p>连接要用于可引导安装器的U盘或其他可用磁盘。确保至少有 12GB 可用储存空间，并已格式化为“Mac OS 扩展”。</p></li><li><p>这里大坑出现了，如果跳过这里操作完发现系统 <strong>无法将制作的引导器识别为启动磁盘</strong>，</p><ul><li>所以，在制作引导器之前，先在mac的磁盘工具里<strong>检查一下自己磁盘</strong>。</li><li>选择左上角的 显示所有设备。（Mojave之后的APFS有了容器的概念，和图中不一样可以不管）</li><li>接着选中要制作引导器的磁盘所在的的<strong>最上层名称</strong>，点击分区，<strong>需要GUID分区图</strong>，macOS扩展（日志式），然后确定。接着就可以愉快的制作引导器了。 <img src="https://img-blog.csdnimg.cn/2020030522060854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></li></ul></li><li><p>打开“应用程序”文件夹内“实用工具”文件夹中的“终端”。在“终端”中键入或粘贴以下命令之一。这些命令假设安装器仍位于“应用程序”文件夹中，并且 MyVolume 是 制作引导器的磁盘（或其他）名。如果不是这个名称，替换 MyVolume 为自己的磁盘名称。</p><p>Catalina：</p><p><code>sudo /Applications/Install\ macOS\ Catalina.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume</code></p><p>Mojave：</p><p><code>sudo /Applications/Install\ macOS\ Mojave.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume</code></p></li><li><p>键入相应的命令后，按下 Return 键。出现提示时，键入管理员密码，然后再次按下 Return 键。在键入密码时，“终端”不会显示任何字符。</p></li><li><p>出现提示时，键入 Y 以确认要抹掉磁盘，然后按下 Return 键。创建可引导安装器过程中，“终端”将显示进度。</p></li><li><p>当“终端”显示这个操作已完成时，该磁盘的名称将与下载的安装器名称相同，例如“安装 macOS Mojave”。现在可以退出“终端”并弹出宗卷。</p></li></ol><h2 id="使用引导器重装">使用引导器重装</h2><ol type="1"><li>将可引导安装器插入兼容的 Mac，兼容性查看<a href="https://support.apple.com/zh-cn/HT201686">官网</a>。</li><li>使用“启动管理器”或“启动磁盘”偏好设置将可引导安装器选择为启动磁盘，然后从中启动。也可以重启时按住option进入选择启动磁盘界面，选择制作的引导器启动。</li><li>如果要抹盘重装，则先选择列表里的磁盘工具，否则直接选择重装macOS进入自动安装。</li><li>选择磁盘工具后，界面和正常mac磁盘工具相同，<strong>选择原来的系统磁盘，抹除，手残点到存放数据的磁盘喜闻乐见（本人老老老版mac，光驱拆了再装一块固态还挺舒服）</strong> <img src="https://img-blog.csdnimg.cn/2020030521355455.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JyZWFraW5nRGF3bjA=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" /></li><li>抹除系统盘后，退出磁盘工具，返回刚才的界面，选择重装macOS。</li><li>接着进入自动安装，都是傻瓜式安装，到了这一步几乎不会出问题了。</li><li>本人安装最后还遇到一点小问题，在选择语言，选择输入法等等过去，到最后连接Wi-Fi时，<strong>卡卡卡住了</strong>，问题不大，重启就好，自动安装到选择语言系统已经完成了安装，重启重新跟着引导配置一次就可以了。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Mac </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MacOS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
